{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan0v0/nninn/blob/master/vq_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypctnKd-rCiy"
      },
      "outputs": [],
      "source": [
        "#!pip3 install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Splitting up neural net params into chunks"
      ],
      "metadata": {
        "id": "mMmijt45OkJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network architecture\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "        # Initialize the weights to be non-negative\n",
        "        nn.init.uniform_(self.fc1.weight, a=0, b=1)\n",
        "        nn.init.uniform_(self.fc2.weight, a=0, b=1)\n",
        "        nn.init.uniform_(self.fc3.weight, a=0, b=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the neural network\n",
        "net = NeuralNetwork()\n",
        "\n",
        "# Split up the neural network parameters into chunks\n",
        "chunk_size = 1000\n",
        "param_chunks = []\n",
        "for param in net.parameters():\n",
        "    flattened_param = param.view(-1)\n",
        "    chunks = torch.split(flattened_param, chunk_size)\n",
        "    param_chunks.extend(chunks)\n",
        "\n",
        "# Print the number of parameter chunks\n",
        "print(\"Number of parameter chunks:\", len(param_chunks))\n",
        "print(\"Parameter chunks:\", param_chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8gb85iH90wU",
        "outputId": "275d758b-fa57-47c7-d36c-118ce7cca955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameter chunks: 15\n",
            "Parameter chunks: [tensor([0.2919, 0.2992, 0.8227, 0.0579, 0.0281, 0.1178, 0.8750, 0.7092, 0.2448,\n",
            "        0.7001, 0.9529, 0.3145, 0.3038, 0.7455, 0.4266, 0.1648, 0.6162, 0.3966,\n",
            "        0.3237, 0.0941, 0.2232, 0.4404, 0.9375, 0.6002, 0.1611, 0.3535, 0.3681,\n",
            "        0.3448, 0.3686, 0.5276, 0.0393, 0.6725, 0.8932, 0.7149, 0.4007, 0.5334,\n",
            "        0.7816, 0.3900, 0.7587, 0.3536, 0.8926, 0.5112, 0.1222, 0.7303, 0.3731,\n",
            "        0.6274, 0.3578, 0.0065, 0.5690, 0.8900, 0.5028, 0.2401, 0.5878, 0.7026,\n",
            "        0.4572, 0.6603, 0.5703, 0.3134, 0.5029, 0.2670, 0.5947, 0.3110, 0.8365,\n",
            "        0.5826, 0.0481, 0.0697, 0.2296, 0.9035, 0.8822, 0.8258, 0.0112, 0.4619,\n",
            "        0.4472, 0.0317, 0.9144, 0.7091, 0.2712, 0.8672, 0.4228, 0.6326, 0.4163,\n",
            "        0.7031, 0.6291, 0.8241, 0.0417, 0.8467, 0.2199, 0.4562, 0.0821, 0.6900,\n",
            "        0.3033, 0.3546, 0.0202, 0.2633, 0.1026, 0.5048, 0.9257, 0.1560, 0.8167,\n",
            "        0.2368, 0.7459, 0.6086, 0.9588, 0.9297, 0.2786, 0.4519, 0.3187, 0.7974,\n",
            "        0.1461, 0.0641, 0.3105, 0.9021, 0.1865, 0.8861, 0.1436, 0.7236, 0.0176,\n",
            "        0.2895, 0.7058, 0.6370, 0.6479, 0.6011, 0.2686, 0.7488, 0.1541, 0.1140,\n",
            "        0.0923, 0.1956, 0.1480, 0.4724, 0.6502, 0.2422, 0.6863, 0.5103, 0.8806,\n",
            "        0.3512, 0.0670, 0.6385, 0.4131, 0.1370, 0.3217, 0.5783, 0.1215, 0.8171,\n",
            "        0.9628, 0.0223, 0.2595, 0.9653, 0.9987, 0.9010, 0.8179, 0.7809, 0.6218,\n",
            "        0.2208, 0.0368, 0.1017, 0.4038, 0.0920, 0.8722, 0.5822, 0.5239, 0.8190,\n",
            "        0.0124, 0.5803, 0.9209, 0.1471, 0.9676, 0.8269, 0.2997, 0.3681, 0.4191,\n",
            "        0.1918, 0.7659, 0.4851, 0.3719, 0.1651, 0.3636, 0.2201, 0.3213, 0.8383,\n",
            "        0.2170, 0.3286, 0.7288, 0.2231, 0.3416, 0.4351, 0.0568, 0.7037, 0.4895,\n",
            "        0.4746, 0.9029, 0.2938, 0.8339, 0.5484, 0.2911, 0.9303, 0.7750, 0.6132,\n",
            "        0.1055, 0.6788, 0.4391, 0.9224, 0.8190, 0.4022, 0.2079, 0.9126, 0.7909,\n",
            "        0.6817, 0.2787, 0.9142, 0.9399, 0.8177, 0.4642, 0.1143, 0.7939, 0.8946,\n",
            "        0.3744, 0.3966, 0.0977, 0.8477, 0.6498, 0.3513, 0.8736, 0.3029, 0.0444,\n",
            "        0.8139, 0.8792, 0.6607, 0.2455, 0.2578, 0.0088, 0.0298, 0.0562, 0.9896,\n",
            "        0.5298, 0.9546, 0.1162, 0.0173, 0.1756, 0.9867, 0.0546, 0.1647, 0.2466,\n",
            "        0.2449, 0.0236, 0.9638, 0.5681, 0.2064, 0.8086, 0.0057, 0.2797, 0.3227,\n",
            "        0.8757, 0.0075, 0.5135, 0.7203, 0.2484, 0.1184, 0.3072, 0.6298, 0.9345,\n",
            "        0.7854, 0.7341, 0.3703, 0.8238, 0.9619, 0.6812, 0.6307, 0.5294, 0.7651,\n",
            "        0.6107, 0.4695, 0.5958, 0.5226, 0.0798, 0.3699, 0.6992, 0.3219, 0.8518,\n",
            "        0.6109, 0.6814, 0.1232, 0.4414, 0.9135, 0.3285, 0.7446, 0.9253, 0.0528,\n",
            "        0.0704, 0.0836, 0.0817, 0.7927, 0.2555, 0.4869, 0.8306, 0.9249, 0.1049,\n",
            "        0.7673, 0.1920, 0.2294, 0.6206, 0.1241, 0.5451, 0.7368, 0.2843, 0.9324,\n",
            "        0.0406, 0.7753, 0.8087, 0.9544, 0.6838, 0.5308, 0.6024, 0.2899, 0.8850,\n",
            "        0.5103, 0.7627, 0.6381, 0.1562, 0.0450, 0.3964, 0.5948, 0.0995, 0.5826,\n",
            "        0.7201, 0.3333, 0.4719, 0.8147, 0.0797, 0.9656, 0.0177, 0.4400, 0.0520,\n",
            "        0.5358, 0.8562, 0.7829, 0.0390, 0.9037, 0.5940, 0.1613, 0.5358, 0.8197,\n",
            "        0.2485, 0.6917, 0.2803, 0.2288, 0.0790, 0.3812, 0.8183, 0.6100, 0.8873,\n",
            "        0.9555, 0.0176, 0.0273, 0.0722, 0.8131, 0.5259, 0.8779, 0.8376, 0.7411,\n",
            "        0.7450, 0.9102, 0.9845, 0.3398, 0.7309, 0.5407, 0.4562, 0.0240, 0.8036,\n",
            "        0.7292, 0.0327, 0.1705, 0.1847, 0.5230, 0.8367, 0.5869, 0.9061, 0.4887,\n",
            "        0.9564, 0.2142, 0.6372, 0.9044, 0.6496, 0.4699, 0.8962, 0.8208, 0.3335,\n",
            "        0.2660, 0.3585, 0.9621, 0.7734, 0.7332, 0.0336, 0.9565, 0.7664, 0.6831,\n",
            "        0.2307, 0.1782, 0.6885, 0.7561, 0.4266, 0.2130, 0.1758, 0.5957, 0.7378,\n",
            "        0.3667, 0.9615, 0.4046, 0.2079, 0.7616, 0.9570, 0.0509, 0.4389, 0.5902,\n",
            "        0.2216, 0.5609, 0.4888, 0.1427, 0.4233, 0.1188, 0.6615, 0.0721, 0.7129,\n",
            "        0.4256, 0.1147, 0.6038, 0.6977, 0.9281, 0.7872, 0.8697, 0.1297, 0.8399,\n",
            "        0.3229, 0.4060, 0.8140, 0.3484, 0.7293, 0.5583, 0.8304, 0.0919, 0.7522,\n",
            "        0.9345, 0.9162, 0.3450, 0.1388, 0.6745, 0.9760, 0.7659, 0.9583, 0.3961,\n",
            "        0.2585, 0.7330, 0.6963, 0.8891, 0.8353, 0.0431, 0.0847, 0.3247, 0.0416,\n",
            "        0.2303, 0.4254, 0.0404, 0.8510, 0.0929, 0.7594, 0.1275, 0.1966, 0.7361,\n",
            "        0.4711, 0.1931, 0.0523, 0.2327, 0.9298, 0.3323, 0.6739, 0.1488, 0.0506,\n",
            "        0.5219, 0.2673, 0.1059, 0.6120, 0.6989, 0.0839, 0.8920, 0.4117, 0.4996,\n",
            "        0.8211, 0.8199, 0.1359, 0.9806, 0.5910, 0.4080, 0.7185, 0.2081, 0.3333,\n",
            "        0.6920, 0.0218, 0.6119, 0.5400, 0.2097, 0.2294, 0.3431, 0.0606, 0.6952,\n",
            "        0.9455, 0.3570, 0.1880, 0.1369, 0.7216, 0.1071, 0.3432, 0.6203, 0.1804,\n",
            "        0.0041, 0.7086, 0.4753, 0.8329, 0.5119, 0.4306, 0.8954, 0.8242, 0.6512,\n",
            "        0.5570, 0.5236, 0.9723, 0.4311, 0.9561, 0.0656, 0.3758, 0.1795, 0.1160,\n",
            "        0.8293, 0.5174, 0.1244, 0.3381, 0.3436, 0.7946, 0.1466, 0.3362, 0.0040,\n",
            "        0.0256, 0.3042, 0.0061, 0.5950, 0.4058, 0.3301, 0.8605, 0.6826, 0.9879,\n",
            "        0.4001, 0.3863, 0.8355, 0.0286, 0.2022, 0.4124, 0.6971, 0.6922, 0.8392,\n",
            "        0.0782, 0.2777, 0.9384, 0.5849, 0.8672, 0.2819, 0.9325, 0.9876, 0.4864,\n",
            "        0.9371, 0.6035, 0.3527, 0.7857, 0.4675, 0.5426, 0.9487, 0.5009, 0.2171,\n",
            "        0.4099, 0.6473, 0.3774, 0.5890, 0.3896, 0.4575, 0.8536, 0.5957, 0.8121,\n",
            "        0.0693, 0.1394, 0.3646, 0.3582, 0.9908, 0.1838, 0.4331, 0.3501, 0.8112,\n",
            "        0.6827, 0.9731, 0.9611, 0.9763, 0.1284, 0.2010, 0.3614, 0.6078, 0.8066,\n",
            "        0.5097, 0.2226, 0.9033, 0.6573, 0.1140, 0.8890, 0.7957, 0.2512, 0.0721,\n",
            "        0.6930, 0.7751, 0.9261, 0.4611, 0.3293, 0.9032, 0.0432, 0.6968, 0.9012,\n",
            "        0.0981, 0.5945, 0.9949, 0.4183, 0.3309, 0.4897, 0.8072, 0.0818, 0.9268,\n",
            "        0.3656, 0.3243, 0.5175, 0.0013, 0.2398, 0.8267, 0.9127, 0.2058, 0.2325,\n",
            "        0.3389, 0.4721, 0.2372, 0.3945, 0.0889, 0.4900, 0.5724, 0.9581, 0.9603,\n",
            "        0.1610, 0.9214, 0.0375, 0.0451, 0.7934, 0.6475, 0.1445, 0.1679, 0.0698,\n",
            "        0.0987, 0.1760, 0.0174, 0.4919, 0.9447, 0.1896, 0.1436, 0.1653, 0.0281,\n",
            "        0.0278, 0.7511, 0.7315, 0.1818, 0.3715, 0.7946, 0.3884, 0.1068, 0.6011,\n",
            "        0.7679, 0.6814, 0.3940, 0.9338, 0.4608, 0.8991, 0.5879, 0.5138, 0.4976,\n",
            "        0.2447, 0.4112, 0.4855, 0.8326, 0.6974, 0.1995, 0.1940, 0.7566, 0.9721,\n",
            "        0.9983, 0.0857, 0.4672, 0.8634, 0.2319, 0.5663, 0.0739, 0.4539, 0.2265,\n",
            "        0.9568, 0.0348, 0.0846, 0.0197, 0.9623, 0.4438, 0.6234, 0.5403, 0.3667,\n",
            "        0.4925, 0.2795, 0.7027, 0.6012, 0.6518, 0.8942, 0.8809, 0.7014, 0.2627,\n",
            "        0.1633, 0.9677, 0.8598, 0.6828, 0.7470, 0.7648, 0.3324, 0.4294, 0.5214,\n",
            "        0.0318, 0.8156, 0.9235, 0.6739, 0.0392, 0.5387, 0.9418, 0.2299, 0.1296,\n",
            "        0.3439, 0.2125, 0.2990, 0.5095, 0.7222, 0.0292, 0.4389, 0.0755, 0.3633,\n",
            "        0.0417, 0.5852, 0.2260, 0.1621, 0.2005, 0.1174, 0.5496, 0.0617, 0.7146,\n",
            "        0.4838, 0.5199, 0.3523, 0.6319, 0.2505, 0.1599, 0.3926, 0.9538, 0.3567,\n",
            "        0.1510, 0.9055, 0.3683, 0.0092, 0.8166, 0.2494, 0.1896, 0.2623, 0.5327,\n",
            "        0.7311, 0.6864, 0.1896, 0.4078, 0.2208, 0.9375, 0.2486, 0.2851, 0.6574,\n",
            "        0.0696, 0.3736, 0.2929, 0.6349, 0.5474, 0.6493, 0.9408, 0.8902, 0.1259,\n",
            "        0.3367, 0.7428, 0.9660, 0.5855, 0.7908, 0.5531, 0.0392, 0.5095, 0.5033,\n",
            "        0.1634, 0.5398, 0.4764, 0.8872, 0.1221, 0.1992, 0.0332, 0.4778, 0.1068,\n",
            "        0.7036, 0.7291, 0.4293, 0.0974, 0.2747, 0.8877, 0.5814, 0.3683, 0.1925,\n",
            "        0.2460, 0.2161, 0.5742, 0.5131, 0.4127, 0.7527, 0.4341, 0.1548, 0.5491,\n",
            "        0.0172, 0.4287, 0.1038, 0.6711, 0.8101, 0.5969, 0.3310, 0.4557, 0.8323,\n",
            "        0.0109, 0.2091, 0.7198, 0.6496, 0.3326, 0.5473, 0.4772, 0.6579, 0.7648,\n",
            "        0.8040, 0.9207, 0.5222, 0.4816, 0.1884, 0.0054, 0.2233, 0.8435, 0.5693,\n",
            "        0.1727, 0.8985, 0.8567, 0.0273, 0.3893, 0.4567, 0.3959, 0.1018, 0.1019,\n",
            "        0.2176, 0.0509, 0.2635, 0.9570, 0.2949, 0.7830, 0.5304, 0.6554, 0.0386,\n",
            "        0.9613, 0.7682, 0.3764, 0.8657, 0.8287, 0.3917, 0.7409, 0.8845, 0.6597,\n",
            "        0.9107, 0.6256, 0.2685, 0.4889, 0.6601, 0.4111, 0.3724, 0.9099, 0.0842,\n",
            "        0.4152, 0.9062, 0.4827, 0.6941, 0.4661, 0.7016, 0.4611, 0.4893, 0.9682,\n",
            "        0.2115, 0.8178, 0.6084, 0.6298, 0.7246, 0.8726, 0.2098, 0.3228, 0.5981,\n",
            "        0.6971, 0.5320, 0.0798, 0.6744, 0.3740, 0.7246, 0.0305, 0.6499, 0.3157,\n",
            "        0.2242, 0.1107, 0.4868, 0.3243, 0.6768, 0.0533, 0.3424, 0.9991, 0.4159,\n",
            "        0.3703, 0.6324, 0.5490, 0.4443, 0.6802, 0.9749, 0.8978, 0.5990, 0.9794,\n",
            "        0.7609, 0.3668, 0.6172, 0.2778, 0.6388, 0.3624, 0.6914, 0.5707, 0.5058,\n",
            "        0.3167, 0.2841, 0.6476, 0.9477, 0.9722, 0.9788, 0.2419, 0.2432, 0.4179,\n",
            "        0.2088, 0.3045, 0.8383, 0.4073, 0.3915, 0.6054, 0.0615, 0.7786, 0.9463,\n",
            "        0.4599, 0.0229, 0.4622, 0.5688, 0.5500, 0.5159, 0.3165, 0.5364, 0.8664,\n",
            "        0.3484, 0.8355, 0.9315, 0.3086, 0.7605, 0.7206, 0.8264, 0.3737, 0.9148,\n",
            "        0.9636, 0.9164, 0.3235, 0.0558, 0.9987, 0.6999, 0.9918, 0.0298, 0.3741,\n",
            "        0.6993, 0.8321, 0.2583, 0.8478, 0.7323, 0.0489, 0.2439, 0.3637, 0.2477,\n",
            "        0.5815], grad_fn=<SplitBackward0>), tensor([ 0.2722, -0.2979, -0.2752, -0.2359,  0.1260,  0.1865,  0.1136, -0.2112,\n",
            "        -0.2538, -0.0460, -0.2369,  0.1943, -0.2965,  0.2589, -0.0735,  0.1418,\n",
            "        -0.0491, -0.2149, -0.0630,  0.0366,  0.0947,  0.0130,  0.3014, -0.1368,\n",
            "         0.1977, -0.1463,  0.2699,  0.1949, -0.0367, -0.1606,  0.1731,  0.2710,\n",
            "        -0.1004,  0.1492, -0.0455,  0.2693, -0.0439,  0.0607,  0.0538,  0.2419,\n",
            "         0.0124,  0.0107,  0.2779, -0.0912, -0.2033, -0.1904, -0.0684, -0.0386,\n",
            "         0.1925, -0.2467,  0.1450, -0.2205, -0.2283,  0.2319, -0.2673,  0.0832,\n",
            "         0.2977,  0.2000, -0.1835,  0.2412,  0.0739,  0.2748,  0.1714,  0.2649,\n",
            "         0.1588, -0.0756,  0.0135, -0.1438, -0.2529,  0.0699, -0.0242,  0.0124,\n",
            "        -0.1972, -0.1478, -0.1024, -0.2978, -0.0770, -0.2395,  0.1456,  0.2580,\n",
            "         0.2018, -0.3112, -0.1134,  0.3136,  0.1619, -0.0555,  0.0375, -0.2290,\n",
            "        -0.1910, -0.2203,  0.2461, -0.1748,  0.1415, -0.1010,  0.0984, -0.0423,\n",
            "         0.2142, -0.1318, -0.1355,  0.0304], grad_fn=<SplitBackward0>), tensor([0.6156, 0.5047, 0.0590, 0.0476, 0.7224, 0.1383, 0.9862, 0.3715, 0.4595,\n",
            "        0.0910, 0.0040, 0.3122, 0.7316, 0.2502, 0.0200, 0.8555, 0.6507, 0.8377,\n",
            "        0.4428, 0.4979, 0.2325, 0.6996, 0.7815, 0.0558, 0.8744, 0.6071, 0.8217,\n",
            "        0.7768, 0.9673, 0.5760, 0.3503, 0.4664, 0.2421, 0.2632, 0.5653, 0.7896,\n",
            "        0.6865, 0.6966, 0.1496, 0.5377, 0.9495, 0.0595, 0.3060, 0.4587, 0.7336,\n",
            "        0.8723, 0.7783, 0.2230, 0.4516, 0.4649, 0.2131, 0.7571, 0.8291, 0.0578,\n",
            "        0.7321, 0.2883, 0.5172, 0.0923, 0.1868, 0.6963, 0.2219, 0.4865, 0.0708,\n",
            "        0.6718, 0.9365, 0.8564, 0.6534, 0.3820, 0.8238, 0.8110, 0.0474, 0.4679,\n",
            "        0.0597, 0.7646, 0.8410, 0.5939, 0.7049, 0.4483, 0.0276, 0.9842, 0.0551,\n",
            "        0.0163, 0.4068, 0.9378, 0.2688, 0.8442, 0.9837, 0.4940, 0.4189, 0.2663,\n",
            "        0.7844, 0.6468, 0.7238, 0.9320, 0.7456, 0.3218, 0.2085, 0.5487, 0.1763,\n",
            "        0.7037, 0.2372, 0.5016, 0.2082, 0.2686, 0.7992, 0.7878, 0.1367, 0.0986,\n",
            "        0.2101, 0.8981, 0.7391, 0.9394, 0.2012, 0.5646, 0.3429, 0.2987, 0.1370,\n",
            "        0.6003, 0.4728, 0.9620, 0.4592, 0.4663, 0.9898, 0.9656, 0.3315, 0.0082,\n",
            "        0.6899, 0.5813, 0.5817, 0.4263, 0.9057, 0.2506, 0.3078, 0.4664, 0.0592,\n",
            "        0.4825, 0.5276, 0.4407, 0.8345, 0.0579, 0.7758, 0.8161, 0.7519, 0.6925,\n",
            "        0.9776, 0.8660, 0.0747, 0.7900, 0.3729, 0.0019, 0.4114, 0.7534, 0.7498,\n",
            "        0.0639, 0.9176, 0.1494, 0.1302, 0.8195, 0.7131, 0.9679, 0.9471, 0.6660,\n",
            "        0.8917, 0.0083, 0.1451, 0.6045, 0.1062, 0.3224, 0.9071, 0.3261, 0.5911,\n",
            "        0.7407, 0.1278, 0.8903, 0.0669, 0.9978, 0.8099, 0.6212, 0.7408, 0.1573,\n",
            "        0.0978, 0.1517, 0.4263, 0.1323, 0.4434, 0.9879, 0.6010, 0.2135, 0.8812,\n",
            "        0.6338, 0.2337, 0.3925, 0.6230, 0.4755, 0.6319, 0.6923, 0.9877, 0.2223,\n",
            "        0.6695, 0.4328, 0.3546, 0.4278, 0.8455, 0.0189, 0.5381, 0.8198, 0.5453,\n",
            "        0.4794, 0.2195, 0.6512, 0.0883, 0.3937, 0.9513, 0.2089, 0.8244, 0.2816,\n",
            "        0.6942, 0.7241, 0.0860, 0.5428, 0.4432, 0.7674, 0.8251, 0.1215, 0.0909,\n",
            "        0.3387, 0.5243, 0.8075, 0.5162, 0.5020, 0.9785, 0.9802, 0.0337, 0.4131,\n",
            "        0.1428, 0.3490, 0.2560, 0.7806, 0.6200, 0.6837, 0.6292, 0.3832, 0.5544,\n",
            "        0.1236, 0.6101, 0.6590, 0.9254, 0.7705, 0.5569, 0.7939, 0.6371, 0.9354,\n",
            "        0.4971, 0.5778, 0.1121, 0.4959, 0.7622, 0.6956, 0.7254, 0.2013, 0.7448,\n",
            "        0.5610, 0.3371, 0.7184, 0.8200, 0.1592, 0.2349, 0.5122, 0.2748, 0.0910,\n",
            "        0.6446, 0.9126, 0.8908, 0.7702, 0.0101, 0.6098, 0.1963, 0.4603, 0.4149,\n",
            "        0.2535, 0.4456, 0.7848, 0.8437, 0.9153, 0.8828, 0.2790, 0.1241, 0.3462,\n",
            "        0.1661, 0.9962, 0.6570, 0.7340, 0.9012, 0.0756, 0.9755, 0.5504, 0.6159,\n",
            "        0.2368, 0.9841, 0.4494, 0.8459, 0.4061, 0.6251, 0.9609, 0.8571, 0.5049,\n",
            "        0.4445, 0.8441, 0.7669, 0.6257, 0.0209, 0.6961, 0.3030, 0.8271, 0.7109,\n",
            "        0.0042, 0.5112, 0.0578, 0.1324, 0.4200, 0.7230, 0.2410, 0.5672, 0.9227,\n",
            "        0.7483, 0.4878, 0.3946, 0.5365, 0.4599, 0.0339, 0.6716, 0.7135, 0.4361,\n",
            "        0.3261, 0.9336, 0.5993, 0.6218, 0.3167, 0.4043, 0.8803, 0.0566, 0.7689,\n",
            "        0.7695, 0.1580, 0.4654, 0.0784, 0.6210, 0.8028, 0.3605, 0.3687, 0.8770,\n",
            "        0.4670, 0.6780, 0.5699, 0.9177, 0.6834, 0.1365, 0.8318, 0.2400, 0.0848,\n",
            "        0.4396, 0.1453, 0.6009, 0.4043, 0.2842, 0.1775, 0.6839, 0.0839, 0.4406,\n",
            "        0.4696, 0.8299, 0.7631, 0.0126, 0.4740, 0.8759, 0.8606, 0.3937, 0.5189,\n",
            "        0.8222, 0.7199, 0.6973, 0.4138, 0.8518, 0.1974, 0.0473, 0.9881, 0.3160,\n",
            "        0.2029, 0.4480, 0.0986, 0.2916, 0.0280, 0.2994, 0.0658, 0.9332, 0.6160,\n",
            "        0.4848, 0.1817, 0.8787, 0.5170, 0.3930, 0.2801, 0.8478, 0.3561, 0.0806,\n",
            "        0.9562, 0.4559, 0.2116, 0.9357, 0.1049, 0.4558, 0.6865, 0.6849, 0.8530,\n",
            "        0.2219, 0.1743, 0.8290, 0.3471, 0.1527, 0.7149, 0.9403, 0.3172, 0.2258,\n",
            "        0.3066, 0.7297, 0.2733, 0.0622, 0.0963, 0.0460, 0.9694, 0.2962, 0.8761,\n",
            "        0.7862, 0.3085, 0.3760, 0.7061, 0.0944, 0.2266, 0.8982, 0.1802, 0.1899,\n",
            "        0.5073, 0.5981, 0.9717, 0.3557, 0.5611, 0.0785, 0.5466, 0.4928, 0.5241,\n",
            "        0.5341, 0.3535, 0.1030, 0.9423, 0.2045, 0.8400, 0.2541, 0.3373, 0.2518,\n",
            "        0.0048, 0.6704, 0.6545, 0.1500, 0.3258, 0.1622, 0.3302, 0.4151, 0.4560,\n",
            "        0.0451, 0.5944, 0.3200, 0.4128, 0.0060, 0.2463, 0.3921, 0.3891, 0.5831,\n",
            "        0.2709, 0.3785, 0.1233, 0.3498, 0.8498, 0.3713, 0.8232, 0.9732, 0.7307,\n",
            "        0.6436, 0.3689, 0.9164, 0.2134, 0.0090, 0.7580, 0.8575, 0.7473, 0.2027,\n",
            "        0.8177, 0.1345, 0.9880, 0.4022, 0.2400, 0.1033, 0.6963, 0.1412, 0.2171,\n",
            "        0.1058, 0.9751, 0.7271, 0.9115, 0.5285, 0.6430, 0.1712, 0.3815, 0.1665,\n",
            "        0.5497, 0.5076, 0.4529, 0.9487, 0.0616, 0.7069, 0.1677, 0.9799, 0.7692,\n",
            "        0.7980, 0.9209, 0.6850, 0.1629, 0.0304, 0.0675, 0.5516, 0.8739, 0.2290,\n",
            "        0.6543, 0.7163, 0.5904, 0.1932, 0.0419, 0.4086, 0.5956, 0.5184, 0.3730,\n",
            "        0.5988, 0.2734, 0.0652, 0.9902, 0.3253, 0.8498, 0.8959, 0.0297, 0.8452,\n",
            "        0.5294, 0.9887, 0.1218, 0.3457, 0.1477, 0.4113, 0.1622, 0.6299, 0.3403,\n",
            "        0.4743, 0.3530, 0.3922, 0.0339, 0.4594, 0.8687, 0.1717, 0.3558, 0.1565,\n",
            "        0.3325, 0.2400, 0.1671, 0.3467, 0.1673, 0.2270, 0.3593, 0.2744, 0.0630,\n",
            "        0.4787, 0.0740, 0.6649, 0.3387, 0.6440, 0.3233, 0.3844, 0.9935, 0.5704,\n",
            "        0.4387, 0.4398, 0.4885, 0.7093, 0.8049, 0.9576, 0.7124, 0.2106, 0.6204,\n",
            "        0.7844, 0.8161, 0.4188, 0.5341, 0.3961, 0.5861, 0.7620, 0.3711, 0.5261,\n",
            "        0.8241, 0.6424, 0.5483, 0.1638, 0.4355, 0.5501, 0.6707, 0.2704, 0.7555,\n",
            "        0.9771, 0.9472, 0.9449, 0.2373, 0.6107, 0.0033, 0.3762, 0.8565, 0.5094,\n",
            "        0.5456, 0.1726, 0.0562, 0.5372, 0.7635, 0.4001, 0.2900, 0.7930, 0.9352,\n",
            "        0.8484, 0.5613, 0.3545, 0.8278, 0.6907, 0.5825, 0.5857, 0.5582, 0.2222,\n",
            "        0.8634, 0.6806, 0.5047, 0.6859, 0.2030, 0.6461, 0.6251, 0.6358, 0.6827,\n",
            "        0.9303, 0.0452, 0.9006, 0.0543, 0.2345, 0.7801, 0.3698, 0.9472, 0.2822,\n",
            "        0.8678, 0.4524, 0.8787, 0.5686, 0.9243, 0.6501, 0.3805, 0.3848, 0.1394,\n",
            "        0.4904, 0.1281, 0.4527, 0.0143, 0.9904, 0.7621, 0.6663, 0.6469, 0.9279,\n",
            "        0.1229, 0.7836, 0.6958, 0.3933, 0.5303, 0.3978, 0.9895, 0.0866, 0.5154,\n",
            "        0.4101, 0.7211, 0.4887, 0.9703, 0.9498, 0.5724, 0.5413, 0.6601, 0.4309,\n",
            "        0.6033, 0.1635, 0.6197, 0.4860, 0.2511, 0.9516, 0.0617, 0.3883, 0.3318,\n",
            "        0.9599, 0.5615, 0.5931, 0.5984, 0.7620, 0.0063, 0.8183, 0.5579, 0.4303,\n",
            "        0.6265, 0.3799, 0.2341, 0.8045, 0.0086, 0.7929, 0.0971, 0.4251, 0.2885,\n",
            "        0.6707, 0.8767, 0.0986, 0.4188, 0.8395, 0.0169, 0.7819, 0.3560, 0.5617,\n",
            "        0.5654, 0.4163, 0.3762, 0.9678, 0.9895, 0.6920, 0.9328, 0.4883, 0.5412,\n",
            "        0.0046, 0.3402, 0.7364, 0.9824, 0.8780, 0.4841, 0.4992, 0.6632, 0.3196,\n",
            "        0.9209, 0.0347, 0.5953, 0.3677, 0.5021, 0.7522, 0.1288, 0.9627, 0.5892,\n",
            "        0.3262, 0.6344, 0.6162, 0.6003, 0.0069, 0.9504, 0.3898, 0.1785, 0.8390,\n",
            "        0.2147, 0.0444, 0.5803, 0.7988, 0.5530, 0.6033, 0.4210, 0.0592, 0.1871,\n",
            "        0.1015, 0.8981, 0.2837, 0.5117, 0.3487, 0.0704, 0.9406, 0.8571, 0.8349,\n",
            "        0.2887, 0.1703, 0.1530, 0.1762, 0.4701, 0.4188, 0.1586, 0.0251, 0.3518,\n",
            "        0.6694, 0.3538, 0.4603, 0.5267, 0.0041, 0.4302, 0.9030, 0.1132, 0.2686,\n",
            "        0.3378, 0.6878, 0.6446, 0.4115, 0.1792, 0.1959, 0.2552, 0.6640, 0.9521,\n",
            "        0.6573, 0.2286, 0.0357, 0.5547, 0.1046, 0.4943, 0.1062, 0.1364, 0.3309,\n",
            "        0.0717, 0.2628, 0.7760, 0.9774, 0.5543, 0.5187, 0.2222, 0.9821, 0.1669,\n",
            "        0.5062, 0.1740, 0.8270, 0.7790, 0.2985, 0.9661, 0.9354, 0.0052, 0.1174,\n",
            "        0.7054, 0.8275, 0.6764, 0.7031, 0.4136, 0.6328, 0.0334, 0.6609, 0.9923,\n",
            "        0.1118, 0.0753, 0.2126, 0.0122, 0.0982, 0.4561, 0.0087, 0.1224, 0.2553,\n",
            "        0.3592, 0.7249, 0.0064, 0.2089, 0.4639, 0.0122, 0.8739, 0.8573, 0.0849,\n",
            "        0.5964, 0.1084, 0.4571, 0.5571, 0.4550, 0.7275, 0.8435, 0.4697, 0.5872,\n",
            "        0.8498, 0.0776, 0.8319, 0.8791, 0.6232, 0.6319, 0.4585, 0.1282, 0.1094,\n",
            "        0.2713, 0.1620, 0.6222, 0.9772, 0.2685, 0.3870, 0.8766, 0.0669, 0.9832,\n",
            "        0.0226, 0.9999, 0.2391, 0.3769, 0.3189, 0.8397, 0.9050, 0.1181, 0.5154,\n",
            "        0.2808, 0.0927, 0.0173, 0.5194, 0.8864, 0.0952, 0.8178, 0.8389, 0.7993,\n",
            "        0.7169, 0.7085, 0.8854, 0.9878, 0.1219, 0.2268, 0.7587, 0.3638, 0.6797,\n",
            "        0.0503, 0.4350, 0.2516, 0.1469, 0.8158, 0.9460, 0.3444, 0.7115, 0.9485,\n",
            "        0.1909, 0.1708, 0.0153, 0.2026, 0.3461, 0.7120, 0.9259, 0.1602, 0.6818,\n",
            "        0.6394, 0.9790, 0.8930, 0.1978, 0.7144, 0.7383, 0.5860, 0.0578, 0.1524,\n",
            "        0.0645, 0.4456, 0.7781, 0.5360, 0.9294, 0.0633, 0.4988, 0.7268, 0.9152,\n",
            "        0.6514, 0.6124, 0.4381, 0.5483, 0.0865, 0.6498, 0.5948, 0.3548, 0.3219,\n",
            "        0.9183, 0.0634, 0.2991, 0.0709, 0.6831, 0.3771, 0.4142, 0.6957, 0.4406,\n",
            "        0.7862, 0.4808, 0.3458, 0.0576, 0.1287, 0.8228, 0.6258, 0.3700, 0.2378,\n",
            "        0.2010, 0.6194, 0.0900, 0.6664, 0.0620, 0.4574, 0.4514, 0.1069, 0.8166,\n",
            "        0.0055, 0.3913, 0.1205, 0.4558, 0.7290, 0.5097, 0.3038, 0.8497, 0.5440,\n",
            "        0.0564], grad_fn=<SplitBackward0>), tensor([1.7005e-01, 1.1666e-01, 3.0017e-01, 3.9219e-01, 7.4174e-01, 5.8998e-01,\n",
            "        4.4836e-01, 8.7748e-01, 9.3727e-01, 5.7159e-01, 2.6482e-01, 3.1377e-02,\n",
            "        4.0732e-01, 3.3316e-01, 1.5970e-01, 9.4419e-02, 3.3062e-01, 7.9154e-02,\n",
            "        4.2634e-02, 4.6286e-02, 2.7817e-01, 5.3322e-01, 1.7151e-01, 1.0883e-01,\n",
            "        1.3883e-01, 2.2318e-01, 2.8213e-01, 6.4256e-01, 9.1600e-02, 1.4612e-01,\n",
            "        2.2775e-01, 2.8537e-01, 3.2612e-01, 9.5129e-02, 7.4341e-01, 3.5387e-01,\n",
            "        8.2403e-01, 3.4122e-01, 8.3827e-01, 1.0065e-01, 6.7993e-01, 3.8542e-01,\n",
            "        4.7832e-01, 6.7260e-01, 3.1434e-02, 4.1277e-01, 2.3319e-01, 1.1493e-01,\n",
            "        6.2500e-01, 2.6925e-01, 7.9441e-01, 5.2788e-01, 1.4458e-01, 2.6232e-01,\n",
            "        2.2971e-01, 6.5098e-01, 1.0089e-01, 3.6444e-01, 2.7613e-01, 1.6095e-01,\n",
            "        6.3433e-01, 9.1743e-01, 7.8122e-01, 7.7779e-01, 3.1587e-01, 7.3638e-01,\n",
            "        6.3768e-01, 3.6370e-01, 6.4700e-01, 3.2074e-01, 9.4303e-02, 2.6365e-01,\n",
            "        7.1318e-01, 5.3018e-01, 7.9588e-01, 1.5422e-01, 5.8473e-02, 8.4953e-02,\n",
            "        8.0641e-01, 1.0385e-01, 1.0919e-01, 3.8080e-01, 5.7152e-01, 8.5621e-01,\n",
            "        5.2722e-01, 4.3233e-01, 8.1488e-01, 2.8431e-01, 3.5745e-01, 9.8340e-01,\n",
            "        9.2416e-01, 7.1367e-01, 4.8294e-01, 3.3183e-01, 7.9400e-02, 7.2483e-01,\n",
            "        2.9301e-01, 8.5526e-01, 7.6991e-01, 6.6288e-01, 6.6718e-01, 4.2626e-01,\n",
            "        4.1672e-01, 9.4594e-01, 5.4138e-02, 6.3073e-01, 5.1069e-01, 4.4477e-01,\n",
            "        3.3838e-01, 5.4483e-01, 4.6422e-01, 9.2004e-01, 2.2765e-01, 9.8119e-01,\n",
            "        7.4588e-02, 3.5732e-01, 1.3802e-01, 3.7555e-01, 1.9677e-01, 5.7869e-01,\n",
            "        8.5794e-01, 2.0770e-02, 1.4178e-01, 5.3179e-01, 2.6140e-01, 3.9258e-01,\n",
            "        4.7864e-01, 3.1654e-01, 4.2198e-01, 2.7434e-02, 8.2952e-01, 2.9318e-01,\n",
            "        6.2357e-01, 2.4152e-01, 5.8429e-01, 8.3493e-01, 4.2084e-01, 4.6301e-01,\n",
            "        1.3926e-01, 1.0935e-01, 8.8446e-01, 5.0919e-01, 1.9895e-01, 6.4898e-02,\n",
            "        1.0643e-01, 2.8251e-01, 1.3549e-01, 2.2180e-01, 9.3063e-01, 7.2061e-01,\n",
            "        7.6975e-01, 9.4237e-01, 2.2381e-01, 3.5581e-01, 6.5576e-01, 1.9706e-01,\n",
            "        9.0492e-01, 1.3622e-01, 2.2807e-01, 1.5835e-02, 6.1035e-01, 3.0366e-01,\n",
            "        8.5291e-01, 2.5442e-01, 3.6357e-01, 7.3450e-01, 8.2166e-01, 5.9105e-01,\n",
            "        8.7567e-01, 9.2838e-01, 3.6813e-01, 3.3251e-01, 4.4624e-01, 9.6604e-01,\n",
            "        7.5360e-01, 1.9126e-01, 6.6865e-01, 8.3346e-01, 7.1907e-01, 5.7920e-01,\n",
            "        7.4702e-01, 9.6508e-01, 9.6058e-01, 9.4176e-02, 4.4352e-01, 4.6611e-01,\n",
            "        6.7276e-01, 2.0116e-01, 6.5430e-01, 7.4586e-01, 8.6180e-01, 1.9206e-01,\n",
            "        4.4736e-01, 3.0387e-01, 8.2002e-01, 9.3689e-01, 2.3084e-01, 8.2387e-01,\n",
            "        5.1666e-01, 2.1584e-01, 2.5252e-01, 8.9761e-01, 3.4680e-01, 2.7950e-01,\n",
            "        1.2791e-02, 2.6185e-01, 3.7040e-01, 3.2530e-01, 8.0861e-01, 9.5100e-02,\n",
            "        1.0306e-01, 9.7973e-01, 3.1503e-01, 4.2498e-01, 7.0982e-01, 3.7152e-01,\n",
            "        5.1912e-01, 4.6323e-02, 9.6175e-01, 5.2501e-02, 3.5777e-02, 4.8079e-01,\n",
            "        6.8623e-02, 3.3860e-01, 1.8105e-01, 5.2092e-01, 2.0818e-01, 2.0490e-01,\n",
            "        3.5303e-01, 8.2455e-01, 8.9929e-01, 2.0675e-01, 3.1155e-01, 4.8783e-01,\n",
            "        7.5159e-01, 2.1051e-01, 2.8793e-01, 8.0752e-01, 8.8935e-01, 7.7994e-01,\n",
            "        5.3571e-01, 6.9511e-01, 7.6367e-01, 3.1760e-01, 8.6620e-01, 5.8491e-01,\n",
            "        9.5618e-01, 6.5680e-02, 2.8611e-01, 2.5194e-01, 9.7906e-01, 2.8239e-01,\n",
            "        4.2041e-01, 9.2192e-01, 8.7344e-02, 5.6343e-02, 9.8979e-01, 6.1526e-02,\n",
            "        6.9939e-01, 1.6985e-01, 1.4977e-01, 5.9720e-01, 8.2210e-01, 1.4749e-01,\n",
            "        7.0268e-01, 5.3120e-01, 4.1281e-01, 3.1713e-01, 1.6293e-01, 6.0297e-01,\n",
            "        5.7417e-01, 1.7543e-01, 6.2245e-01, 4.0132e-01, 8.8909e-01, 9.3741e-01,\n",
            "        2.8685e-01, 6.5944e-01, 5.8039e-01, 8.6449e-01, 7.7501e-03, 2.4469e-02,\n",
            "        7.0196e-01, 4.0404e-01, 1.1174e-01, 4.0859e-01, 6.2958e-01, 3.7209e-02,\n",
            "        3.0135e-01, 4.2019e-01, 7.3251e-02, 4.2163e-01, 8.0964e-01, 1.3237e-01,\n",
            "        4.0383e-01, 4.1976e-02, 5.1902e-01, 8.1282e-01, 9.4370e-01, 3.4592e-01,\n",
            "        8.5962e-01, 2.0496e-01, 8.3592e-01, 2.2357e-01, 5.1899e-01, 5.1449e-01,\n",
            "        4.9179e-02, 4.7831e-01, 6.7946e-01, 4.2688e-01, 1.8509e-01, 8.2833e-01,\n",
            "        4.6986e-02, 7.9702e-01, 8.8384e-01, 5.5483e-02, 5.4860e-01, 4.9125e-01,\n",
            "        1.3874e-01, 7.6268e-01, 4.7116e-01, 4.2179e-01, 4.7029e-01, 9.0328e-01,\n",
            "        6.9537e-01, 2.7364e-04, 4.6744e-02, 8.2734e-01, 3.8787e-01, 8.9154e-01,\n",
            "        3.0420e-01, 2.9863e-01, 9.0176e-01, 2.4517e-01, 8.9591e-01, 3.9290e-01,\n",
            "        2.7138e-01, 7.2356e-02, 7.5573e-01, 7.6389e-01, 8.7432e-01, 4.6033e-02,\n",
            "        4.6867e-01, 8.4838e-01, 2.8408e-01, 2.8373e-01, 6.9504e-01, 5.3131e-01,\n",
            "        8.5662e-01, 7.5316e-01, 1.4161e-01, 5.1248e-01, 4.4653e-01, 2.4532e-01,\n",
            "        3.7518e-01, 9.6192e-01, 7.1266e-01, 5.1898e-01, 3.4921e-01, 8.4536e-01,\n",
            "        9.1142e-01, 5.9937e-01, 9.6927e-01, 7.5522e-01, 2.4317e-02, 1.0499e-01,\n",
            "        1.4952e-01, 2.2524e-01, 6.6086e-01, 5.8560e-01, 5.0387e-01, 2.2404e-01,\n",
            "        5.7418e-01, 6.7140e-01, 9.9775e-01, 7.5994e-01, 6.4386e-01, 4.8158e-01,\n",
            "        6.1415e-01, 1.3626e-01, 9.6032e-01, 9.9841e-01, 2.8935e-01, 4.8423e-02,\n",
            "        5.4588e-01, 2.7709e-01, 1.3132e-01, 1.8312e-01, 4.4643e-01, 4.4688e-01,\n",
            "        9.0033e-01, 8.1476e-01, 2.5926e-01, 5.4006e-01, 8.4081e-01, 6.6149e-01,\n",
            "        4.5713e-01, 2.2104e-01, 1.1819e-01, 6.2622e-02, 4.3112e-02, 1.4242e-01,\n",
            "        7.5520e-01, 5.2646e-01, 9.1014e-02, 5.6572e-01, 8.5047e-01, 3.0696e-01,\n",
            "        1.4261e-01, 2.4934e-01, 6.1853e-01, 2.6761e-01, 9.8322e-01, 1.0662e-01,\n",
            "        3.0731e-03, 5.8613e-01, 6.0068e-01, 9.6413e-01, 4.7935e-01, 7.8267e-01,\n",
            "        2.0559e-01, 8.3167e-01, 5.2035e-01, 1.5843e-01, 2.6989e-01, 6.8861e-01,\n",
            "        9.8567e-01, 8.8446e-02, 8.4443e-01, 7.0701e-03, 7.4526e-01, 3.7128e-01,\n",
            "        1.9515e-01, 1.2785e-01, 7.7458e-01, 2.4632e-01, 1.9707e-01, 6.4752e-01,\n",
            "        7.0682e-01, 8.6354e-01, 4.5426e-01, 6.0352e-01, 7.6665e-02, 1.9540e-01,\n",
            "        6.9507e-01, 8.9103e-01, 2.6339e-01, 5.9060e-01, 5.1544e-01, 3.7111e-01,\n",
            "        4.2763e-01, 1.7024e-01, 4.2212e-01, 5.4432e-01, 4.4172e-01, 5.2280e-01,\n",
            "        7.3298e-01, 5.1096e-01, 2.6756e-01, 7.1993e-01, 6.9032e-01, 1.4685e-01,\n",
            "        8.5995e-01, 9.5765e-01, 3.0743e-01, 8.4447e-01, 1.7598e-01, 1.3941e-02,\n",
            "        3.1753e-01, 8.4180e-01, 8.8180e-01, 6.2055e-01, 2.0462e-01, 2.3511e-01,\n",
            "        3.3028e-01, 1.6676e-01, 8.2712e-02, 5.0483e-01, 1.5359e-01, 5.3544e-01,\n",
            "        8.9707e-01, 9.3196e-01, 9.5862e-01, 1.6998e-01, 1.2393e-01, 3.8408e-01,\n",
            "        3.9733e-01, 9.8487e-01, 1.8384e-01, 8.4719e-02, 7.4308e-01, 8.8326e-01,\n",
            "        9.2346e-01, 5.1791e-01, 3.4190e-01, 7.5905e-01, 1.1830e-01, 6.8024e-01,\n",
            "        8.7210e-01, 8.4960e-01, 3.6726e-01, 2.0493e-01, 3.3206e-01, 7.5828e-02,\n",
            "        3.8605e-01, 1.0225e-01, 8.9853e-02, 5.0168e-01, 2.2696e-02, 3.7882e-01,\n",
            "        4.5978e-01, 7.3548e-02, 9.0169e-01, 6.0388e-01, 4.9351e-03, 6.6341e-01,\n",
            "        3.1704e-01, 4.8123e-01, 3.5728e-01, 3.7180e-01, 2.8336e-01, 4.2730e-01,\n",
            "        1.0179e-01, 9.7361e-01, 4.0922e-02, 1.4208e-01, 7.8593e-01, 8.8070e-01,\n",
            "        7.5949e-02, 6.0494e-01, 4.2074e-01, 5.2482e-01, 5.9548e-01, 4.9479e-01,\n",
            "        5.4550e-01, 8.6927e-01, 6.2646e-01, 6.6350e-02, 5.8590e-01, 1.2408e-01,\n",
            "        3.5280e-01, 5.2302e-01, 4.2644e-01, 5.5858e-01, 4.6435e-01, 1.4186e-01,\n",
            "        2.2734e-02, 9.0950e-01, 6.4617e-01, 3.1105e-01, 2.5621e-01, 4.2197e-01,\n",
            "        4.4438e-01, 8.3902e-01, 1.7741e-01, 1.2512e-01, 6.1543e-01, 7.7354e-01,\n",
            "        3.9532e-01, 3.8963e-01, 4.6385e-01, 4.6716e-01, 6.3026e-01, 3.8787e-01,\n",
            "        4.9613e-01, 6.6913e-01, 2.4379e-01, 9.6212e-02, 8.9553e-01, 8.5939e-01,\n",
            "        7.6679e-01, 2.4219e-01, 9.7597e-01, 5.1442e-01, 1.7074e-01, 8.5409e-01,\n",
            "        1.7356e-01, 7.2067e-01, 6.6869e-02, 3.0801e-01, 5.2492e-01, 7.2995e-01,\n",
            "        3.1266e-01, 1.3449e-01, 1.9907e-01, 7.9076e-02, 5.1127e-02, 1.7030e-01,\n",
            "        6.1963e-01, 6.8761e-03, 3.9424e-01, 5.9956e-01, 3.7497e-01, 5.8577e-01,\n",
            "        6.4418e-01, 5.2290e-01, 2.5830e-01, 5.7184e-01, 3.4656e-01, 2.1712e-01,\n",
            "        9.3367e-01, 7.2935e-01, 3.0840e-01, 7.2091e-01, 1.6392e-01, 5.3592e-01,\n",
            "        1.6049e-01, 8.1834e-01, 4.1632e-03, 5.2426e-01, 9.3892e-01, 5.3703e-01,\n",
            "        6.1665e-01, 1.9482e-02, 6.3529e-02, 2.8566e-01, 7.8334e-01, 3.7192e-01,\n",
            "        3.7733e-01, 6.7578e-01, 3.2294e-01, 6.5653e-01, 1.9412e-01, 7.8988e-01,\n",
            "        6.7393e-01, 9.8516e-01, 8.4820e-01, 9.0386e-01, 3.5579e-01, 7.3360e-01,\n",
            "        5.2082e-01, 5.5059e-01, 6.4680e-01, 2.5178e-02, 2.9848e-01, 9.1105e-01,\n",
            "        6.8161e-01, 9.7696e-01, 5.2227e-01, 2.5316e-01, 4.6507e-01, 6.2676e-01,\n",
            "        2.4276e-01, 8.7361e-01, 4.2923e-01, 5.1528e-01, 7.4745e-01, 7.5996e-01,\n",
            "        2.6656e-01, 6.2860e-01, 1.2210e-01, 7.7992e-02, 5.4483e-01, 3.0504e-01,\n",
            "        8.0313e-01, 3.5736e-01, 3.6328e-01, 9.2281e-01, 1.3732e-01, 8.4596e-01,\n",
            "        8.9444e-01, 1.7497e-01, 4.8589e-02, 8.4779e-02, 1.0374e-01, 6.2337e-01,\n",
            "        8.0454e-01, 4.8293e-01, 1.7789e-01, 1.9225e-01, 2.7900e-01, 5.3478e-01,\n",
            "        8.3257e-01, 4.9717e-01, 1.3031e-01, 9.5306e-01, 4.9045e-01, 7.6630e-01,\n",
            "        6.7337e-01, 6.0650e-01, 5.3449e-01, 3.2851e-01, 1.2814e-01, 3.5469e-01,\n",
            "        5.4094e-01, 9.7498e-01, 4.8103e-01, 9.2211e-01, 9.1887e-01, 1.2846e-01,\n",
            "        4.8088e-01, 7.7132e-02, 2.2145e-01, 7.0690e-01, 1.2794e-01, 5.7778e-01,\n",
            "        8.5942e-01, 1.8864e-01, 8.1646e-01, 9.8537e-01, 7.3244e-01, 5.5392e-01,\n",
            "        9.8813e-01, 8.2371e-01, 9.4541e-01, 4.3640e-01, 8.6370e-02, 4.6699e-01,\n",
            "        4.5199e-01, 8.4129e-01, 5.5687e-01, 9.9428e-01, 6.8420e-01, 1.2193e-01,\n",
            "        6.3380e-01, 1.9628e-01, 7.0196e-01, 2.8684e-01, 5.2535e-01, 6.0582e-01,\n",
            "        7.3332e-01, 7.8634e-01, 2.6769e-01, 5.4117e-01, 7.6337e-01, 8.2082e-01,\n",
            "        5.9583e-03, 9.1445e-01, 6.2172e-01, 3.5870e-01, 3.6755e-01, 4.7666e-01,\n",
            "        8.7162e-01, 1.1606e-01, 7.0398e-01, 5.6342e-01, 6.9116e-01, 4.9093e-02,\n",
            "        2.4258e-01, 2.8912e-01, 8.9873e-01, 2.9220e-01, 1.8977e-01, 9.2828e-01,\n",
            "        1.2885e-01, 6.2854e-01, 4.7518e-01, 8.8112e-01, 3.6826e-01, 1.9411e-01,\n",
            "        8.6104e-01, 7.4039e-01, 9.8154e-01, 7.7913e-01, 7.2796e-01, 8.6890e-01,\n",
            "        7.5808e-01, 5.9062e-01, 1.3648e-01, 2.7051e-01, 6.2196e-01, 9.4650e-01,\n",
            "        9.1468e-01, 8.0408e-01, 3.5359e-01, 5.4272e-01, 7.8907e-01, 3.9938e-01,\n",
            "        6.0442e-01, 5.3934e-01, 2.6431e-01, 8.8037e-01, 8.8657e-01, 8.9616e-01,\n",
            "        2.0573e-01, 3.7843e-01, 4.7254e-01, 9.5340e-01, 6.4885e-02, 8.8891e-01,\n",
            "        6.8152e-01, 8.6282e-01, 2.1815e-01, 6.8990e-01, 4.1855e-02, 1.0510e-02,\n",
            "        6.2519e-01, 2.2814e-01, 5.7934e-01, 5.6535e-01, 9.4360e-01, 5.7492e-01,\n",
            "        6.2487e-01, 1.2947e-01, 4.8932e-01, 9.1663e-01, 1.0622e-01, 3.1898e-01,\n",
            "        7.3071e-01, 9.7579e-01, 4.3600e-01, 6.2025e-01, 6.9734e-01, 6.1953e-01,\n",
            "        9.3895e-01, 5.8103e-01, 4.2630e-01, 9.9368e-01, 2.7681e-01, 1.6213e-01,\n",
            "        7.3685e-01, 9.2858e-01, 2.6640e-01, 6.9646e-01, 8.1599e-01, 4.1385e-01,\n",
            "        4.2492e-01, 3.8916e-01, 9.3658e-01, 8.7049e-01, 4.5277e-02, 6.2847e-01,\n",
            "        1.2338e-01, 9.4561e-01, 3.3327e-01, 4.8719e-01, 4.8219e-01, 2.2453e-01,\n",
            "        7.4020e-01, 9.8629e-02, 2.6855e-01, 6.1580e-01, 5.8799e-01, 6.9764e-01,\n",
            "        7.1276e-01, 4.5881e-01, 1.3201e-01, 6.3033e-01, 3.9892e-01, 1.7015e-01,\n",
            "        9.9970e-01, 4.6053e-01, 3.7058e-01, 3.9178e-01, 2.5608e-01, 8.2464e-01,\n",
            "        7.1979e-01, 8.1908e-01, 9.2570e-02, 1.7074e-01, 4.8665e-01, 5.2187e-01,\n",
            "        6.7852e-01, 8.1056e-01, 5.6971e-01, 2.3120e-01, 7.1460e-01, 7.4141e-01,\n",
            "        7.1939e-01, 1.1488e-01, 5.6104e-02, 7.4426e-01, 9.0886e-01, 3.8618e-01,\n",
            "        2.7703e-02, 2.8512e-02, 6.8716e-01, 8.2865e-01, 1.5769e-01, 3.3748e-01,\n",
            "        7.8608e-01, 6.2773e-01, 2.1057e-01, 1.4214e-01, 6.9624e-02, 6.0433e-01,\n",
            "        5.1126e-01, 2.6381e-01, 4.1922e-01, 7.7065e-01, 4.7489e-01, 5.3732e-02,\n",
            "        1.9187e-01, 3.0338e-01, 3.2625e-01, 2.0834e-01, 3.4983e-01, 8.9638e-01,\n",
            "        1.0077e-01, 3.8959e-02, 4.0128e-01, 8.2388e-01, 1.5779e-01, 9.4968e-01,\n",
            "        8.7511e-01, 7.1263e-01, 4.8214e-02, 3.4159e-01, 5.4617e-01, 7.9715e-01,\n",
            "        2.1637e-01, 4.0021e-01, 4.8355e-01, 3.7251e-01, 2.3675e-01, 4.4611e-01,\n",
            "        3.0902e-01, 1.2537e-01, 5.9303e-01, 1.7958e-01, 2.4312e-01, 9.4495e-01,\n",
            "        4.8312e-01, 9.6944e-01, 9.0234e-01, 5.9917e-02, 5.8126e-01, 7.7481e-01,\n",
            "        5.4437e-01, 5.1612e-01, 8.2866e-02, 1.1600e-01, 5.7049e-01, 1.7439e-01,\n",
            "        1.8957e-01, 1.9898e-01, 3.5809e-01, 2.7773e-01, 6.7125e-01, 2.8056e-02,\n",
            "        2.0527e-01, 2.9704e-02, 7.5943e-01, 7.9190e-01, 5.4953e-01, 9.8213e-01,\n",
            "        4.8641e-01, 1.5908e-01, 7.9281e-01, 1.0527e-01, 6.6671e-02, 6.3096e-01,\n",
            "        7.1486e-01, 7.3923e-01, 5.8023e-01, 5.8136e-01, 5.1043e-01, 8.1104e-01,\n",
            "        7.0976e-01, 6.1934e-01, 3.9853e-01, 3.3839e-01, 2.5379e-01, 4.9912e-01,\n",
            "        5.3290e-01, 9.6111e-01, 4.5638e-01, 1.7885e-01, 5.7721e-01, 8.6650e-01,\n",
            "        8.1133e-01, 6.6998e-01, 7.4434e-01, 6.6671e-01, 6.9205e-01, 3.9824e-02,\n",
            "        2.3456e-01, 4.1711e-01, 9.9012e-02, 6.3110e-01, 2.3210e-02, 1.3668e-01,\n",
            "        4.9812e-01, 5.2462e-01, 5.9158e-01, 4.2387e-01, 3.2786e-01, 1.4822e-01,\n",
            "        2.8082e-01, 7.7469e-01, 8.6983e-01, 6.1497e-01, 3.0101e-01, 2.9953e-01,\n",
            "        8.8479e-01, 1.4575e-01, 6.4328e-01, 2.3502e-01, 3.9903e-01, 2.8077e-01,\n",
            "        8.7391e-01, 1.1383e-01, 7.8451e-01, 3.7174e-01, 2.6164e-01, 1.5148e-01,\n",
            "        5.8617e-01, 3.0316e-01, 7.4597e-01, 3.2926e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([1.7564e-02, 1.8219e-01, 3.1310e-03, 9.2360e-02, 5.5483e-01, 6.9849e-01,\n",
            "        3.0806e-01, 4.7106e-01, 4.1557e-01, 9.8463e-01, 3.0542e-01, 6.7426e-02,\n",
            "        5.4947e-01, 7.8247e-01, 6.0267e-03, 4.1830e-01, 3.0887e-01, 3.4378e-01,\n",
            "        1.5374e-01, 7.3882e-01, 6.7574e-01, 5.8969e-01, 3.6315e-01, 9.1246e-01,\n",
            "        6.8785e-01, 7.2041e-01, 8.5190e-01, 8.5992e-01, 6.4019e-01, 7.4468e-01,\n",
            "        8.8273e-01, 2.5837e-01, 7.4389e-01, 6.7067e-01, 4.0784e-01, 2.4011e-01,\n",
            "        1.7414e-01, 9.6690e-01, 3.1873e-01, 7.2162e-01, 4.5696e-01, 4.0119e-01,\n",
            "        3.2875e-01, 1.5961e-01, 6.8466e-01, 8.2181e-01, 4.5680e-01, 6.7410e-01,\n",
            "        1.2142e-02, 2.9000e-01, 1.5274e-01, 3.7578e-01, 3.2202e-01, 9.6742e-01,\n",
            "        1.8736e-01, 9.2644e-01, 8.1507e-01, 8.3443e-01, 1.2077e-01, 4.6296e-01,\n",
            "        5.0236e-01, 1.9798e-01, 4.0140e-01, 7.3465e-01, 4.9957e-01, 1.9589e-02,\n",
            "        3.3414e-03, 1.0648e-02, 4.2826e-01, 6.6616e-01, 3.9938e-03, 8.1381e-01,\n",
            "        1.9222e-01, 4.4424e-01, 6.1193e-01, 6.3987e-01, 8.3018e-01, 1.4773e-01,\n",
            "        8.7550e-01, 7.3026e-01, 2.0251e-01, 3.1295e-01, 8.0848e-01, 3.0750e-01,\n",
            "        7.5616e-01, 2.0707e-01, 5.4076e-01, 5.0204e-01, 6.7262e-01, 2.5571e-01,\n",
            "        9.3028e-01, 5.6557e-01, 1.5780e-01, 8.4351e-01, 8.5744e-01, 4.0293e-01,\n",
            "        4.2006e-01, 2.6454e-01, 6.6614e-01, 2.8525e-01, 8.3372e-01, 5.0522e-01,\n",
            "        6.2671e-01, 4.0520e-01, 7.7480e-01, 4.3648e-01, 9.6647e-02, 3.7128e-01,\n",
            "        8.8028e-01, 4.4730e-01, 7.4303e-01, 6.0616e-01, 6.7910e-01, 4.7633e-01,\n",
            "        8.3494e-01, 2.9555e-01, 1.3551e-01, 8.4324e-01, 5.2363e-01, 6.3520e-01,\n",
            "        1.3524e-01, 2.4703e-01, 4.6987e-01, 3.7114e-01, 3.1030e-01, 2.5823e-01,\n",
            "        4.2315e-01, 7.2272e-01, 7.7447e-01, 4.3051e-01, 1.2868e-01, 7.3594e-01,\n",
            "        6.3336e-01, 3.2867e-01, 5.2169e-01, 3.3320e-01, 8.9442e-02, 9.7734e-01,\n",
            "        7.6762e-02, 2.6850e-01, 7.8116e-01, 4.5309e-01, 3.0688e-01, 4.1572e-01,\n",
            "        6.9542e-01, 4.5433e-01, 5.3412e-01, 8.4085e-01, 7.9395e-01, 7.9846e-01,\n",
            "        4.7940e-01, 4.4584e-01, 2.6693e-01, 9.3985e-01, 8.9272e-01, 9.8753e-02,\n",
            "        7.1567e-01, 3.9697e-01, 5.0486e-01, 8.1042e-01, 8.1657e-01, 6.6684e-01,\n",
            "        1.6334e-01, 2.9722e-01, 6.4523e-01, 1.0122e-01, 9.6854e-01, 4.9426e-02,\n",
            "        3.1875e-02, 8.9381e-01, 8.4692e-01, 1.4255e-01, 3.7741e-01, 7.4229e-01,\n",
            "        9.7388e-01, 4.6265e-01, 2.9526e-01, 6.8811e-01, 2.6558e-01, 4.9657e-02,\n",
            "        5.4433e-01, 7.7943e-01, 5.0198e-01, 6.7488e-01, 9.0247e-01, 9.8129e-01,\n",
            "        5.8322e-02, 5.7180e-01, 3.5860e-01, 4.9650e-03, 2.1627e-01, 6.4050e-01,\n",
            "        5.5296e-01, 6.5113e-01, 4.9369e-01, 5.2527e-01, 6.0212e-01, 1.9873e-01,\n",
            "        9.9590e-01, 7.5423e-01, 4.6929e-01, 1.9845e-01, 5.2027e-01, 9.4314e-02,\n",
            "        2.0761e-01, 1.5889e-01, 6.1463e-01, 3.8394e-01, 6.1226e-01, 4.4032e-01,\n",
            "        4.3492e-01, 5.3385e-01, 1.0159e-01, 3.1282e-01, 5.3747e-01, 9.1828e-01,\n",
            "        3.3898e-01, 3.0769e-01, 9.9361e-01, 2.6610e-01, 7.1909e-01, 2.2381e-01,\n",
            "        5.9378e-01, 6.8661e-01, 2.7080e-01, 9.9243e-01, 4.2242e-01, 5.5907e-01,\n",
            "        3.9378e-01, 9.6773e-01, 8.7604e-01, 4.2018e-01, 7.2175e-01, 2.0372e-01,\n",
            "        3.9487e-01, 1.0662e-01, 2.4454e-01, 9.4998e-01, 5.2851e-01, 1.0619e-01,\n",
            "        2.7249e-01, 5.4491e-01, 8.3675e-01, 7.6053e-01, 1.2616e-01, 4.1387e-01,\n",
            "        6.2112e-01, 2.1919e-01, 7.0044e-01, 3.9122e-01, 7.5143e-02, 1.6539e-01,\n",
            "        4.2244e-02, 9.3053e-01, 6.1396e-01, 5.9660e-02, 8.4280e-01, 4.1033e-01,\n",
            "        6.9071e-01, 7.2420e-01, 9.5637e-01, 9.1045e-01, 3.2621e-01, 6.0834e-01,\n",
            "        8.7327e-01, 1.9977e-01, 3.0562e-01, 7.3493e-01, 9.7739e-01, 1.3323e-01,\n",
            "        6.8329e-01, 6.4451e-01, 3.0781e-01, 3.7330e-01, 3.2848e-01, 1.1118e-01,\n",
            "        2.3622e-01, 6.5407e-02, 8.7389e-01, 2.0469e-01, 4.5098e-01, 2.7596e-01,\n",
            "        8.7441e-01, 3.1080e-01, 5.3833e-01, 6.9249e-01, 2.6249e-01, 8.9389e-01,\n",
            "        5.3906e-01, 6.8268e-01, 5.4764e-01, 5.9072e-01, 2.2538e-01, 1.7428e-01,\n",
            "        6.2061e-01, 1.5224e-02, 2.8850e-01, 1.1351e-01, 1.3690e-01, 1.8030e-01,\n",
            "        4.5949e-01, 9.1800e-01, 4.3208e-01, 3.2384e-01, 3.4917e-01, 4.9247e-01,\n",
            "        9.8450e-01, 7.6582e-01, 8.5322e-01, 6.2040e-01, 2.8365e-01, 2.3104e-01,\n",
            "        5.5892e-01, 7.4785e-01, 2.2528e-01, 4.4257e-01, 5.7647e-01, 7.2403e-01,\n",
            "        8.4059e-01, 7.2568e-01, 9.2428e-01, 8.7407e-01, 1.1379e-01, 2.6165e-01,\n",
            "        9.4016e-02, 7.0583e-01, 7.5504e-01, 4.0289e-02, 1.0114e-01, 5.0298e-01,\n",
            "        1.1297e-01, 6.8692e-01, 9.1103e-01, 8.9752e-01, 4.0954e-01, 5.5391e-01,\n",
            "        4.5741e-01, 1.6095e-01, 4.4608e-02, 2.8407e-01, 4.1564e-01, 7.2614e-01,\n",
            "        2.7479e-01, 5.8012e-01, 7.2097e-02, 9.5305e-02, 8.2424e-01, 8.8089e-01,\n",
            "        4.6830e-01, 4.3743e-01, 5.0461e-01, 6.8926e-01, 1.9364e-01, 4.7545e-01,\n",
            "        6.6859e-01, 9.3743e-01, 8.4512e-01, 2.8411e-01, 9.5500e-02, 6.9221e-01,\n",
            "        2.1051e-01, 9.7353e-01, 5.4200e-01, 8.9135e-01, 4.2366e-01, 9.8098e-01,\n",
            "        7.9021e-01, 6.7747e-01, 6.7493e-01, 4.3893e-01, 4.3356e-01, 1.0876e-01,\n",
            "        4.4183e-01, 5.4824e-01, 9.5873e-01, 6.3195e-01, 1.2218e-01, 3.5466e-01,\n",
            "        6.2011e-01, 5.6292e-01, 1.5677e-01, 2.1819e-01, 5.8731e-01, 2.1672e-01,\n",
            "        5.1370e-01, 2.7937e-01, 1.6972e-01, 1.0706e-02, 7.0368e-01, 2.2671e-01,\n",
            "        3.9749e-01, 2.5503e-01, 7.0971e-01, 5.7406e-01, 6.6451e-01, 6.8276e-02,\n",
            "        9.9119e-01, 5.9043e-01, 1.4836e-01, 2.5569e-01, 4.6365e-01, 2.7362e-01,\n",
            "        8.8266e-01, 2.4509e-01, 8.9223e-01, 9.1680e-01, 6.6706e-01, 2.3440e-01,\n",
            "        9.8757e-01, 8.6626e-01, 3.8719e-01, 4.5371e-01, 3.6585e-01, 8.8371e-01,\n",
            "        6.3854e-01, 5.1082e-01, 4.2805e-02, 9.2477e-01, 8.6252e-01, 5.4676e-01,\n",
            "        3.2471e-01, 7.9675e-01, 5.2154e-01, 2.4207e-01, 5.2191e-01, 5.0551e-01,\n",
            "        2.7300e-01, 1.1273e-01, 2.1504e-01, 8.1115e-01, 7.6663e-02, 7.5165e-01,\n",
            "        9.3843e-01, 8.2643e-01, 9.4269e-01, 3.6200e-01, 4.9248e-01, 2.2701e-02,\n",
            "        6.9832e-01, 1.5554e-01, 1.8264e-01, 8.5223e-01, 3.6788e-01, 9.7464e-01,\n",
            "        4.2597e-01, 3.2428e-01, 6.5486e-01, 7.0390e-01, 9.2082e-01, 3.2680e-01,\n",
            "        3.3370e-01, 2.6587e-01, 4.8015e-01, 7.4203e-01, 9.2862e-01, 1.4232e-01,\n",
            "        6.1525e-01, 5.0330e-01, 6.0371e-01, 9.0965e-01, 4.9724e-01, 4.5277e-01,\n",
            "        5.0441e-01, 4.7133e-01, 6.8154e-01, 3.6700e-01, 7.8195e-01, 6.4938e-01,\n",
            "        8.9796e-02, 3.1105e-01, 4.2576e-02, 3.7126e-01, 3.7094e-01, 6.6435e-01,\n",
            "        2.0761e-01, 5.1867e-01, 9.0348e-01, 1.1768e-01, 1.5826e-01, 3.6359e-01,\n",
            "        7.4853e-01, 4.3614e-01, 8.2487e-01, 4.2543e-01, 9.8234e-01, 9.5642e-01,\n",
            "        9.7709e-01, 3.3174e-01, 9.5500e-01, 7.8440e-01, 7.5929e-01, 3.4912e-01,\n",
            "        4.6330e-01, 2.9975e-01, 1.8627e-01, 7.2650e-01, 4.1935e-02, 5.3590e-01,\n",
            "        6.3102e-01, 9.8022e-01, 2.6255e-01, 1.9884e-01, 2.7837e-01, 8.2055e-01,\n",
            "        8.5812e-01, 8.8145e-01, 4.4140e-01, 1.5080e-01, 4.0505e-01, 7.1761e-01,\n",
            "        9.5297e-01, 2.0471e-01, 8.5512e-01, 8.3453e-01, 6.5376e-01, 1.6809e-01,\n",
            "        8.6123e-01, 8.5334e-01, 2.2289e-02, 1.0242e-01, 6.6901e-01, 4.4237e-01,\n",
            "        2.5449e-01, 6.6813e-01, 7.6366e-01, 8.2904e-01, 1.1376e-01, 6.5465e-01,\n",
            "        3.0297e-01, 4.5504e-01, 3.0128e-01, 3.8972e-01, 9.4707e-01, 2.4964e-02,\n",
            "        2.7111e-01, 5.4024e-01, 2.0474e-01, 1.4862e-01, 1.4927e-01, 1.2827e-01,\n",
            "        5.5489e-01, 2.9751e-01, 7.0628e-01, 5.9812e-02, 9.2854e-01, 5.3593e-01,\n",
            "        9.0355e-02, 1.4521e-01, 2.5679e-01, 4.8044e-01, 4.8606e-01, 4.3688e-01,\n",
            "        3.9548e-01, 4.3575e-01, 9.3024e-01, 3.1526e-02, 1.0090e-01, 2.6813e-01,\n",
            "        4.5521e-01, 1.9207e-01, 6.6775e-01, 2.4435e-01, 1.8946e-01, 2.1288e-01,\n",
            "        6.0199e-01, 5.1922e-02, 8.6206e-01, 1.1590e-01, 9.5953e-02, 3.0584e-01,\n",
            "        2.8942e-01, 1.9302e-01, 7.9672e-01, 5.9452e-01, 9.1496e-01, 1.4706e-01,\n",
            "        7.6231e-03, 4.4874e-01, 8.3995e-01, 9.9684e-01, 5.1692e-01, 9.5229e-01,\n",
            "        8.5074e-01, 3.1150e-01, 4.0360e-01, 2.8774e-01, 6.5541e-01, 1.0085e-01,\n",
            "        5.3476e-01, 3.1827e-02, 9.7838e-02, 7.7600e-01, 4.5563e-01, 1.1139e-01,\n",
            "        8.2500e-01, 8.5751e-01, 1.4399e-01, 6.8142e-02, 5.3272e-01, 2.0074e-01,\n",
            "        3.2544e-01, 3.4279e-01, 8.9556e-01, 9.2621e-01, 8.3493e-01, 9.2618e-01,\n",
            "        5.4011e-01, 1.4629e-01, 3.7065e-01, 5.2975e-01, 8.8543e-01, 9.0021e-01,\n",
            "        1.5693e-01, 6.6797e-01, 6.5143e-02, 8.5883e-01, 2.0465e-01, 3.2971e-01,\n",
            "        5.0807e-01, 1.8801e-01, 3.9520e-01, 8.2552e-01, 6.0006e-01, 2.6401e-01,\n",
            "        8.0726e-02, 3.1980e-01, 9.2343e-01, 5.4254e-01, 8.3953e-01, 8.2112e-01,\n",
            "        2.9420e-01, 2.5315e-01, 3.8169e-01, 4.4948e-01, 9.8573e-01, 2.6861e-01,\n",
            "        3.5054e-01, 5.3584e-01, 1.1322e-01, 3.0981e-01, 3.3762e-01, 6.6558e-01,\n",
            "        8.0072e-01, 2.9067e-01, 1.6385e-01, 7.8957e-01, 7.4911e-01, 9.1226e-01,\n",
            "        8.3465e-01, 9.9316e-01, 4.0121e-01, 4.4966e-01, 5.4160e-01, 3.6375e-01,\n",
            "        7.0422e-01, 3.0661e-01, 3.8034e-01, 9.5593e-01, 5.9704e-01, 7.9081e-01,\n",
            "        9.4290e-01, 3.1662e-01, 9.7865e-01, 1.6732e-01, 9.8787e-01, 5.9503e-01,\n",
            "        6.6093e-01, 9.4081e-01, 2.7570e-01, 5.6491e-01, 2.0515e-01, 6.1342e-01,\n",
            "        7.8983e-01, 6.6439e-01, 9.6981e-01, 4.2800e-02, 3.8741e-01, 2.5244e-01,\n",
            "        2.3237e-01, 4.2479e-02, 6.8356e-02, 6.2826e-02, 4.0252e-01, 4.7226e-02,\n",
            "        3.2612e-01, 1.5263e-01, 2.5780e-01, 3.4901e-01, 2.2378e-01, 6.6317e-01,\n",
            "        4.6547e-01, 9.5714e-01, 3.1419e-01, 3.9684e-01, 5.3462e-01, 4.8990e-01,\n",
            "        8.3906e-01, 5.9939e-01, 7.5615e-01, 9.2940e-01, 6.0383e-01, 8.4300e-01,\n",
            "        9.8502e-01, 7.8624e-01, 4.4841e-01, 4.2310e-01, 4.1367e-01, 5.1238e-01,\n",
            "        2.1450e-01, 8.1798e-01, 6.1006e-01, 1.1559e-03, 6.5277e-01, 6.0851e-01,\n",
            "        4.5340e-01, 5.9465e-01, 7.0270e-01, 8.2403e-01, 1.7510e-01, 5.6076e-01,\n",
            "        9.5730e-01, 8.0946e-01, 4.7549e-01, 9.4370e-01, 6.2264e-01, 8.9763e-02,\n",
            "        5.9230e-01, 9.1118e-01, 3.8278e-01, 4.5805e-01, 9.2915e-01, 7.9967e-01,\n",
            "        5.3980e-01, 7.9258e-01, 7.7161e-01, 4.4062e-03, 3.8884e-03, 2.5151e-01,\n",
            "        9.9019e-02, 7.5478e-01, 8.8567e-01, 9.3281e-01, 4.0147e-01, 5.9759e-01,\n",
            "        2.2351e-02, 5.4157e-01, 5.0290e-01, 6.3187e-01, 6.2003e-02, 3.3419e-01,\n",
            "        8.5880e-01, 1.0071e-01, 7.1629e-01, 5.3933e-01, 3.3781e-02, 7.4563e-02,\n",
            "        9.5750e-01, 9.3225e-01, 5.1110e-02, 3.7676e-01, 8.4934e-01, 3.2795e-01,\n",
            "        3.9689e-01, 5.3992e-01, 9.3024e-01, 1.4848e-01, 6.7253e-01, 9.2639e-01,\n",
            "        3.9510e-01, 4.9778e-01, 7.0122e-01, 5.3024e-01, 7.3952e-01, 4.2703e-01,\n",
            "        5.9064e-01, 8.6690e-01, 6.0491e-01, 4.9401e-01, 6.4131e-01, 5.8790e-01,\n",
            "        6.9390e-01, 8.3716e-01, 9.1733e-01, 1.3648e-01, 3.9534e-01, 1.3748e-01,\n",
            "        9.1290e-01, 3.6120e-01, 7.7028e-02, 7.5963e-01, 6.0989e-01, 9.1097e-01,\n",
            "        4.5681e-01, 3.9514e-01, 6.1010e-01, 7.8710e-01, 3.0527e-01, 4.9004e-01,\n",
            "        7.0705e-01, 7.9253e-01, 4.1708e-01, 5.8175e-01, 1.5074e-01, 7.8035e-02,\n",
            "        7.7648e-01, 4.1695e-01, 5.3194e-02, 7.7633e-01, 8.6861e-02, 3.3568e-01,\n",
            "        8.0602e-01, 1.4501e-01, 1.7523e-01, 4.7677e-01, 5.8954e-01, 5.4095e-01,\n",
            "        1.3614e-01, 3.9994e-01, 1.0147e-01, 8.1509e-01, 5.2640e-01, 6.5470e-01,\n",
            "        7.0126e-01, 6.3405e-01, 7.8430e-01, 1.6672e-01, 6.5658e-01, 6.7532e-01,\n",
            "        5.4542e-01, 7.4424e-01, 8.3253e-01, 3.9841e-01, 9.3986e-01, 7.2223e-01,\n",
            "        9.9183e-01, 7.9608e-02, 2.0481e-01, 2.2317e-01, 5.9024e-01, 4.0833e-01,\n",
            "        1.2359e-01, 8.3017e-01, 3.0438e-01, 7.8109e-01, 2.5457e-01, 9.6767e-01,\n",
            "        6.2482e-01, 6.8561e-01, 1.4801e-01, 6.8816e-01, 5.1725e-01, 4.1721e-01,\n",
            "        3.8890e-01, 9.8296e-01, 2.7463e-01, 2.1401e-01, 2.2165e-01, 8.7768e-01,\n",
            "        3.3386e-01, 2.6321e-01, 1.8829e-01, 5.5810e-01, 1.2953e-01, 2.1506e-01,\n",
            "        4.3605e-01, 9.4028e-01, 4.7577e-01, 9.0499e-01, 6.7470e-01, 4.6699e-01,\n",
            "        6.4468e-01, 7.9591e-01, 9.0512e-01, 6.8963e-02, 5.0276e-01, 4.8960e-01,\n",
            "        8.2787e-01, 4.5526e-01, 4.1343e-01, 7.0524e-01, 9.4922e-01, 4.2438e-01,\n",
            "        6.1291e-01, 8.3195e-01, 7.5932e-01, 1.1172e-01, 8.2571e-01, 9.5057e-01,\n",
            "        9.8260e-01, 4.0276e-01, 5.6941e-01, 2.0936e-01, 7.2709e-01, 7.3009e-01,\n",
            "        6.6801e-01, 3.0641e-01, 5.6322e-01, 4.5627e-01, 5.9797e-01, 6.5809e-01,\n",
            "        2.7391e-01, 7.7673e-01, 3.0439e-01, 7.8945e-01, 2.3286e-02, 7.3720e-01,\n",
            "        9.1528e-01, 6.8854e-01, 7.2804e-01, 4.2544e-01, 3.5032e-01, 5.2807e-03,\n",
            "        6.9962e-01, 9.3250e-01, 5.3462e-01, 8.6388e-01, 6.4136e-02, 1.5177e-02,\n",
            "        7.8441e-01, 7.8690e-01, 2.2986e-01, 1.4696e-01, 5.9332e-01, 4.5798e-01,\n",
            "        1.5233e-01, 8.7655e-01, 1.2831e-01, 9.7657e-01, 3.1464e-01, 5.1961e-01,\n",
            "        9.2581e-01, 6.0413e-01, 6.2087e-02, 9.9369e-01, 6.4728e-01, 6.6720e-02,\n",
            "        1.6374e-01, 9.5143e-01, 4.3628e-01, 3.6744e-01, 6.8199e-01, 8.3456e-01,\n",
            "        2.8145e-04, 2.4395e-01, 7.0260e-01, 4.3037e-01, 6.8199e-01, 6.8912e-01,\n",
            "        2.1418e-01, 5.2976e-02, 4.6840e-01, 5.1159e-01, 6.4577e-01, 1.1359e-01,\n",
            "        2.7960e-01, 2.1030e-01, 6.9707e-01, 7.4123e-01, 5.6131e-01, 7.6200e-01,\n",
            "        6.0730e-01, 5.5410e-01, 9.1485e-01, 3.6823e-01, 6.3104e-01, 2.0980e-02,\n",
            "        1.4937e-01, 3.0016e-01, 4.5400e-01, 4.9274e-01, 3.2413e-01, 2.6554e-01,\n",
            "        8.0064e-01, 1.3954e-01, 2.3396e-01, 9.9025e-01, 7.0850e-01, 4.7992e-01,\n",
            "        7.7844e-01, 9.6144e-01, 5.5096e-01, 8.1635e-01, 2.6003e-02, 9.4852e-01,\n",
            "        3.6047e-01, 4.9895e-01, 6.5029e-01, 6.0987e-01, 4.9153e-01, 7.8363e-01,\n",
            "        8.0388e-01, 2.1084e-01, 3.6552e-02, 5.1603e-02, 6.6062e-01, 9.3540e-01,\n",
            "        9.2314e-01, 9.7175e-01, 7.3415e-01, 5.7242e-02],\n",
            "       grad_fn=<SplitBackward0>), tensor([8.1942e-01, 3.1401e-01, 3.1109e-01, 4.0815e-01, 2.1152e-01, 6.1225e-01,\n",
            "        4.3915e-02, 2.9397e-01, 9.3081e-02, 6.3740e-01, 1.9449e-01, 2.8269e-01,\n",
            "        7.4363e-01, 7.7672e-01, 7.1377e-02, 2.4672e-01, 4.6589e-01, 4.4077e-01,\n",
            "        2.1140e-01, 2.1191e-01, 3.0180e-01, 7.4534e-01, 5.9943e-01, 4.7238e-01,\n",
            "        2.7245e-01, 3.0406e-01, 4.0134e-01, 7.4137e-01, 2.8036e-01, 3.3633e-01,\n",
            "        8.4182e-01, 3.8614e-01, 6.1706e-01, 3.1556e-01, 2.2426e-01, 3.8365e-01,\n",
            "        5.4819e-01, 5.4291e-01, 3.7485e-01, 1.7689e-01, 2.2969e-01, 1.0352e-02,\n",
            "        3.7976e-02, 5.7495e-01, 3.2612e-01, 6.1156e-02, 4.6218e-02, 9.2012e-01,\n",
            "        6.1709e-01, 6.9310e-01, 4.5285e-01, 1.8608e-01, 2.3415e-01, 7.9094e-01,\n",
            "        6.0755e-01, 5.6154e-01, 6.0969e-01, 6.0889e-01, 1.0455e-01, 7.7858e-01,\n",
            "        2.7437e-01, 7.5105e-01, 3.0336e-01, 8.8920e-01, 3.1221e-01, 2.1271e-01,\n",
            "        4.1528e-02, 6.0878e-01, 2.4391e-01, 9.9026e-01, 2.9241e-01, 2.5894e-01,\n",
            "        8.9365e-01, 3.5383e-01, 3.1183e-01, 5.5782e-01, 8.5380e-01, 9.8194e-01,\n",
            "        5.7999e-01, 5.6905e-01, 8.4136e-01, 4.2400e-01, 5.2574e-01, 8.9924e-01,\n",
            "        4.3440e-01, 6.4611e-01, 7.6326e-01, 8.4076e-01, 2.0682e-02, 2.7918e-01,\n",
            "        9.9284e-01, 7.9635e-01, 8.9413e-01, 8.1707e-01, 7.4600e-01, 9.9780e-01,\n",
            "        2.2104e-01, 9.8315e-01, 7.9860e-01, 2.9025e-01, 5.2899e-01, 2.4795e-01,\n",
            "        1.7367e-01, 6.0551e-01, 3.0183e-01, 9.8460e-01, 2.6066e-01, 4.4122e-01,\n",
            "        1.8511e-01, 2.6645e-01, 5.6763e-01, 2.2458e-01, 5.3297e-01, 3.2489e-01,\n",
            "        8.3283e-01, 5.3192e-01, 3.8372e-01, 9.5792e-01, 1.2493e-01, 2.7034e-01,\n",
            "        3.3743e-01, 7.7513e-01, 2.6529e-01, 3.2847e-01, 9.7787e-01, 1.0526e-01,\n",
            "        9.1846e-01, 3.9922e-01, 5.4309e-01, 6.8549e-02, 7.5810e-01, 9.8886e-01,\n",
            "        2.6806e-01, 2.4970e-01, 5.3686e-01, 3.1873e-01, 6.1855e-01, 8.3141e-01,\n",
            "        4.5922e-01, 4.5866e-01, 3.1086e-01, 7.0829e-01, 2.0767e-01, 9.4075e-01,\n",
            "        1.5295e-01, 7.0881e-01, 2.6967e-01, 7.9207e-01, 5.2411e-01, 5.0588e-01,\n",
            "        1.9960e-01, 7.6285e-01, 6.0723e-02, 7.5160e-01, 3.8084e-01, 4.2926e-01,\n",
            "        8.5039e-01, 5.2507e-03, 4.4602e-01, 7.8799e-01, 4.7682e-01, 4.0935e-01,\n",
            "        8.4444e-01, 3.0840e-01, 1.1045e-01, 9.5477e-01, 4.3528e-01, 7.4326e-01,\n",
            "        6.7825e-02, 9.0381e-02, 7.5859e-01, 3.1277e-01, 3.5237e-02, 4.1030e-01,\n",
            "        4.7248e-01, 2.3829e-01, 3.8736e-01, 5.3541e-01, 8.1889e-01, 3.6260e-01,\n",
            "        6.8909e-01, 2.5719e-01, 2.0136e-02, 5.4675e-01, 5.3516e-01, 6.2483e-01,\n",
            "        1.2145e-01, 7.3665e-01, 3.2583e-01, 5.9200e-01, 6.7594e-01, 4.2857e-01,\n",
            "        8.6960e-01, 9.5054e-01, 2.9735e-01, 4.4579e-01, 9.5967e-01, 4.8047e-01,\n",
            "        5.6047e-01, 5.6743e-01, 8.4939e-01, 2.1586e-01, 6.9431e-01, 9.3792e-01,\n",
            "        4.8968e-02, 4.2335e-01, 9.1384e-01, 5.0208e-01, 4.6110e-01, 6.9424e-01,\n",
            "        2.1435e-01, 5.3053e-01, 2.4853e-01, 5.1385e-01, 1.0778e-01, 4.9417e-01,\n",
            "        5.0148e-01, 2.7749e-01, 8.9948e-02, 6.0377e-01, 6.4637e-01, 2.7465e-02,\n",
            "        8.5690e-01, 1.7894e-01, 6.7288e-01, 6.3953e-01, 7.1929e-02, 5.9529e-01,\n",
            "        4.7356e-01, 2.9936e-01, 6.9007e-01, 7.6871e-01, 7.8547e-04, 4.3827e-01,\n",
            "        7.7811e-01, 8.5026e-01, 6.1092e-01, 2.6488e-01, 3.6425e-01, 1.8145e-01,\n",
            "        9.3552e-02, 5.4541e-01, 1.2871e-01, 4.9755e-01, 7.0176e-01, 8.7044e-01,\n",
            "        1.6638e-01, 8.7460e-01, 6.9639e-01, 3.6112e-01, 8.2907e-01, 5.6978e-01,\n",
            "        4.2307e-01, 2.9004e-01, 7.2491e-01, 7.0383e-01, 2.9170e-02, 5.5470e-01,\n",
            "        5.9510e-01, 6.8264e-01, 4.9824e-02, 8.9058e-02, 3.2847e-01, 7.0426e-01,\n",
            "        7.0031e-01, 3.9110e-01, 9.7467e-01, 5.9110e-01, 7.7314e-01, 9.3406e-01,\n",
            "        5.9323e-01, 4.0005e-01, 9.3537e-01, 5.3711e-01, 2.3619e-01, 8.7194e-01,\n",
            "        1.5052e-01, 4.0107e-01, 8.0055e-01, 1.4259e-01, 6.9369e-01, 5.8775e-01,\n",
            "        7.4355e-02, 9.6787e-01, 9.5835e-01, 1.8773e-01, 7.2314e-01, 8.4837e-01,\n",
            "        7.4021e-01, 7.6633e-02, 8.7874e-01, 8.2450e-01, 6.2566e-01, 9.2044e-01,\n",
            "        4.0861e-01, 6.1036e-01, 9.5244e-01, 9.0967e-01, 6.9492e-01, 5.5109e-01,\n",
            "        6.2939e-01, 5.2165e-01, 6.6304e-01, 8.1268e-01, 1.3898e-01, 4.6986e-01,\n",
            "        1.8864e-01, 3.0566e-01, 3.8591e-01, 5.2737e-01, 7.1086e-01, 3.5724e-01,\n",
            "        5.7889e-01, 6.6445e-01, 1.0942e-02, 8.5667e-01, 6.0215e-02, 3.8451e-01,\n",
            "        6.1449e-01, 1.0051e-01, 1.4973e-01, 2.7407e-01, 6.8087e-01, 4.8671e-01,\n",
            "        1.6304e-01, 4.2864e-01, 4.0439e-01, 4.2044e-01, 1.6076e-01, 9.0226e-01,\n",
            "        9.1943e-01, 9.7330e-01, 1.3611e-01, 8.8844e-02, 3.3355e-01, 2.2071e-01,\n",
            "        3.3010e-01, 7.2566e-01, 9.3769e-01, 4.1234e-01, 2.2105e-01, 3.6834e-01,\n",
            "        9.0096e-01, 7.2058e-01, 8.6178e-01, 1.7263e-01, 9.5717e-01, 7.7930e-01,\n",
            "        2.4841e-01, 7.1569e-01, 6.0755e-01, 7.4143e-01, 6.5494e-01, 1.7247e-01,\n",
            "        3.4829e-01, 1.9729e-01, 9.7201e-01, 8.0118e-01, 1.9616e-01, 8.3700e-01,\n",
            "        7.5286e-01, 4.4051e-01, 9.9348e-01, 5.0545e-01, 6.3572e-01, 4.4094e-01,\n",
            "        5.0467e-01, 2.2596e-01, 5.7698e-01, 4.7382e-01, 9.5501e-01, 1.3071e-01,\n",
            "        1.1861e-01, 9.8553e-01, 3.4619e-01, 8.0057e-02, 1.6467e-01, 7.9617e-01,\n",
            "        1.0655e-01, 8.1153e-01, 5.2453e-01, 4.0805e-01, 9.0087e-01, 9.0961e-01,\n",
            "        7.1939e-01, 3.5565e-01, 8.1334e-01, 1.8546e-01, 3.5815e-01, 9.1256e-01,\n",
            "        5.7250e-02, 1.6592e-01, 1.8341e-01, 3.8759e-01, 7.3827e-01, 8.5957e-01,\n",
            "        2.8934e-01, 1.3308e-01, 2.5802e-01, 6.5307e-01, 6.2526e-01, 7.0563e-01,\n",
            "        9.2602e-01, 7.7428e-02, 5.3321e-01, 6.8397e-01, 1.4239e-01, 8.7015e-01,\n",
            "        6.3195e-02, 7.0283e-01, 3.8680e-01, 2.3484e-01, 3.3540e-02, 5.0812e-01,\n",
            "        3.4828e-01, 5.1972e-01, 6.8838e-01, 1.0375e-01, 9.4712e-01, 8.2690e-01,\n",
            "        5.4452e-01, 9.8581e-01, 2.0013e-01, 9.1565e-01, 1.8387e-01, 6.8870e-01,\n",
            "        8.4548e-01, 3.0786e-01, 4.6680e-01, 9.0712e-01, 7.5272e-01, 5.0682e-03,\n",
            "        9.6231e-01, 3.7595e-01, 6.9716e-01, 3.5839e-01, 7.5137e-01, 8.7021e-01,\n",
            "        4.0417e-01, 3.2294e-01, 5.2121e-01, 2.8966e-01, 8.7136e-01, 7.7014e-01,\n",
            "        7.6524e-01, 4.3211e-01, 1.0529e-01, 1.7217e-01, 8.4095e-01, 3.4537e-01,\n",
            "        2.6724e-01, 6.4405e-01, 3.6752e-01, 3.4264e-01, 2.0669e-01, 4.7332e-01,\n",
            "        6.7989e-01, 9.1327e-01, 6.7789e-01, 4.4775e-01, 6.5314e-01, 7.1681e-01,\n",
            "        2.4233e-01, 4.6904e-01, 5.4864e-01, 5.5450e-02, 2.5275e-01, 7.7485e-01,\n",
            "        1.8590e-01, 9.7598e-01, 5.2178e-01, 3.7227e-02, 6.3884e-01, 3.5145e-01,\n",
            "        8.3481e-01, 1.5026e-01, 1.1662e-01, 4.2167e-01, 5.3384e-01, 8.4290e-01,\n",
            "        8.7841e-01, 6.0548e-01, 2.9529e-01, 5.6956e-01, 5.9789e-01, 8.9377e-01,\n",
            "        6.2081e-01, 7.5758e-01, 2.4535e-01, 3.8686e-01, 4.3014e-01, 2.2265e-01,\n",
            "        4.2752e-01, 6.0901e-01, 8.8093e-01, 8.3406e-02, 3.3545e-01, 1.8969e-02,\n",
            "        5.1148e-01, 3.5639e-01, 8.2457e-01, 1.8861e-01, 6.7359e-01, 5.9328e-01,\n",
            "        8.2993e-01, 9.1377e-01, 4.0450e-01, 4.3236e-01, 4.7063e-01, 8.9827e-01,\n",
            "        6.9807e-01, 2.2694e-01, 5.9863e-01, 4.5296e-01, 1.0708e-01, 1.8322e-01,\n",
            "        7.0397e-01, 8.2928e-01, 2.7816e-01, 9.7234e-01, 1.8313e-01, 9.7368e-02,\n",
            "        5.8901e-01, 1.1971e-01, 7.3233e-01, 6.0157e-01, 8.6459e-01, 3.2743e-01,\n",
            "        5.5074e-01, 4.5571e-02, 2.3544e-01, 2.8710e-01, 6.0931e-01, 1.0930e-01,\n",
            "        6.8500e-01, 4.0683e-01, 5.7471e-01, 7.0478e-01, 7.7834e-01, 3.2125e-01,\n",
            "        4.8368e-01, 4.9093e-01, 2.5871e-01, 8.4766e-01, 4.3189e-01, 5.4810e-01,\n",
            "        7.6723e-01, 1.2253e-01, 9.5015e-01, 5.6708e-01, 6.2279e-01, 7.5553e-02,\n",
            "        1.3776e-01, 7.9112e-01, 5.8730e-01, 4.1415e-01, 3.3517e-01, 4.2212e-01,\n",
            "        4.2322e-01, 3.8426e-01, 5.0909e-02, 2.2890e-01, 6.0175e-01, 9.5702e-01,\n",
            "        8.0280e-01, 2.4401e-02, 7.8247e-01, 2.2973e-01, 7.8333e-01, 2.2679e-01,\n",
            "        6.3120e-02, 5.6033e-01, 3.2049e-01, 9.9059e-01, 7.5734e-02, 5.1892e-01,\n",
            "        9.2760e-01, 2.6506e-01, 6.9597e-01, 9.7255e-01, 7.9237e-01, 3.2229e-01,\n",
            "        1.6256e-01, 9.4693e-01, 3.3925e-01, 2.6971e-01, 8.0450e-01, 6.7524e-01,\n",
            "        7.9424e-01, 8.3664e-01, 6.8372e-01, 8.8755e-01, 6.2400e-01, 8.5087e-01,\n",
            "        9.9811e-01, 5.8456e-01, 9.6950e-01, 5.2001e-01, 5.2177e-02, 7.6532e-01,\n",
            "        8.6686e-01, 9.2357e-01, 3.3992e-01, 3.8181e-01, 8.2523e-01, 1.1404e-01,\n",
            "        8.9706e-01, 2.4240e-01, 4.5691e-01, 6.1030e-01, 7.1622e-01, 1.1834e-01,\n",
            "        9.7916e-02, 1.9815e-01, 5.0707e-01, 5.9938e-01, 7.1448e-01, 2.8691e-01,\n",
            "        6.0695e-01, 8.6320e-01, 8.2463e-01, 9.1253e-01, 8.0813e-01, 4.6505e-01,\n",
            "        6.6878e-01, 5.1410e-01, 5.7607e-01, 8.0810e-01, 2.5208e-01, 3.0944e-01,\n",
            "        9.4499e-01, 2.8530e-01, 8.4488e-01, 9.9434e-01, 4.7051e-01, 1.7336e-01,\n",
            "        2.4754e-01, 9.0201e-01, 2.4157e-01, 8.6863e-01, 3.3418e-01, 8.1722e-01,\n",
            "        3.3577e-01, 7.0485e-01, 6.1624e-01, 6.0336e-01, 2.8544e-01, 3.4701e-01,\n",
            "        1.0769e-01, 9.1405e-01, 2.9948e-01, 7.0124e-01, 7.7934e-02, 4.5055e-01,\n",
            "        5.7826e-01, 3.9875e-01, 8.8868e-01, 9.1319e-02, 2.7561e-01, 3.7142e-01,\n",
            "        7.3866e-01, 2.1593e-01, 4.9352e-02, 8.2595e-01, 8.6507e-01, 7.7973e-01,\n",
            "        9.0787e-01, 9.5311e-01, 8.1276e-01, 3.1967e-02, 5.5458e-01, 1.5953e-01,\n",
            "        6.6907e-01, 4.8242e-01, 7.1684e-01, 3.5771e-01, 8.8120e-01, 8.7827e-01,\n",
            "        1.4520e-01, 3.3341e-01, 7.3972e-01, 4.1926e-01, 6.3835e-02, 6.9245e-01,\n",
            "        9.0908e-01, 9.0118e-01, 6.1293e-01, 9.1490e-01, 7.4155e-01, 1.4591e-01,\n",
            "        3.8766e-02, 3.0987e-01, 2.2486e-02, 5.3345e-01, 7.9323e-01, 8.8959e-01,\n",
            "        9.8367e-01, 5.2412e-01, 2.8778e-01, 3.8680e-01, 8.5742e-02, 7.9130e-01,\n",
            "        5.4161e-01, 2.1538e-01, 9.6916e-01, 9.4082e-01, 8.2018e-01, 8.9786e-02,\n",
            "        6.6615e-01, 8.3884e-01, 4.9853e-01, 5.2472e-02, 1.0870e-01, 2.6749e-01,\n",
            "        6.1427e-02, 1.8893e-01, 5.9481e-01, 9.8759e-01, 9.1263e-01, 8.3381e-01,\n",
            "        7.5545e-01, 5.9034e-01, 1.4290e-01, 5.5901e-02, 8.2978e-01, 7.8900e-01,\n",
            "        6.7823e-01, 1.2632e-01, 8.6546e-01, 6.8915e-01, 7.9793e-01, 8.4894e-01,\n",
            "        9.5056e-02, 6.0570e-01, 2.8517e-02, 7.0533e-01, 5.4230e-02, 7.0091e-01,\n",
            "        5.0560e-01, 8.6476e-01, 2.5614e-01, 2.1679e-02, 2.7764e-01, 3.4180e-01,\n",
            "        9.0799e-01, 7.2025e-02, 7.8237e-01, 6.2093e-01, 1.5000e-01, 1.0004e-01,\n",
            "        7.6612e-01, 2.3471e-01, 2.3474e-01, 9.6512e-01, 4.3333e-02, 9.8480e-01,\n",
            "        3.7118e-01, 6.1392e-01, 5.8415e-01, 5.7907e-01, 4.8105e-02, 9.4868e-02,\n",
            "        6.2885e-01, 9.6289e-01, 9.5332e-01, 2.9815e-01, 1.1925e-01, 6.5174e-01,\n",
            "        9.5051e-01, 2.1570e-01, 2.7181e-01, 6.2334e-01, 3.2973e-01, 2.4643e-01,\n",
            "        6.6245e-01, 2.4060e-01, 8.2943e-01, 2.7008e-01, 1.7925e-01, 9.8874e-01,\n",
            "        9.7114e-01, 5.0123e-01, 2.8961e-01, 2.9672e-01, 6.9066e-03, 2.5642e-01,\n",
            "        5.9100e-01, 3.9635e-01, 1.8820e-01, 3.8678e-01, 7.5275e-01, 2.9350e-01,\n",
            "        7.2786e-02, 4.8006e-01, 7.4672e-02, 8.9562e-01, 7.4551e-01, 7.3719e-01,\n",
            "        2.9483e-02, 9.5301e-01, 2.4445e-01, 6.8256e-01, 7.3143e-01, 7.0606e-01,\n",
            "        3.1658e-03, 7.1487e-01, 4.1060e-01, 1.8038e-01, 3.6702e-01, 6.8454e-01,\n",
            "        2.2642e-01, 7.1070e-01, 6.4528e-01, 2.2101e-01, 2.2859e-01, 2.6806e-01,\n",
            "        6.5265e-02, 6.2050e-01, 7.7027e-02, 1.1473e-01, 4.4158e-01, 9.6661e-01,\n",
            "        7.6980e-01, 6.9648e-01, 2.2712e-01, 6.7681e-01, 3.1889e-01, 8.7702e-01,\n",
            "        6.3607e-01, 1.8601e-01, 3.0703e-03, 7.3579e-01, 7.1762e-01, 3.1522e-01,\n",
            "        3.5752e-01, 4.6690e-01, 1.9975e-01, 4.8481e-01, 9.8933e-01, 5.3206e-01,\n",
            "        9.9664e-01, 4.1432e-01, 5.8814e-01, 9.4828e-01, 3.8053e-01, 6.2244e-03,\n",
            "        3.1853e-01, 7.0703e-01, 7.7382e-01, 9.8907e-01, 6.6859e-01, 5.5164e-01,\n",
            "        9.9305e-01, 4.9459e-01, 4.0362e-01, 9.8095e-02, 4.5438e-01, 5.4321e-01,\n",
            "        4.0440e-01, 5.8296e-01, 5.0086e-01, 5.3604e-01, 7.5218e-01, 1.4268e-01,\n",
            "        9.3857e-01, 1.0126e-01, 4.1396e-01, 6.7120e-01, 1.1597e-02, 8.2973e-01,\n",
            "        5.7726e-01, 8.6955e-02, 3.5882e-01, 2.0672e-01, 7.0080e-01, 5.9466e-01,\n",
            "        2.8547e-01, 4.0914e-01, 5.6646e-01, 4.1028e-01, 4.7995e-01, 9.0437e-01,\n",
            "        9.0645e-01, 1.2931e-01, 9.7210e-01, 1.8205e-01, 2.3417e-01, 4.8044e-02,\n",
            "        6.2039e-01, 1.9015e-01, 2.5363e-01, 1.1410e-01, 7.8142e-01, 8.5987e-01,\n",
            "        2.0762e-01, 5.6225e-01, 2.8960e-01, 5.4135e-01, 9.5550e-01, 9.0033e-01,\n",
            "        8.3690e-01, 2.8155e-01, 8.0956e-01, 4.1579e-01, 4.7855e-01, 4.7403e-01,\n",
            "        7.2823e-01, 3.8521e-01, 3.6123e-02, 6.6275e-01, 6.3736e-01, 1.1058e-01,\n",
            "        3.9829e-01, 5.2475e-01, 8.9324e-01, 8.0418e-01, 2.8247e-02, 9.5563e-02,\n",
            "        1.8805e-01, 8.3979e-01, 1.7585e-01, 3.8585e-01, 1.6785e-01, 5.5347e-01,\n",
            "        8.1318e-01, 7.7982e-02, 2.6209e-02, 9.1125e-01, 7.0350e-01, 3.6137e-01,\n",
            "        8.6876e-02, 9.3342e-01, 2.8026e-01, 5.2689e-01, 1.5718e-01, 5.5978e-01,\n",
            "        4.1545e-02, 6.3537e-01, 7.8354e-01, 2.2119e-01, 3.9651e-01, 4.8584e-01,\n",
            "        2.9108e-01, 5.0122e-01, 1.4453e-01, 5.8067e-01, 6.9679e-01, 4.3325e-01,\n",
            "        6.9621e-01, 4.0921e-01, 5.3608e-01, 6.3213e-01, 7.9363e-01, 4.1944e-01,\n",
            "        7.5721e-01, 8.4026e-01, 6.0090e-01, 5.6489e-01, 9.5940e-01, 4.0318e-02,\n",
            "        1.8440e-01, 3.8689e-01, 1.3417e-01, 1.4064e-01, 6.4850e-01, 3.5973e-01,\n",
            "        1.4726e-01, 4.9494e-01, 8.9885e-01, 5.8328e-01, 6.7712e-01, 7.8929e-01,\n",
            "        3.7313e-01, 8.9363e-01, 4.3809e-01, 2.6606e-01, 8.8008e-01, 7.6389e-01,\n",
            "        6.1753e-01, 9.4476e-01, 4.8615e-01, 6.7064e-01, 8.4115e-02, 6.8894e-01,\n",
            "        1.2269e-01, 4.2583e-01, 5.0831e-01, 9.6680e-01, 1.4530e-01, 6.1615e-01,\n",
            "        4.5003e-01, 6.1731e-01, 4.9754e-01, 9.9957e-01, 8.4894e-01, 2.0097e-01,\n",
            "        3.6756e-01, 8.9480e-01, 5.4997e-01, 1.3049e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([3.5791e-01, 4.4322e-01, 7.6069e-01, 8.4215e-01, 2.2920e-01, 1.9458e-01,\n",
            "        6.4876e-02, 9.6730e-01, 1.7236e-02, 5.7060e-01, 7.6486e-01, 8.2239e-01,\n",
            "        8.3147e-01, 2.4222e-02, 4.3566e-01, 5.4864e-01, 5.6927e-01, 5.4209e-01,\n",
            "        6.3472e-01, 6.5234e-01, 5.8987e-01, 3.3335e-01, 7.5213e-01, 9.5523e-01,\n",
            "        8.1072e-01, 1.1815e-01, 7.8761e-01, 5.5386e-01, 5.6833e-01, 6.2816e-01,\n",
            "        8.0421e-01, 3.5354e-01, 9.7290e-01, 8.4508e-01, 2.7503e-01, 5.6186e-01,\n",
            "        6.0746e-01, 4.9748e-01, 6.1820e-01, 1.6447e-01, 6.6688e-01, 4.0867e-03,\n",
            "        3.4815e-01, 7.2626e-01, 9.5332e-02, 9.1798e-01, 3.3519e-01, 4.4432e-01,\n",
            "        8.1069e-01, 2.5043e-01, 4.4316e-01, 4.3911e-01, 8.0150e-01, 7.6933e-01,\n",
            "        6.9272e-01, 4.4950e-01, 7.8279e-01, 3.9862e-01, 9.9929e-01, 9.1215e-01,\n",
            "        9.8597e-01, 2.8614e-01, 7.7980e-01, 1.8835e-01, 4.7678e-01, 3.5108e-01,\n",
            "        7.0706e-01, 7.3909e-01, 1.7657e-01, 7.0873e-02, 2.7514e-02, 8.3737e-01,\n",
            "        1.3320e-01, 3.3266e-01, 1.6135e-01, 4.0276e-01, 1.4458e-01, 2.4392e-01,\n",
            "        1.4830e-01, 6.1896e-01, 5.6686e-01, 7.1487e-01, 6.0861e-01, 6.8267e-01,\n",
            "        1.8731e-01, 1.5099e-01, 1.4714e-01, 1.2678e-01, 8.8166e-01, 8.9622e-01,\n",
            "        6.0078e-01, 1.0580e-01, 5.6582e-01, 3.6090e-01, 3.6975e-01, 7.4656e-02,\n",
            "        5.2084e-01, 9.1074e-01, 2.5325e-01, 2.4796e-01, 9.9170e-01, 8.4685e-01,\n",
            "        4.5789e-01, 3.4843e-01, 8.9563e-01, 7.7790e-01, 5.9632e-01, 4.3271e-01,\n",
            "        6.5691e-01, 3.5303e-01, 9.3894e-01, 5.6275e-01, 8.5380e-01, 3.2619e-02,\n",
            "        9.7823e-01, 7.8074e-01, 9.1072e-01, 6.0196e-01, 3.9245e-01, 2.1269e-01,\n",
            "        4.3007e-01, 5.4493e-01, 1.4141e-02, 9.6891e-01, 2.8613e-01, 8.7341e-01,\n",
            "        2.1083e-02, 4.1896e-01, 9.9315e-01, 6.0474e-01, 1.2447e-03, 1.0159e-02,\n",
            "        5.1011e-01, 2.4517e-01, 6.7388e-01, 4.7231e-01, 3.7389e-01, 5.6409e-02,\n",
            "        7.9752e-01, 4.4900e-01, 2.9587e-01, 9.7070e-01, 9.0640e-01, 6.2670e-01,\n",
            "        9.2368e-01, 6.6576e-01, 9.7673e-01, 8.0439e-01, 6.5194e-01, 1.3916e-01,\n",
            "        1.0060e-01, 9.1494e-01, 9.6088e-01, 7.3519e-01, 3.8439e-02, 1.8443e-01,\n",
            "        7.5024e-01, 4.0575e-01, 5.5688e-01, 6.5896e-01, 8.4343e-01, 6.0055e-01,\n",
            "        4.4537e-01, 6.2576e-01, 1.0639e-01, 9.9362e-01, 4.2369e-02, 9.7694e-01,\n",
            "        4.0834e-01, 2.1870e-01, 6.4844e-01, 7.7537e-01, 2.1841e-01, 7.8505e-01,\n",
            "        1.1531e-01, 7.2013e-01, 9.3745e-01, 7.0096e-01, 1.1346e-01, 7.2288e-01,\n",
            "        9.6254e-01, 9.1371e-01, 7.9939e-01, 6.1017e-02, 8.7789e-01, 6.1536e-01,\n",
            "        1.7825e-01, 8.5243e-01, 3.1230e-01, 5.5978e-01, 1.1662e-01, 7.6426e-01,\n",
            "        3.2192e-01, 4.4458e-02, 6.2121e-02, 8.7160e-01, 1.8950e-02, 8.3028e-01,\n",
            "        7.3416e-02, 7.9134e-01, 1.5548e-01, 4.7484e-01, 2.3942e-01, 8.3501e-01,\n",
            "        4.6816e-01, 4.2226e-01, 4.4236e-01, 4.4953e-01, 2.0125e-01, 2.2562e-01,\n",
            "        6.0594e-02, 1.4741e-01, 4.8110e-01, 3.0375e-02, 2.7279e-02, 4.8165e-01,\n",
            "        1.9912e-01, 6.0286e-01, 9.2307e-01, 9.5690e-01, 4.1333e-01, 7.6015e-01,\n",
            "        3.8282e-01, 2.8141e-01, 9.7217e-01, 5.8610e-01, 5.1305e-01, 4.0664e-01,\n",
            "        3.9897e-01, 7.5110e-01, 7.1450e-01, 9.8247e-02, 8.8116e-02, 8.1846e-01,\n",
            "        2.8924e-01, 1.1041e-01, 7.4885e-01, 5.2655e-01, 2.6937e-01, 7.7832e-01,\n",
            "        3.8011e-01, 4.0772e-01, 2.5848e-01, 7.2204e-01, 6.1108e-01, 7.6527e-01,\n",
            "        6.7144e-01, 3.8525e-01, 2.7547e-01, 3.6521e-01, 7.9266e-02, 9.0394e-01,\n",
            "        8.8765e-01, 4.9026e-02, 3.8688e-01, 7.6880e-01, 2.8204e-01, 7.1586e-01,\n",
            "        2.7064e-01, 4.6703e-01, 8.3004e-01, 1.7886e-01, 8.3940e-01, 8.1858e-01,\n",
            "        6.6177e-01, 8.4641e-01, 9.2298e-01, 8.0574e-01, 9.8942e-01, 8.9995e-01,\n",
            "        1.1223e-01, 1.7020e-01, 8.6096e-01, 3.7252e-01, 4.5583e-01, 4.4715e-01,\n",
            "        4.7672e-01, 8.9138e-01, 6.3984e-01, 7.2983e-02, 3.3625e-01, 4.4607e-02,\n",
            "        6.7889e-01, 9.2818e-01, 5.0318e-01, 5.5104e-01, 5.6564e-01, 1.5363e-01,\n",
            "        1.4972e-01, 8.1696e-01, 3.2420e-01, 1.8129e-01, 9.6652e-01, 8.5326e-01,\n",
            "        6.0398e-01, 2.9980e-01, 9.8551e-01, 7.0608e-01, 7.9241e-01, 5.3086e-01,\n",
            "        4.7592e-01, 3.7696e-01, 5.6367e-01, 4.1997e-01, 2.8169e-01, 9.5696e-01,\n",
            "        7.5858e-01, 6.7919e-01, 8.9890e-01, 3.0396e-01, 6.8260e-01, 9.7090e-01,\n",
            "        4.0175e-01, 9.6855e-03, 3.0797e-01, 2.5440e-01, 8.7577e-01, 5.0506e-01,\n",
            "        2.3414e-01, 4.7661e-01, 9.1747e-01, 9.3422e-01, 2.6284e-01, 2.5335e-01,\n",
            "        4.7412e-01, 2.7296e-01, 6.7837e-01, 9.1077e-01, 3.0056e-01, 4.4618e-01,\n",
            "        2.0859e-01, 6.5833e-01, 9.5626e-03, 5.1527e-01, 4.6835e-01, 8.1159e-01,\n",
            "        1.0508e-01, 9.1310e-01, 7.3941e-01, 3.2200e-01, 8.6723e-01, 7.3995e-01,\n",
            "        4.7142e-01, 8.4426e-01, 2.0511e-01, 4.4660e-01, 7.7585e-01, 4.4624e-01,\n",
            "        9.8257e-01, 5.5472e-01, 1.2443e-01, 8.4538e-01, 5.5392e-01, 3.5910e-01,\n",
            "        1.8975e-01, 5.7229e-01, 5.7497e-01, 7.1465e-01, 4.2336e-01, 2.4756e-02,\n",
            "        8.3547e-01, 4.4261e-01, 3.1862e-01, 8.2692e-01, 8.4768e-01, 8.6120e-01,\n",
            "        7.8986e-02, 4.5364e-01, 3.2324e-03, 6.4781e-02, 6.5566e-01, 7.2447e-01,\n",
            "        1.7450e-03, 1.4560e-03, 5.6780e-01, 9.8060e-01, 9.5970e-01, 6.9165e-01,\n",
            "        5.9112e-01, 7.5682e-01, 3.7793e-01, 5.6684e-01, 4.4488e-01, 3.3063e-01,\n",
            "        2.7349e-01, 2.4780e-01, 7.6580e-01, 4.3316e-01, 8.0389e-01, 8.5975e-01,\n",
            "        2.4596e-01, 8.3612e-01, 9.6905e-01, 4.5872e-01, 1.2263e-01, 2.8265e-01,\n",
            "        8.0694e-01, 7.2002e-01, 6.9935e-01, 3.4141e-01, 2.3071e-01, 4.8063e-01,\n",
            "        2.0123e-01, 5.6531e-01, 8.0965e-01, 8.3759e-01, 5.4734e-01, 4.4948e-01,\n",
            "        2.8881e-01, 6.6024e-01, 1.7835e-01, 4.1207e-01, 3.2751e-01, 8.0840e-01,\n",
            "        2.8614e-01, 5.1614e-01, 3.2432e-01, 6.0777e-01, 9.3472e-01, 5.1933e-01,\n",
            "        3.3626e-02, 7.2221e-01, 6.7635e-01, 2.7865e-01, 2.0116e-01, 3.0313e-01,\n",
            "        9.2793e-01, 4.1328e-01, 2.9033e-01, 9.6315e-02, 5.8655e-01, 6.3420e-02,\n",
            "        6.4649e-01, 7.4154e-01, 2.6553e-01, 2.6032e-01, 8.8425e-01, 8.1957e-03,\n",
            "        2.0744e-01, 9.2581e-01, 1.9346e-01, 6.4650e-01, 3.8135e-01, 8.1542e-01,\n",
            "        6.1109e-01, 6.1066e-01, 4.6733e-02, 3.4628e-01, 4.9796e-01, 9.5773e-01,\n",
            "        6.1595e-01, 1.7280e-01, 1.7398e-01, 8.3690e-01, 2.6201e-01, 6.1910e-01,\n",
            "        2.4505e-01, 4.4332e-01, 3.2718e-01, 1.0974e-01, 7.4519e-01, 1.3801e-02,\n",
            "        1.4363e-01, 9.5221e-01, 5.1153e-01, 1.2899e-01, 5.0153e-01, 7.4299e-01,\n",
            "        3.2861e-01, 8.9942e-01, 6.8884e-01, 1.9818e-01, 3.3502e-01, 2.9178e-01,\n",
            "        4.7502e-01, 4.1407e-01, 1.4109e-01, 2.3114e-01, 7.6737e-01, 4.1402e-01,\n",
            "        2.4131e-01, 6.6270e-01, 3.2713e-01, 3.9062e-01, 9.3970e-02, 1.5856e-01,\n",
            "        3.1284e-01, 7.3674e-01, 4.1503e-01, 4.3824e-01, 3.2808e-01, 3.3290e-01,\n",
            "        1.5898e-01, 5.2597e-01, 2.7850e-02, 7.9541e-01, 7.6544e-01, 8.2991e-02,\n",
            "        8.3050e-01, 5.0739e-01, 2.1049e-01, 5.8333e-01, 9.1856e-02, 6.2779e-01,\n",
            "        3.6864e-01, 4.4112e-01, 9.8551e-01, 9.1941e-01, 2.4430e-01, 2.2217e-01,\n",
            "        4.9430e-01, 7.1419e-01, 1.0425e-01, 7.0643e-01, 4.3073e-01, 2.9118e-01,\n",
            "        7.7261e-01, 2.7558e-01, 7.9873e-01, 7.3966e-01, 3.1866e-01, 7.9612e-01,\n",
            "        6.2342e-01, 7.4807e-01, 8.6104e-01, 4.4915e-01, 6.9824e-01, 8.2075e-01,\n",
            "        6.6440e-01, 8.1731e-01, 8.7884e-02, 7.4215e-02, 4.6070e-02, 4.8929e-01,\n",
            "        3.8845e-01, 5.8702e-01, 7.8508e-01, 4.5781e-01, 5.7017e-01, 1.8239e-01,\n",
            "        3.0556e-02, 9.3086e-01, 8.6004e-01, 6.8853e-01, 3.4036e-02, 4.1499e-01,\n",
            "        4.6573e-01, 2.1394e-01, 1.1428e-01, 1.7004e-01, 8.3436e-03, 6.8900e-01,\n",
            "        3.2550e-03, 1.8331e-01, 3.8932e-01, 9.4086e-01, 3.2552e-01, 6.5350e-02,\n",
            "        8.7507e-01, 3.8806e-01, 4.8013e-01, 9.9713e-02, 4.3157e-01, 6.5920e-01,\n",
            "        1.1009e-01, 7.1619e-01, 5.7738e-01, 1.6252e-01, 8.1191e-01, 1.7335e-01,\n",
            "        9.5792e-01, 1.3078e-02, 7.7241e-01, 2.1501e-02, 5.5946e-01, 2.1303e-01,\n",
            "        7.0355e-01, 4.0608e-01, 8.8638e-01, 1.3159e-01, 1.2453e-01, 2.1073e-01,\n",
            "        8.3557e-01, 1.5277e-01, 5.8989e-01, 9.1099e-01, 6.2100e-01, 8.7831e-02,\n",
            "        3.2940e-01, 9.3902e-01, 3.1126e-02, 8.2007e-01, 9.9194e-01, 6.7451e-01,\n",
            "        9.1631e-01, 8.6789e-01, 9.6404e-02, 4.7263e-01, 8.8442e-01, 6.0324e-01,\n",
            "        5.8401e-01, 4.6116e-01, 3.1135e-01, 8.4579e-01, 3.6097e-01, 7.5075e-01,\n",
            "        8.4840e-01, 4.4007e-01, 3.8699e-01, 3.7948e-01, 3.8998e-01, 6.4125e-01,\n",
            "        6.8710e-01, 6.7943e-01, 4.4763e-01, 2.2156e-01, 8.8659e-01, 2.8697e-01,\n",
            "        6.1330e-01, 7.8916e-02, 1.6213e-01, 8.4526e-02, 3.7733e-01, 7.5402e-01,\n",
            "        6.5416e-01, 3.7803e-01, 4.7383e-01, 2.8750e-01, 8.4896e-01, 9.6358e-01,\n",
            "        5.5217e-01, 7.5744e-01, 3.9003e-01, 8.3311e-01, 8.4805e-01, 7.3085e-01,\n",
            "        5.6299e-01, 2.0388e-01, 4.5068e-01, 7.0069e-01, 5.0960e-01, 3.5316e-01,\n",
            "        4.3438e-01, 6.4068e-01, 9.6410e-01, 4.8224e-01, 8.7226e-01, 8.2567e-01,\n",
            "        2.6213e-01, 4.7665e-01, 6.6155e-01, 9.9102e-01, 1.5961e-01, 8.4290e-01,\n",
            "        1.4209e-02, 3.2199e-01, 1.8299e-01, 3.8161e-01, 5.7080e-01, 2.9260e-01,\n",
            "        7.8771e-01, 1.5540e-01, 6.1117e-01, 6.1822e-01, 4.0134e-01, 5.4969e-01,\n",
            "        3.4557e-01, 1.8925e-01, 6.9458e-01, 7.5315e-01, 2.6178e-01, 8.1976e-01,\n",
            "        7.7586e-01, 2.9111e-04, 8.7758e-01, 7.5788e-01, 8.3539e-01, 5.9924e-01,\n",
            "        8.9816e-01, 8.7825e-01, 5.2220e-01, 3.8068e-01, 7.7386e-01, 6.3838e-01,\n",
            "        8.7906e-01, 2.4678e-01, 5.8687e-01, 5.5599e-01, 3.6085e-01, 1.1768e-01,\n",
            "        5.1135e-01, 2.0422e-01, 2.0592e-01, 6.9370e-01, 9.3089e-01, 3.7604e-01,\n",
            "        5.3212e-01, 7.9246e-01, 2.8448e-01, 1.3066e-02, 6.6296e-01, 2.3658e-01,\n",
            "        3.9133e-02, 4.6853e-01, 5.6039e-01, 7.1635e-01, 8.7556e-01, 1.4252e-02,\n",
            "        4.8916e-01, 8.0094e-01, 9.0687e-02, 2.0691e-01, 8.7288e-01, 1.8006e-01,\n",
            "        9.1835e-01, 3.1159e-01, 2.3196e-01, 2.0371e-01, 6.6079e-01, 7.9730e-01,\n",
            "        4.4871e-01, 3.4199e-01, 3.8928e-01, 9.1592e-01, 6.2543e-02, 5.0565e-01,\n",
            "        8.5360e-01, 7.3397e-01, 4.0100e-01, 7.9303e-01, 7.3573e-01, 9.5292e-01,\n",
            "        5.5769e-01, 2.6741e-01, 8.2882e-01, 9.1612e-01, 9.8623e-01, 5.6144e-01,\n",
            "        2.3294e-01, 3.9473e-01, 8.5717e-01, 6.4849e-01, 1.1931e-01, 7.4223e-01,\n",
            "        8.1137e-03, 3.8732e-01, 1.8070e-01, 6.6857e-01, 4.8395e-01, 9.7259e-01,\n",
            "        8.9352e-01, 3.5795e-02, 4.9701e-01, 9.8165e-01, 4.0496e-01, 9.3477e-01,\n",
            "        3.4708e-01, 4.7705e-01, 1.6149e-01, 7.2888e-01, 7.9483e-01, 6.6934e-01,\n",
            "        3.3772e-02, 1.1972e-01, 7.2748e-03, 3.1215e-01, 1.9054e-01, 3.9211e-01,\n",
            "        9.1901e-01, 8.2757e-01, 2.5729e-01, 1.2571e-01, 1.1003e-01, 6.5877e-02,\n",
            "        8.9401e-01, 2.2603e-01, 3.4128e-01, 6.6980e-01, 8.6723e-01, 8.6532e-01,\n",
            "        6.4250e-03, 7.7268e-01, 7.8362e-01, 5.0019e-01, 9.4206e-01, 8.0849e-01,\n",
            "        5.9642e-01, 5.9989e-01, 3.3632e-01, 1.9008e-01, 7.4998e-01, 6.4772e-01,\n",
            "        9.6609e-01, 7.5883e-01, 9.9257e-01, 8.2549e-01, 7.1599e-01, 5.5456e-01,\n",
            "        6.3747e-01, 6.7022e-01, 8.5099e-02, 5.5871e-01, 1.3373e-01, 6.1447e-01,\n",
            "        5.7053e-01, 8.5717e-02, 6.4033e-02, 7.2894e-01, 7.6008e-01, 7.4000e-01,\n",
            "        9.2581e-01, 5.5843e-01, 8.2417e-01, 8.1065e-01, 8.5588e-01, 7.5995e-03,\n",
            "        1.3237e-01, 7.4588e-01, 8.2974e-01, 9.6114e-01, 5.0292e-01, 3.2670e-01,\n",
            "        7.7769e-01, 5.4767e-01, 7.7360e-01, 3.0567e-01, 5.1643e-01, 1.1978e-01,\n",
            "        1.1491e-01, 6.6304e-01, 4.8387e-01, 5.6664e-01, 3.4881e-01, 1.4889e-01,\n",
            "        1.2312e-01, 3.8616e-01, 1.0964e-01, 9.9142e-01, 8.5318e-01, 9.5596e-01,\n",
            "        8.3679e-01, 1.4708e-01, 1.6207e-01, 7.6464e-01, 9.5469e-01, 6.3308e-02,\n",
            "        2.1035e-02, 4.4447e-01, 7.5627e-01, 2.6234e-01, 3.2199e-01, 1.7268e-01,\n",
            "        1.3702e-01, 9.3678e-01, 8.4327e-02, 2.1162e-01, 7.2298e-01, 4.3914e-02,\n",
            "        7.1296e-01, 1.5318e-01, 6.4987e-01, 3.5217e-01, 1.0315e-01, 1.3686e-01,\n",
            "        8.5364e-02, 5.3325e-01, 1.2548e-01, 4.3507e-01, 9.4487e-02, 4.0218e-01,\n",
            "        5.1422e-01, 1.0462e-01, 5.4645e-01, 3.8946e-01, 8.1817e-01, 8.0482e-02,\n",
            "        3.6924e-01, 5.3765e-01, 8.0536e-03, 7.2738e-01, 6.6978e-01, 8.0195e-01,\n",
            "        9.1046e-01, 2.6951e-01, 8.5414e-01, 6.9869e-01, 4.3988e-01, 6.9071e-01,\n",
            "        9.8366e-01, 5.3887e-01, 9.4894e-01, 5.0641e-01, 8.8118e-01, 3.9040e-01,\n",
            "        8.3327e-02, 7.4550e-01, 5.9782e-02, 6.2963e-01, 3.8722e-01, 6.4587e-01,\n",
            "        7.7587e-02, 7.6488e-01, 5.9503e-01, 9.5107e-01, 1.8077e-01, 7.3376e-01,\n",
            "        7.6805e-01, 8.4585e-01, 5.8074e-01, 9.0219e-01, 3.9682e-01, 5.4315e-01,\n",
            "        6.8975e-01, 7.9240e-01, 1.6065e-01, 5.6537e-01, 1.6027e-01, 2.5989e-01,\n",
            "        6.4029e-01, 2.8304e-01, 7.6876e-01, 4.9551e-01, 3.7488e-01, 1.1683e-01,\n",
            "        8.7686e-01, 9.5911e-01, 7.5448e-01, 2.5534e-01, 9.5043e-01, 5.8468e-01,\n",
            "        8.2274e-01, 6.8715e-01, 5.9153e-01, 1.5472e-01, 9.3992e-01, 4.2958e-01,\n",
            "        6.7484e-01, 5.7288e-01, 7.9133e-01, 3.1059e-01, 2.1828e-02, 3.9598e-01,\n",
            "        3.9515e-01, 5.2837e-01, 8.6519e-01, 3.4925e-01, 1.3923e-01, 7.1684e-01,\n",
            "        7.7997e-02, 8.1370e-01, 5.6118e-01, 8.3531e-01, 3.9085e-01, 2.8390e-01,\n",
            "        3.0246e-01, 9.1386e-01, 4.8008e-01, 5.9595e-01, 4.6360e-01, 7.1801e-01,\n",
            "        4.5031e-01, 7.3563e-02, 7.2188e-01, 7.8492e-01, 3.3170e-01, 3.7439e-01,\n",
            "        5.0919e-01, 7.3168e-01, 3.3705e-01, 7.2952e-01, 9.9621e-01, 7.1257e-01,\n",
            "        9.7546e-01, 3.2138e-01, 7.4820e-01, 3.7280e-01, 9.9538e-01, 7.8072e-01,\n",
            "        5.3588e-01, 8.4501e-01, 2.6355e-03, 6.6671e-01, 7.7225e-01, 8.1219e-01,\n",
            "        1.7697e-01, 1.9055e-01, 1.1748e-02, 6.7271e-02, 9.7495e-01, 7.8242e-01,\n",
            "        8.8988e-01, 7.8542e-01, 7.1227e-01, 3.8911e-01, 5.7230e-01, 4.2219e-01,\n",
            "        1.5891e-02, 4.6952e-01, 1.6570e-01, 7.3427e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([4.1176e-01, 4.7111e-01, 6.1221e-01, 7.5617e-01, 8.2524e-01, 4.6172e-01,\n",
            "        5.4660e-01, 5.0643e-01, 9.2890e-03, 9.6484e-01, 3.8601e-01, 7.0120e-01,\n",
            "        3.1566e-01, 5.0761e-02, 4.3659e-01, 6.1296e-01, 7.0507e-01, 2.3638e-01,\n",
            "        8.8378e-01, 6.5204e-01, 7.5216e-01, 5.9626e-01, 7.0531e-01, 6.4608e-01,\n",
            "        4.1997e-01, 1.6366e-01, 6.7288e-01, 1.2919e-01, 4.8165e-01, 9.6559e-01,\n",
            "        2.9180e-01, 4.8563e-01, 4.3587e-01, 7.8527e-01, 9.8528e-01, 8.8311e-01,\n",
            "        9.3715e-01, 6.4346e-01, 6.1112e-01, 4.2559e-01, 3.5509e-01, 1.3473e-01,\n",
            "        5.4490e-01, 3.3280e-01, 6.2026e-01, 3.9607e-01, 1.1603e-01, 1.1557e-01,\n",
            "        9.6838e-02, 5.4424e-01, 7.4801e-01, 2.6228e-01, 1.8156e-01, 8.8350e-01,\n",
            "        4.7339e-01, 7.2509e-01, 6.4864e-01, 3.1570e-01, 8.4003e-01, 6.9350e-01,\n",
            "        1.7944e-01, 6.2212e-01, 6.1739e-01, 2.5178e-02, 7.5956e-01, 1.8019e-01,\n",
            "        1.4890e-01, 1.6675e-01, 1.6005e-01, 3.4676e-01, 3.0276e-01, 8.9537e-01,\n",
            "        3.0280e-01, 7.6029e-01, 8.6709e-01, 2.4361e-02, 7.0408e-01, 7.3654e-01,\n",
            "        4.1124e-01, 4.1310e-01, 6.9647e-01, 7.1472e-01, 1.8795e-01, 6.1716e-01,\n",
            "        3.3777e-01, 6.9511e-01, 2.5385e-01, 5.8685e-01, 1.5796e-01, 2.1934e-01,\n",
            "        2.9032e-01, 8.7254e-01, 8.6034e-01, 6.4060e-01, 2.5997e-01, 2.9907e-01,\n",
            "        3.0627e-01, 6.7905e-01, 8.1411e-01, 9.2759e-01, 8.6840e-02, 9.9767e-01,\n",
            "        3.9574e-01, 5.7899e-01, 6.6276e-01, 5.9415e-01, 1.9697e-01, 1.8496e-01,\n",
            "        6.6501e-01, 4.5301e-01, 9.9572e-01, 5.9384e-01, 9.0880e-01, 5.8812e-01,\n",
            "        7.5253e-01, 3.2028e-01, 3.7131e-01, 4.1559e-01, 5.1923e-01, 1.2606e-01,\n",
            "        2.8616e-01, 2.2642e-02, 8.1767e-01, 2.3568e-01, 9.9584e-01, 5.4321e-01,\n",
            "        6.6881e-01, 2.5573e-01, 7.5837e-01, 3.3776e-02, 9.8221e-01, 3.3937e-01,\n",
            "        9.5772e-01, 9.0779e-01, 2.6952e-01, 1.1661e-01, 4.9927e-01, 5.8210e-01,\n",
            "        6.6657e-01, 9.6096e-01, 3.2517e-01, 8.8729e-01, 7.2055e-02, 7.4370e-01,\n",
            "        9.5818e-01, 9.2631e-01, 3.9829e-01, 4.3297e-01, 6.4018e-01, 8.3591e-01,\n",
            "        7.8989e-02, 1.6339e-01, 2.6653e-01, 8.0709e-01, 1.4818e-01, 8.3781e-01,\n",
            "        6.8483e-01, 4.9817e-01, 4.7076e-01, 7.8313e-01, 7.3738e-01, 4.8084e-02,\n",
            "        4.5494e-01, 7.8093e-01, 8.8168e-02, 9.7072e-01, 2.6448e-02, 1.6934e-01,\n",
            "        6.7611e-01, 9.6122e-01, 2.8123e-01, 2.6382e-01, 1.4853e-01, 4.7493e-01,\n",
            "        8.6484e-01, 2.1075e-01, 5.0082e-01, 2.0405e-01, 3.4538e-01, 1.7826e-01,\n",
            "        4.4743e-01, 9.3948e-01, 3.4206e-01, 1.4556e-02, 5.3893e-01, 3.4569e-02,\n",
            "        3.3223e-02, 2.1862e-01, 6.8539e-01, 4.8629e-02, 7.8357e-01, 7.9119e-01,\n",
            "        7.2253e-01, 2.5718e-01, 7.0551e-02, 9.5375e-01, 3.3349e-01, 2.9699e-01,\n",
            "        1.0259e-01, 2.7710e-01, 8.2750e-01, 9.4958e-01, 4.2248e-01, 4.4635e-01,\n",
            "        9.3698e-01, 6.3990e-01, 5.2124e-01, 9.2933e-01, 5.9736e-02, 3.9969e-01,\n",
            "        4.9284e-01, 6.3209e-01, 6.3550e-02, 1.5618e-01, 2.6678e-01, 4.4780e-01,\n",
            "        2.6645e-01, 9.7504e-01, 1.7189e-01, 3.1883e-01, 1.6247e-01, 1.4060e-01,\n",
            "        9.6784e-01, 6.0099e-01, 7.1778e-02, 2.9772e-01, 3.4055e-01, 9.5493e-02,\n",
            "        8.5957e-01, 3.6285e-01, 7.8441e-01, 6.1076e-01, 7.7266e-01, 5.4187e-01,\n",
            "        7.0139e-01, 7.4848e-01, 3.2974e-01, 2.5560e-01, 9.9398e-01, 9.5624e-02,\n",
            "        3.4641e-01, 4.6586e-01, 3.0966e-01, 5.5261e-02, 3.6810e-01, 7.1765e-01,\n",
            "        9.4692e-01, 4.9003e-01, 8.1363e-01, 4.3357e-01, 5.2645e-01, 7.3449e-01,\n",
            "        1.1315e-01, 7.0867e-01, 8.2675e-01, 1.9476e-01, 4.2766e-01, 7.5937e-02,\n",
            "        1.2803e-01, 4.8209e-02, 1.0204e-01, 5.8770e-02, 1.3902e-01, 6.8047e-01,\n",
            "        8.4502e-01, 7.5927e-01, 2.5861e-01, 7.5198e-01, 5.9349e-01, 5.3798e-01,\n",
            "        9.9672e-01, 5.6050e-01, 1.7067e-01, 3.4423e-01, 9.9781e-01, 2.3052e-01,\n",
            "        9.5277e-03, 7.7459e-01, 5.6046e-01, 7.6474e-01, 8.0231e-02, 8.9746e-01,\n",
            "        5.2082e-01, 7.3244e-01, 8.6838e-02, 4.9782e-01, 5.6094e-01, 8.6416e-02,\n",
            "        6.6450e-01, 7.3789e-01, 6.5357e-01, 7.8192e-01, 1.3786e-01, 8.8862e-01,\n",
            "        4.5705e-01, 2.6882e-01, 9.3671e-01, 1.0753e-01, 2.1293e-03, 9.8507e-01,\n",
            "        7.0664e-01, 6.5899e-01, 3.0816e-02, 1.1042e-01, 8.3640e-01, 9.8221e-01,\n",
            "        8.8913e-01, 3.7798e-01, 3.6678e-01, 9.8048e-01, 2.0911e-01, 4.4730e-01,\n",
            "        9.7866e-02, 6.7762e-01, 7.7152e-01, 9.7397e-02, 2.1533e-02, 4.5259e-01,\n",
            "        4.6966e-01, 3.7478e-01, 5.7552e-01, 8.8659e-01, 2.3834e-01, 2.1902e-01,\n",
            "        3.9061e-02, 4.4304e-01, 4.7876e-01, 6.0911e-01, 7.3808e-01, 1.9040e-01,\n",
            "        9.0651e-01, 4.3379e-01, 2.8013e-01, 3.1962e-01, 6.6115e-01, 9.0351e-01,\n",
            "        1.0384e-01, 9.9098e-01, 4.2424e-01, 3.1154e-01, 7.0409e-01, 3.5978e-01,\n",
            "        7.1527e-01, 6.5013e-01, 6.0692e-01, 8.9583e-01, 5.2446e-01, 6.9710e-01,\n",
            "        8.1030e-01, 2.3181e-01, 1.1654e-01, 8.5067e-01, 4.4278e-01, 1.3988e-01,\n",
            "        8.7556e-01, 8.5456e-01, 4.4359e-01, 1.9541e-01, 7.2042e-01, 6.6831e-01,\n",
            "        2.1595e-01, 3.6989e-01, 7.7058e-01, 9.1233e-02, 5.0328e-01, 7.7426e-01,\n",
            "        7.8217e-01, 1.4610e-01, 5.6831e-02, 1.3163e-01, 7.9621e-01, 6.4390e-01,\n",
            "        1.5883e-01, 6.7180e-01, 2.8358e-02, 9.7309e-02, 9.0225e-01, 7.0338e-01,\n",
            "        3.4666e-01, 9.9100e-01, 4.4421e-01, 9.3217e-01, 3.7391e-01, 6.0169e-01,\n",
            "        6.4526e-01, 8.7608e-01, 4.8000e-01, 2.6464e-01, 6.4578e-02, 9.3900e-01,\n",
            "        5.2632e-01, 1.3265e-01, 5.9254e-01, 9.0027e-01, 5.4587e-01, 4.0038e-01,\n",
            "        5.9960e-01, 2.2511e-01, 3.9757e-01, 5.8877e-01, 3.6927e-01, 6.2703e-01,\n",
            "        5.3174e-02, 6.6921e-01, 6.9326e-01, 9.8670e-01, 4.5305e-01, 9.5628e-01,\n",
            "        8.9777e-01, 7.2377e-01, 8.9883e-01, 4.0902e-01, 4.3339e-01, 8.2120e-02,\n",
            "        6.8416e-01, 5.7043e-01, 7.9529e-01, 3.9204e-01, 5.8304e-01, 8.5056e-01,\n",
            "        1.2531e-01, 3.9186e-01, 4.8888e-01, 1.1172e-01, 5.3030e-01, 1.2791e-01,\n",
            "        8.8463e-01, 8.7329e-01, 6.6012e-01, 4.1536e-01, 3.2833e-01, 2.7738e-02,\n",
            "        6.1181e-01, 9.9637e-01, 1.9294e-01, 3.4262e-02, 9.6602e-01, 3.2164e-01,\n",
            "        9.4205e-01, 3.5587e-01, 7.8573e-02, 5.4380e-01, 4.0690e-01, 8.3136e-01,\n",
            "        8.0184e-02, 8.5464e-01, 6.8134e-01, 5.5365e-01, 2.3592e-01, 7.1643e-01,\n",
            "        5.0872e-01, 5.0180e-02, 2.6664e-02, 2.2078e-01, 3.6647e-01, 1.1317e-01,\n",
            "        2.9895e-02, 3.3911e-01, 2.7011e-01, 6.2999e-01, 6.4908e-01, 6.0907e-01,\n",
            "        5.0030e-01, 6.4951e-01, 5.9199e-01, 5.5592e-01, 4.5824e-01, 2.5427e-01,\n",
            "        2.1077e-01, 4.3344e-01, 1.2017e-01, 1.0131e-01, 4.6821e-01, 6.1572e-01,\n",
            "        1.6431e-01, 6.1454e-01, 8.9742e-01, 2.9798e-01, 7.1308e-01, 2.9242e-01,\n",
            "        6.1142e-01, 3.3822e-01, 2.1838e-01, 9.6824e-01, 4.2968e-01, 3.5619e-01,\n",
            "        6.0720e-01, 9.7862e-01, 3.3254e-01, 3.2334e-01, 8.5261e-01, 6.4878e-01,\n",
            "        1.1342e-01, 1.7011e-02, 3.9550e-01, 3.2928e-01, 7.4310e-01, 7.6908e-01,\n",
            "        5.8139e-02, 8.4741e-02, 3.0665e-01, 8.7211e-01, 2.1454e-01, 8.1233e-01,\n",
            "        4.7377e-01, 4.9289e-01, 5.0320e-01, 2.2051e-01, 2.7219e-01, 4.4033e-01,\n",
            "        9.2459e-02, 1.0178e-01, 4.0656e-01, 5.3412e-01, 9.6886e-01, 7.3080e-01,\n",
            "        7.2140e-01, 2.8031e-01, 6.7894e-01, 8.5652e-01, 3.7428e-01, 2.5437e-01,\n",
            "        3.0042e-01, 3.7251e-01, 5.4845e-01, 7.3211e-01, 3.1053e-01, 9.7434e-01,\n",
            "        1.6603e-01, 7.6696e-01, 9.9762e-01, 5.3996e-01, 4.7868e-02, 9.3609e-01,\n",
            "        9.8672e-01, 6.2750e-01, 7.0937e-01, 4.0642e-01, 9.2880e-01, 5.8822e-01,\n",
            "        9.5305e-01, 4.0546e-01, 9.7665e-01, 7.8160e-01, 9.7974e-02, 4.6841e-01,\n",
            "        8.9046e-01, 7.8338e-01, 3.6755e-01, 6.4923e-01, 9.6485e-01, 8.4944e-01,\n",
            "        9.1939e-01, 7.3093e-01, 6.4878e-01, 1.4402e-01, 4.4817e-01, 5.8442e-01,\n",
            "        1.3046e-01, 4.3783e-01, 1.0615e-01, 1.4737e-01, 2.4214e-01, 7.4926e-01,\n",
            "        1.8570e-02, 7.2961e-01, 9.7373e-01, 2.4954e-01, 5.3259e-01, 7.2689e-01,\n",
            "        9.6731e-02, 5.1414e-01, 5.0008e-01, 8.7397e-01, 1.1619e-01, 9.5082e-01,\n",
            "        9.8874e-01, 7.4803e-01, 9.0783e-02, 2.9121e-01, 3.1548e-01, 7.6226e-01,\n",
            "        9.2575e-01, 1.4191e-01, 5.5350e-01, 2.1470e-01, 8.4225e-01, 5.3334e-01,\n",
            "        7.6821e-01, 3.9702e-02, 8.2251e-01, 2.3811e-01, 5.3419e-01, 9.7264e-01,\n",
            "        1.1001e-01, 5.6248e-02, 5.3777e-01, 2.6837e-01, 2.8439e-01, 4.0840e-01,\n",
            "        2.0553e-01, 9.7549e-01, 1.4786e-02, 3.8508e-01, 5.6497e-01, 5.5942e-01,\n",
            "        1.7653e-01, 2.8705e-01, 4.2106e-01, 7.1013e-01, 6.0811e-01, 6.9770e-01,\n",
            "        8.3124e-01, 4.9936e-01, 1.6752e-01, 4.4235e-01, 6.5087e-01, 7.1064e-01,\n",
            "        1.3957e-01, 6.1910e-01, 4.8865e-01, 5.0070e-01, 1.8734e-01, 2.1991e-01,\n",
            "        7.1179e-01, 5.4774e-01, 1.4355e-01, 2.1172e-01, 7.9534e-01, 2.3601e-01,\n",
            "        8.4828e-02, 3.6661e-01, 2.8578e-01, 9.6489e-01, 7.9207e-01, 2.3466e-02,\n",
            "        3.5952e-01, 5.8050e-01, 2.2057e-02, 2.9976e-01, 1.2489e-01, 9.8413e-02,\n",
            "        3.9957e-01, 3.6980e-01, 1.1241e-01, 3.4885e-01, 5.0133e-04, 8.9222e-02,\n",
            "        6.9766e-01, 3.3607e-01, 1.7378e-01, 7.9527e-01, 6.7998e-01, 5.3569e-01,\n",
            "        9.5308e-01, 9.5548e-03, 1.0541e-01, 2.6145e-01, 6.7876e-01, 1.3769e-01,\n",
            "        4.8095e-01, 5.5398e-01, 3.8095e-01, 9.2239e-01, 3.0719e-01, 1.8122e-01,\n",
            "        3.5943e-01, 6.9187e-01, 5.4799e-01, 6.0056e-01, 9.9286e-01, 8.2729e-01,\n",
            "        9.2798e-01, 8.7875e-01, 3.8334e-01, 5.1158e-01, 1.4786e-02, 1.1710e-01,\n",
            "        7.7332e-01, 1.4158e-01, 5.0133e-01, 9.0288e-01, 5.0008e-01, 1.9625e-01,\n",
            "        1.1755e-01, 3.9881e-01, 7.2153e-01, 4.8373e-01, 6.2349e-01, 7.0011e-01,\n",
            "        9.7604e-01, 1.1451e-01, 9.3385e-01, 9.2617e-01, 5.2674e-01, 2.0275e-01,\n",
            "        7.9443e-01, 2.5652e-01, 2.0442e-01, 8.7108e-01, 4.4108e-01, 9.9458e-01,\n",
            "        7.8940e-01, 6.2355e-01, 6.4937e-01, 6.9670e-01, 7.0194e-01, 8.4891e-01,\n",
            "        5.8037e-01, 1.5947e-01, 6.1642e-01, 7.8971e-01, 2.4581e-01, 4.3983e-03,\n",
            "        5.3431e-01, 7.4443e-01, 4.5540e-01, 4.0843e-01, 3.4353e-01, 3.9619e-01,\n",
            "        1.3028e-01, 5.7148e-01, 8.3716e-01, 1.5861e-01, 3.5003e-01, 9.5395e-01,\n",
            "        8.8736e-01, 1.5098e-01, 6.2525e-01, 8.0553e-01, 8.0230e-01, 5.4522e-01,\n",
            "        2.2516e-01, 8.2508e-02, 5.4092e-01, 8.1306e-01, 8.2358e-01, 9.6155e-01,\n",
            "        3.8145e-01, 6.2424e-01, 6.9973e-01, 7.5222e-01, 8.8050e-01, 5.9815e-01,\n",
            "        8.9768e-01, 6.5608e-01, 1.5967e-01, 5.4233e-02, 6.8940e-01, 3.8781e-01,\n",
            "        4.7373e-01, 5.5297e-02, 7.4675e-02, 9.2899e-01, 5.5134e-03, 9.0680e-01,\n",
            "        5.6707e-01, 6.8128e-01, 7.0888e-02, 3.9820e-01, 6.7375e-01, 1.2435e-01,\n",
            "        4.6634e-01, 9.8101e-01, 2.8961e-01, 6.1228e-01, 4.8052e-01, 9.4545e-01,\n",
            "        1.9613e-02, 6.4734e-01, 4.5699e-01, 1.1220e-01, 8.8422e-01, 9.9545e-02,\n",
            "        7.0561e-01, 4.9789e-01, 9.8430e-01, 5.3080e-01, 8.1925e-01, 4.5137e-01,\n",
            "        3.1450e-01, 7.7232e-01, 9.9942e-01, 4.3205e-01, 1.3084e-01, 5.8161e-01,\n",
            "        5.9549e-02, 3.8364e-01, 5.4987e-01, 1.8233e-01, 7.8238e-01, 8.6673e-01,\n",
            "        3.1585e-01, 2.4546e-01, 9.8968e-01, 2.0697e-01, 4.2385e-01, 4.9367e-01,\n",
            "        6.9654e-01, 2.2758e-01, 8.4551e-01, 4.9379e-01, 3.8260e-01, 5.3348e-01,\n",
            "        2.5174e-01, 5.9503e-01, 4.0405e-01, 5.7544e-01, 9.0409e-02, 3.9416e-01,\n",
            "        6.8971e-01, 9.5458e-01, 4.9715e-02, 8.2569e-01, 9.7258e-01, 3.5890e-01,\n",
            "        8.0857e-01, 5.7967e-01, 5.9422e-01, 8.6373e-01, 7.8591e-01, 3.5280e-04,\n",
            "        3.1010e-02, 9.3309e-01, 7.5533e-01, 8.1146e-02, 6.9108e-01, 9.3922e-01,\n",
            "        5.5863e-01, 4.5315e-01, 3.6493e-01, 6.2807e-02, 9.8852e-01, 8.2808e-01,\n",
            "        9.9861e-01, 5.5655e-01, 2.0684e-01, 5.7887e-02, 2.6247e-01, 5.2571e-01,\n",
            "        5.1034e-01, 6.5041e-01, 7.5294e-01, 7.5400e-01, 7.8405e-01, 7.1341e-01,\n",
            "        5.5201e-01, 6.1760e-01, 2.4329e-01, 8.6952e-01, 2.7706e-01, 5.7557e-02,\n",
            "        2.8474e-01, 8.2309e-01, 1.9683e-01, 1.5542e-01, 4.4242e-01, 4.5513e-02,\n",
            "        9.7975e-01, 5.2502e-01, 3.7989e-02, 3.9481e-01, 3.9125e-01, 9.0085e-01,\n",
            "        4.1428e-01, 2.4999e-01, 3.0752e-01, 3.8549e-02, 6.4691e-02, 5.6195e-01,\n",
            "        6.9719e-01, 7.4693e-01, 2.8163e-03, 5.2627e-01, 6.8945e-01, 4.1219e-01,\n",
            "        4.2714e-01, 8.9255e-01, 8.0502e-01, 2.6154e-02, 5.5790e-01, 5.1280e-01,\n",
            "        7.3870e-01, 9.8857e-01, 9.1793e-01, 8.1419e-01, 8.0390e-01, 2.5065e-01,\n",
            "        5.0305e-01, 3.4717e-01, 3.4348e-01, 6.7810e-01, 9.0269e-02, 5.6055e-01,\n",
            "        2.2101e-01, 2.0662e-01, 1.6286e-01, 1.0934e-01, 6.1126e-01, 8.7952e-01,\n",
            "        4.0078e-01, 7.1126e-01, 9.3119e-02, 8.8130e-01, 3.9611e-01, 7.4742e-02,\n",
            "        4.7846e-01, 8.1777e-01, 6.0227e-01, 9.9239e-01, 8.5905e-01, 1.6362e-01,\n",
            "        1.5582e-01, 2.0692e-01, 9.9337e-01, 4.5638e-01, 5.2625e-01, 3.5151e-01,\n",
            "        3.3015e-01, 1.1580e-01, 4.1555e-01, 9.5639e-01, 5.7321e-02, 9.7462e-02,\n",
            "        5.6112e-01, 7.3753e-01, 4.1983e-01, 1.8582e-01, 5.9980e-02, 9.5052e-01,\n",
            "        2.1143e-01, 3.9912e-01, 3.4385e-01, 2.8668e-01, 9.8510e-01, 6.6304e-01,\n",
            "        6.5833e-01, 9.4171e-01, 4.1859e-02, 7.4471e-01, 9.6206e-01, 3.0546e-01,\n",
            "        2.3975e-02, 2.8542e-01, 3.0947e-02, 5.8526e-01, 7.7179e-01, 9.5529e-01,\n",
            "        1.2960e-01, 6.6605e-01, 3.0656e-01, 8.0588e-01, 5.4569e-03, 5.6601e-02,\n",
            "        1.7138e-01, 2.9052e-01, 5.2093e-01, 6.5298e-01, 6.2003e-01, 8.0786e-01,\n",
            "        4.1872e-01, 6.5176e-01, 3.0359e-01, 1.2780e-01, 2.5523e-01, 1.1213e-01,\n",
            "        6.6710e-01, 1.1326e-01, 4.7483e-01, 6.5156e-02, 7.3432e-01, 5.6728e-01,\n",
            "        7.1929e-01, 3.7904e-01, 4.6645e-01, 1.9205e-01, 8.1177e-01, 3.0552e-01,\n",
            "        1.4345e-01, 3.9826e-01, 3.5606e-01, 7.1177e-01, 5.6170e-01, 4.6819e-01,\n",
            "        3.5522e-01, 9.8736e-01, 8.6911e-01, 1.3739e-01, 2.5825e-01, 3.2358e-01,\n",
            "        8.3683e-01, 5.1990e-01, 5.7172e-01, 5.7266e-01, 4.9539e-01, 1.7864e-01,\n",
            "        7.4153e-01, 1.6955e-01, 1.9185e-01, 6.9730e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([0.2088, 0.6788, 0.6294, 0.9407, 0.2484, 0.9137, 0.5921, 0.6786, 0.1993,\n",
            "        0.8559, 0.5428, 0.4975, 0.3647, 0.6132, 0.4496, 0.8710, 0.9599, 0.5073,\n",
            "        0.2419, 0.9967, 0.3300, 0.7506, 0.6922, 0.9542, 0.3384, 0.3729, 0.7201,\n",
            "        0.0213, 0.6488, 0.9134, 0.7726, 0.5074, 0.2150, 0.9597, 0.1259, 0.4904,\n",
            "        0.3130, 0.4137, 0.9386, 0.6795, 0.0903, 0.1380, 0.6641, 0.8143, 0.9743,\n",
            "        0.5557, 0.9668, 0.5982, 0.6488, 0.2536, 0.8034, 0.1547, 0.9246, 0.0099,\n",
            "        0.8380, 0.7830, 0.8840, 0.6772, 0.8009, 0.2057, 0.5828, 0.9046, 0.3133,\n",
            "        0.3131, 0.5068, 0.0548, 0.4894, 0.8981, 0.0806, 0.6552, 0.5513, 0.8862,\n",
            "        0.8611, 0.7639, 0.8372, 0.0576, 0.1913, 0.9262, 0.4054, 0.2085, 0.3750,\n",
            "        0.9580, 0.6074, 0.0343, 0.2794, 0.7524, 0.9013, 0.6076, 0.6262, 0.9134,\n",
            "        0.4914, 0.3548, 0.4897, 0.9562, 0.8693, 0.3444, 0.6128, 0.6626, 0.3943,\n",
            "        0.8639, 0.8037, 0.0440, 0.4530, 0.8033, 0.3951, 0.3975, 0.8059, 0.5230,\n",
            "        0.5605, 0.6310, 0.3393, 0.2623, 0.8299, 0.2386, 0.2712, 0.2561, 0.4553,\n",
            "        0.7716, 0.1517, 0.1561, 0.1827, 0.2762, 0.8910, 0.6629, 0.1853, 0.6056,\n",
            "        0.7210, 0.7110, 0.0282, 0.4370, 0.1288, 0.6204, 0.0975, 0.4414, 0.1585,\n",
            "        0.4269, 0.5858, 0.0036, 0.2355, 0.3337, 0.2796, 0.5147, 0.3975, 0.9552,\n",
            "        0.7803, 0.7989, 0.3963, 0.6688, 0.3135, 0.5808, 0.6215, 0.1976, 0.9704,\n",
            "        0.8260, 0.8888, 0.2079, 0.5294, 0.9026, 0.0302, 0.1304, 0.7615, 0.9967,\n",
            "        0.2437, 0.5592, 0.2971, 0.4614, 0.4440, 0.0283, 0.6517, 0.2495, 0.4580,\n",
            "        0.7285, 0.0306, 0.5534, 0.6527, 0.2059, 0.7963, 0.6686, 0.1005, 0.4447,\n",
            "        0.9574, 0.7742, 0.1917, 0.3778, 0.6553, 0.5767, 0.3570, 0.7577, 0.6122,\n",
            "        0.8698, 0.8943, 0.9571, 0.2049, 0.2299, 0.3386, 0.5144, 0.9480, 0.1232,\n",
            "        0.9353, 0.3998, 0.2868, 0.7708, 0.9866, 0.1822, 0.2622, 0.6535, 0.5694,\n",
            "        0.3630, 0.0068, 0.1341, 0.3731, 0.4544, 0.8027, 0.0590, 0.7282, 0.2161,\n",
            "        0.0017, 0.4424, 0.7761, 0.0500, 0.8976, 0.1234, 0.8292, 0.7164, 0.1183,\n",
            "        0.4139, 0.2075, 0.0075, 0.5441, 0.2907, 0.4377, 0.0533, 0.0035, 0.5454,\n",
            "        0.2254, 0.6542, 0.0444, 0.2642, 0.2635, 0.3973, 0.7316, 0.3510, 0.7336,\n",
            "        0.0520, 0.5195, 0.4266, 0.2403, 0.7898, 0.0241, 0.0394, 0.7146, 0.4588,\n",
            "        0.6542, 0.9084, 0.0472, 0.4158, 0.1525, 0.6741, 0.1179, 0.0016, 0.7455,\n",
            "        0.8161, 0.6126, 0.1616, 0.0299, 0.3044, 0.6109, 0.0911, 0.6867, 0.7732,\n",
            "        0.6918, 0.9065, 0.4588, 0.2754, 0.2586, 0.0764, 0.5162, 0.2776, 0.8743,\n",
            "        0.5905, 0.4716, 0.6381, 0.9365, 0.1568, 0.4342, 0.8263, 0.9697, 0.4949,\n",
            "        0.7530, 0.6475, 0.4728, 0.0236, 0.8690, 0.6357, 0.5098, 0.7655, 0.0226,\n",
            "        0.6465, 0.0781, 0.3130, 0.1108, 0.9618, 0.7219, 0.3435, 0.9871, 0.2548,\n",
            "        0.5811, 0.2130, 0.5296, 0.7547, 0.4302, 0.9817, 0.9680, 0.4879, 0.1114,\n",
            "        0.2276, 0.8178, 0.7553, 0.1679, 0.2150, 0.3245, 0.3360, 0.7105, 0.1372,\n",
            "        0.7920, 0.0261, 0.9042, 0.6740, 0.3492, 0.6524, 0.0712, 0.7569, 0.2183,\n",
            "        0.3797, 0.3863, 0.7070, 0.5257, 0.7820, 0.5192, 0.8368, 0.5798, 0.1215,\n",
            "        0.4234, 0.9389, 0.7821, 0.7050, 0.8442, 0.2435, 0.4140, 0.5643, 0.8608,\n",
            "        0.8147, 0.4604, 0.5167, 0.1842, 0.2031, 0.1603, 0.0362, 0.6607, 0.1678,\n",
            "        0.2333, 0.0524, 0.7835, 0.1560, 0.5175, 0.0948, 0.3205, 0.7147, 0.6516,\n",
            "        0.6987, 0.3242, 0.4710, 0.2595, 0.4772, 0.4413, 0.6454, 0.0923, 0.6768,\n",
            "        0.7691, 0.9612, 0.6205, 0.6580, 0.1174, 0.0241, 0.2971, 0.9131, 0.6278,\n",
            "        0.7539, 0.7568, 0.0822, 0.3095, 0.7856, 0.2722, 0.4435, 0.3251, 0.4959,\n",
            "        0.0638, 0.1247, 0.4132, 0.9603, 0.9046, 0.9630, 0.0236, 0.4490, 0.3896,\n",
            "        0.9383, 0.5945, 0.6869, 0.9857, 0.6917, 0.6797, 0.0618, 0.5555, 0.9499,\n",
            "        0.6274, 0.5157, 0.6185, 0.1410, 0.1021, 0.3555, 0.6810, 0.9575, 0.7267,\n",
            "        0.2349, 0.5253, 0.8384, 0.8273, 0.4010, 0.2566, 0.6079, 0.9823, 0.2350,\n",
            "        0.2776, 0.3575, 0.9624, 0.0158, 0.4119, 0.5358, 0.8911, 0.5379, 0.4255,\n",
            "        0.6998, 0.5529, 0.4598, 0.0389, 0.8994, 0.2900, 0.9628, 0.4633, 0.5178,\n",
            "        0.6930, 0.2665, 0.2616, 0.9733, 0.5070, 0.3842, 0.3712, 0.6976, 0.9834,\n",
            "        0.4206, 0.7462, 0.5209, 0.5420, 0.5278, 0.9976, 0.8693, 0.5776, 0.4392,\n",
            "        0.3364, 0.8763, 0.7334, 0.4604, 0.2224, 0.1527, 0.4759, 0.7850, 0.9374,\n",
            "        0.3778, 0.7747, 0.3128, 0.5046, 0.6281, 0.5387, 0.5485, 0.8001, 0.6052,\n",
            "        0.5840, 0.2536, 0.1394, 0.7059, 0.0140, 0.2811, 0.3533, 0.4345, 0.7530,\n",
            "        0.3920, 0.9397, 0.4531, 0.8333, 0.7093, 0.0555, 0.8646, 0.8718, 0.2510,\n",
            "        0.1480, 0.6187, 0.8558, 0.4991, 0.7559, 0.0859, 0.1079, 0.5615, 0.4837,\n",
            "        0.7559, 0.1576, 0.9190, 0.9445, 0.1165, 0.5413, 0.8674, 0.5299, 0.8632,\n",
            "        0.0936, 0.2314, 0.3658, 0.8675, 0.0108, 0.4923, 0.6881, 0.0775, 0.6868,\n",
            "        0.0685, 0.8000, 0.1142, 0.1943, 0.3415, 0.4832, 0.8988, 0.7795, 0.3361,\n",
            "        0.7937, 0.5856, 0.0689, 0.3675, 0.3343, 0.0776, 0.1441, 0.6523, 0.8607,\n",
            "        0.5418, 0.5905, 0.8142, 0.2262, 0.2442, 0.7668, 0.9968, 0.5856, 0.2622,\n",
            "        0.2585, 0.8416, 0.7788, 0.7096, 0.1507, 0.0855, 0.8960, 0.3507, 0.7766,\n",
            "        0.0057, 0.9833, 0.4637, 0.8664, 0.0579, 0.1367, 0.4807, 0.2684, 0.8397,\n",
            "        0.4314, 0.3953, 0.2555, 0.6437, 0.3482, 0.8908, 0.8528, 0.5638, 0.8183,\n",
            "        0.8237, 0.8329, 0.2502, 0.9394, 0.3090, 0.9453, 0.8613, 0.2032, 0.3387,\n",
            "        0.2225, 0.1042, 0.5137, 0.3197, 0.1009, 0.7991, 0.2248, 0.7210, 0.8783,\n",
            "        0.4863, 0.5568, 0.0957, 0.9482, 0.6926, 0.0968, 0.8285, 0.1315, 0.1482,\n",
            "        0.2239, 0.5286, 0.1601, 0.7659, 0.3701, 0.6441, 0.1683, 0.9674, 0.3447,\n",
            "        0.1519, 0.0845, 0.6144, 0.6471, 0.6979, 0.5835, 0.6592, 0.0918, 0.1095,\n",
            "        0.0721, 0.5574, 0.0593, 0.0488, 0.8586, 0.1750, 0.5952, 0.8904, 0.6086,\n",
            "        0.4422, 0.9358, 0.8585, 0.7249, 0.8229, 0.8058, 0.3080, 0.9726, 0.8094,\n",
            "        0.5533, 0.7804, 0.6552, 0.4847, 0.6915, 0.5884, 0.4436, 0.7248, 0.2492,\n",
            "        0.7496, 0.4188, 0.7474, 0.2393, 0.6347, 0.5051, 0.7714, 0.3596, 0.2727,\n",
            "        0.0399, 0.8537, 0.4051, 0.1666, 0.6272, 0.5643, 0.9792, 0.2983, 0.3325,\n",
            "        0.1680, 0.1333, 0.9474, 0.4208, 0.5225, 0.0133, 0.7314, 0.3293, 0.9753,\n",
            "        0.1277, 0.7803, 0.9180, 0.7146, 0.4901, 0.3136, 0.7763, 0.8301, 0.3515,\n",
            "        0.0244, 0.6003, 0.6282, 0.1761, 0.5266, 0.1362, 0.6563, 0.0373, 0.5373,\n",
            "        0.0637, 0.9266, 0.8679, 0.2511, 0.3195, 0.3266, 0.0147, 0.4459, 0.3030,\n",
            "        0.3843, 0.0248, 0.2770, 0.5444, 0.4098, 0.5160, 0.8020, 0.8228, 0.9772,\n",
            "        0.4197, 0.6653, 0.5068, 0.5928, 0.9147, 0.2241, 0.3132, 0.6205, 0.1874,\n",
            "        0.0979, 0.5599, 0.9235, 0.5213, 0.2447, 0.9686, 0.3248, 0.1692, 0.6584,\n",
            "        0.1661, 0.4317, 0.3896, 0.7693, 0.3739, 0.9347, 0.5748, 0.9586, 0.9916,\n",
            "        0.9409, 0.5954, 0.4055, 0.4213, 0.4757, 0.0090, 0.9112, 0.6159, 0.4783,\n",
            "        0.8852, 0.8873, 0.3553, 0.4467, 0.6519, 0.7358, 0.8067, 0.6829, 0.1141,\n",
            "        0.1846, 0.4305, 0.3223, 0.9045, 0.9790, 0.3162, 0.3067, 0.7958, 0.5887,\n",
            "        0.1807, 0.1505, 0.4427, 0.6243, 0.4912, 0.8531, 0.9800, 0.1300, 0.2937,\n",
            "        0.3904, 0.6273, 0.9373, 0.0717, 0.4884, 0.8269, 0.0942, 0.8418, 0.9183,\n",
            "        0.0368, 0.2807, 0.0299, 0.6862, 0.9529, 0.7326, 0.3374, 0.2850, 0.0415,\n",
            "        0.9616, 0.7841, 0.2902, 0.3712, 0.2409, 0.7131, 0.1599, 0.7182, 0.3177,\n",
            "        0.6617, 0.4268, 0.4419, 0.8102, 0.8851, 0.5298, 0.6541, 0.6451, 0.4210,\n",
            "        0.4896, 0.4285, 0.0231, 0.0189, 0.0187, 0.6708, 0.2593, 0.8611, 0.6725,\n",
            "        0.0658, 0.3979, 0.2697, 0.0824, 0.2257, 0.2613, 0.3547, 0.9602, 0.4546,\n",
            "        0.6246, 0.2401, 0.1422, 0.0955, 0.5085, 0.2218, 0.7544, 0.0950, 0.5868,\n",
            "        0.2029, 0.2684, 0.2221, 0.7621, 0.2997, 0.8138, 0.6279, 0.2342, 0.9084,\n",
            "        0.0753, 0.1671, 0.3901, 0.0041, 0.6092, 0.8532, 0.2672, 0.3003, 0.8354,\n",
            "        0.8285, 0.6874, 0.2798, 0.0214, 0.8059, 0.5593, 0.6031, 0.8187, 0.6299,\n",
            "        0.2527, 0.1962, 0.2341, 0.0324, 0.5542, 0.6133, 0.3964, 0.3981, 0.8240,\n",
            "        0.2398, 0.4272, 0.0532, 0.7494, 0.1920, 0.5626, 0.3771, 0.9936, 0.4584,\n",
            "        0.9159, 0.8931, 0.5437, 0.7527, 0.3934, 0.5231, 0.2548, 0.1871, 0.5525,\n",
            "        0.9532, 0.7789, 0.5913, 0.9254, 0.9116, 0.3497, 0.6182, 0.4671, 0.0174,\n",
            "        0.9869, 0.8881, 0.0767, 0.5508, 0.4812, 0.2550, 0.6028, 0.0968, 0.8729,\n",
            "        0.1875, 0.2291, 0.4737, 0.1462, 0.6306, 0.3506, 0.9957, 0.1997, 0.8023,\n",
            "        0.2432, 0.6755, 0.7838, 0.3892, 0.3202, 0.5308, 0.1316, 0.5083, 0.8033,\n",
            "        0.5355, 0.4493, 0.5351, 0.1059, 0.3173, 0.3324, 0.6055, 0.1063, 0.5569,\n",
            "        0.4988, 0.5317, 0.2880, 0.0197, 0.9597, 0.8268, 0.6114, 0.7830, 0.6478,\n",
            "        0.6563, 0.9277, 0.1157, 0.1773, 0.8539, 0.1894, 0.8804, 0.8699, 0.8349,\n",
            "        0.1234, 0.2351, 0.3729, 0.7144, 0.6537, 0.2671, 0.1834, 0.4363, 0.7706,\n",
            "        0.6306, 0.6087, 0.8284, 0.5101, 0.7269, 0.5349, 0.3725, 0.8179, 0.9022,\n",
            "        0.9613, 0.9327, 0.6057, 0.2544, 0.8415, 0.0849, 0.5653, 0.9058, 0.4843,\n",
            "        0.6753, 0.0746, 0.9389, 0.3943, 0.2947, 0.5478, 0.5530, 0.9644, 0.2897,\n",
            "        0.3796], grad_fn=<SplitBackward0>), tensor([0.9155, 0.3911, 0.2355, 0.4951, 0.4189, 0.7776, 0.3028, 0.2071, 0.5760,\n",
            "        0.6013, 0.3184, 0.0118, 0.5010, 0.8942, 0.1321, 0.2798, 0.1163, 0.7126,\n",
            "        0.3694, 0.2300, 0.2349, 0.3893, 0.0126, 0.8057, 0.6446, 0.7729, 0.5655,\n",
            "        0.5550, 0.2227, 0.2846, 0.9182, 0.7551, 0.1454, 0.8435, 0.1058, 0.0361,\n",
            "        0.0691, 0.5290, 0.7326, 0.8128, 0.7088, 0.9366, 0.2369, 0.5819, 0.7509,\n",
            "        0.0545, 0.0375, 0.4585, 0.3639, 0.2964, 0.2307, 0.2858, 0.9262, 0.7015,\n",
            "        0.8008, 0.4990, 0.1093, 0.3318, 0.2151, 0.3405, 0.3970, 0.2820, 0.6284,\n",
            "        0.3603, 0.9180, 0.8434, 0.3689, 0.7731, 0.7571, 0.4288, 0.0305, 0.3342,\n",
            "        0.8334, 0.8342, 0.9137, 0.9610, 0.6418, 0.2689, 0.9767, 0.9880, 0.1463,\n",
            "        0.1842, 0.8648, 0.6789, 0.1208, 0.9266, 0.6444, 0.9645, 0.2057, 0.5728,\n",
            "        0.5230, 0.0547, 0.8290, 0.5085, 0.1621, 0.1648, 0.0113, 0.8081, 0.8372,\n",
            "        0.4645, 0.3099, 0.3298, 0.2927, 0.3585, 0.2851, 0.3211, 0.8476, 0.8205,\n",
            "        0.3156, 0.4865, 0.6995, 0.5063, 0.2146, 0.1070, 0.6808, 0.5937, 0.8662,\n",
            "        0.6133, 0.2346, 0.9185, 0.5126, 0.9140, 0.1970, 0.7114, 0.3790, 0.4522,\n",
            "        0.7769, 0.1630, 0.1051, 0.9014, 0.5698, 0.6404, 0.8228, 0.0183, 0.6728,\n",
            "        0.9614, 0.8921, 0.3786, 0.5183, 0.2760, 0.7497, 0.0143, 0.0522, 0.0972,\n",
            "        0.3446, 0.8438, 0.3540, 0.8399, 0.9441, 0.7447, 0.3826, 0.1861, 0.9663,\n",
            "        0.2433, 0.3771, 0.5045, 0.8609, 0.4596, 0.4116, 0.9796, 0.8790, 0.0355,\n",
            "        0.2299, 0.2976, 0.8611, 0.5722, 0.9926, 0.9880, 0.1473, 0.4740, 0.7851,\n",
            "        0.1439, 0.6088, 0.6670, 0.0770, 0.8226, 0.6252, 0.6998, 0.0301, 0.6688,\n",
            "        0.1801, 0.0733, 0.1387, 0.1727, 0.3382, 0.8135, 0.2962, 0.0599, 0.1358,\n",
            "        0.2681, 0.6651, 0.2608, 0.2287, 0.4034, 0.6929, 0.5512, 0.5239, 0.4783,\n",
            "        0.9894, 0.9763, 0.2716, 0.2955, 0.2739, 0.3195, 0.2118, 0.8656, 0.5967,\n",
            "        0.9292, 0.9934, 0.7173, 0.0528, 0.7679, 0.7750, 0.0961, 0.4183, 0.6958,\n",
            "        0.1504, 0.1636, 0.8619, 0.7057, 0.0569, 0.2752, 0.4107, 0.1791, 0.5270,\n",
            "        0.1394, 0.8121, 0.7866, 0.9061, 0.8873, 0.0695, 0.0962, 0.3437, 0.9541,\n",
            "        0.0445, 0.3641, 0.0093, 0.4263, 0.8050, 0.0191, 0.5053, 0.9250, 0.4443,\n",
            "        0.6740, 0.0544, 0.2691, 0.4619, 0.7799, 0.4476, 0.4940, 0.1290, 0.2961,\n",
            "        0.9106, 0.9894, 0.1557, 0.9169, 0.9027, 0.9009, 0.1325, 0.8639, 0.1084,\n",
            "        0.2066, 0.3041, 0.9917, 0.0115, 0.9957, 0.0115, 0.0263, 0.2874, 0.3594,\n",
            "        0.1639, 0.3407, 0.4418, 0.7701, 0.8952, 0.9342, 0.0832, 0.6638, 0.3631,\n",
            "        0.3146, 0.5340, 0.1653, 0.9733, 0.8039, 0.4077, 0.3121, 0.3257, 0.9514,\n",
            "        0.3524, 0.0032, 0.8358, 0.3836, 0.6836, 0.4252, 0.7363, 0.3445, 0.4469,\n",
            "        0.3810, 0.7374, 0.9649, 0.3659, 0.4208, 0.3033, 0.9775, 0.9679, 0.4861,\n",
            "        0.4523, 0.2732, 0.8987, 0.5115, 0.8888, 0.9551, 0.8020, 0.3994, 0.7259,\n",
            "        0.8054, 0.7776, 0.4218, 0.8541, 0.6560, 0.9693, 0.6499, 0.3414, 0.0990,\n",
            "        0.8782, 0.7177, 0.3093, 0.0091, 0.3389, 0.6487, 0.0072, 0.1203, 0.2163,\n",
            "        0.1644, 0.8585, 0.1363, 0.1436, 0.3459, 0.5520, 0.4709, 0.7861, 0.1724,\n",
            "        0.0633, 0.2100, 0.0186, 0.7504, 0.3908, 0.8297, 0.0619, 0.3538, 0.3078,\n",
            "        0.5485, 0.1220, 0.9141, 0.7332, 0.9908, 0.2375, 0.5021, 0.7709, 0.3510,\n",
            "        0.9058, 0.0494, 0.7776, 0.2948, 0.5197, 0.2067, 0.7077, 0.2054, 0.8276,\n",
            "        0.3538, 0.3396, 0.0153, 0.7012, 0.2185, 0.4562, 0.2568, 0.8182, 0.3283,\n",
            "        0.6882, 0.2252, 0.0071, 0.7491, 0.3170, 0.6128, 0.5087, 0.2926, 0.6996,\n",
            "        0.6116, 0.9607, 0.7612, 0.4183, 0.7716, 0.9620, 0.9971, 0.0047, 0.6497,\n",
            "        0.9907, 0.4868, 0.1090, 0.5085, 0.3173, 0.8293, 0.8822, 0.1087, 0.8602,\n",
            "        0.6859, 0.7782, 0.4371, 0.5853, 0.8775, 0.9055, 0.9375, 0.2621, 0.0756,\n",
            "        0.0424, 0.5114, 0.6586, 0.6047, 0.2012, 0.0186, 0.0626, 0.2601, 0.9710,\n",
            "        0.2240, 0.6636, 0.7275, 0.8864, 0.8864, 0.7893, 0.6053, 0.6260, 0.0578,\n",
            "        0.9826, 0.9987, 0.1833, 0.8236, 0.2892, 0.7789, 0.9798, 0.7522, 0.0635,\n",
            "        0.9887, 0.3441, 0.2396, 0.4335, 0.9627, 0.1948, 0.7865, 0.3086, 0.0547,\n",
            "        0.9042, 0.0376, 0.6497, 0.1786, 0.4869, 0.8590, 0.3002, 0.7220, 0.6337,\n",
            "        0.0944, 0.7974, 0.8597, 0.8320, 0.4662, 0.2379, 0.0588, 0.5764, 0.4577,\n",
            "        0.0264, 0.6589, 0.5083, 0.4033, 0.6682, 0.0963, 0.6173, 0.5824, 0.6143,\n",
            "        0.5137, 0.4213, 0.5582, 0.3007, 0.8324, 0.7369, 0.6199, 0.0668, 0.0317,\n",
            "        0.2082, 0.1466, 0.7168, 0.2163, 0.1386, 0.4430, 0.1975, 0.8327, 0.2971,\n",
            "        0.2539, 0.0353, 0.4102, 0.1481, 0.3517, 0.7173, 0.5975, 0.0421, 0.9236,\n",
            "        0.2835, 0.9074, 0.4885, 0.7900, 0.9859, 0.4312, 0.1466, 0.2436, 0.8222,\n",
            "        0.7131, 0.2011, 0.0644, 0.8804, 0.9289, 0.8251, 0.6845, 0.8031, 0.1001,\n",
            "        0.3216, 0.6335, 0.7902, 0.1035, 0.8498, 0.3889, 0.7769, 0.8997, 0.9661,\n",
            "        0.2739, 0.4262, 0.9249, 0.9362, 0.1433, 0.8784, 0.1483, 0.9492, 0.9532,\n",
            "        0.8596, 0.6074, 0.0258, 0.0084, 0.7305, 0.1156, 0.1852, 0.9432, 0.5039,\n",
            "        0.7614, 0.1257, 0.7648, 0.4919, 0.0898, 0.0203, 0.7704, 0.2024, 0.0642,\n",
            "        0.5859, 0.8869, 0.6721, 0.5813, 0.9737, 0.5635, 0.7490, 0.8028, 0.5026,\n",
            "        0.8998, 0.8117, 0.1927, 0.8750, 0.2779, 0.7097, 0.3691, 0.1398, 0.9507,\n",
            "        0.0953, 0.2230, 0.4886, 0.3512, 0.9897, 0.3260, 0.8361, 0.7096, 0.4895,\n",
            "        0.7562, 0.6072, 0.6835, 0.5245, 0.6996, 0.3988, 0.2528, 0.7840, 0.1589,\n",
            "        0.8724, 0.2277, 0.7203, 0.4144, 0.9754, 0.6083, 0.4723, 0.0046, 0.0871,\n",
            "        0.2733, 0.7463, 0.2594, 0.3985, 0.6042, 0.2206, 0.2034, 0.8656, 0.8904,\n",
            "        0.5701, 0.6484, 0.2191, 0.7792, 0.7323, 0.3630, 0.0116, 0.5271, 0.0910,\n",
            "        0.3845, 0.2152, 0.7791, 0.9148, 0.6818, 0.4443, 0.1576, 0.8529, 0.9176,\n",
            "        0.8079, 0.5073, 0.2053, 0.5276, 0.0528, 0.3159, 0.0933, 0.5400, 0.3409,\n",
            "        0.7121, 0.2387, 0.7466, 0.2317, 0.4803, 0.9400, 0.6394, 0.5106, 0.4375,\n",
            "        0.6766, 0.2234, 0.5567, 0.4573, 0.2127, 0.3779, 0.5666, 0.8414, 0.7850,\n",
            "        0.4692, 0.3307, 0.1721, 0.0206, 0.9824, 0.5749, 0.7357, 0.1766, 0.3945,\n",
            "        0.7179, 0.0314, 0.1060, 0.3011, 0.1565, 0.0880, 0.7131, 0.9051, 0.3242,\n",
            "        0.2679, 0.7910, 0.9620, 0.3132, 0.1627, 0.0957, 0.3321, 0.4757, 0.7584,\n",
            "        0.3264, 0.2182, 0.7630, 0.6249, 0.4986, 0.0186, 0.1004, 0.1436, 0.3255,\n",
            "        0.3688, 0.7500, 0.0900, 0.3719, 0.5030, 0.3463, 0.4046, 0.6691, 0.6900,\n",
            "        0.2749, 0.4187, 0.8226, 0.0696, 0.4166, 0.9538, 0.6622, 0.1626, 0.6278,\n",
            "        0.2578, 0.4715, 0.8723, 0.1863, 0.1936, 0.2280, 0.7785, 0.3684, 0.6395,\n",
            "        0.1114, 0.1475, 0.5528, 0.7771, 0.3994, 0.5787, 0.2168, 0.3101, 0.2035,\n",
            "        0.5974, 0.9957, 0.6345, 0.1792, 0.5195, 0.3307, 0.2371, 0.3037, 0.9471,\n",
            "        0.5935, 0.7524, 0.6434, 0.0970, 0.2650, 0.0246, 0.9842, 0.4616, 0.3611,\n",
            "        0.6255, 0.6950, 0.2588, 0.8940, 0.7712, 0.6338, 0.2263, 0.9574, 0.7301,\n",
            "        0.9275, 0.0569, 0.5369, 0.8795, 0.2479, 0.8238, 0.3398, 0.4478, 0.8198,\n",
            "        0.1026, 0.4938, 0.8662, 0.2997, 0.8788, 0.4871, 0.1482, 0.5172, 0.4037,\n",
            "        0.3959, 0.7762, 0.4490, 0.1294, 0.7737, 0.2447, 0.5262, 0.9935, 0.5988,\n",
            "        0.7505, 0.6870, 0.3952, 0.1381, 0.8813, 0.2562, 0.3861, 0.5775, 0.5478,\n",
            "        0.7779, 0.3397, 0.5138, 0.0958, 0.4643, 0.0735, 0.5460, 0.1698, 0.6072,\n",
            "        0.3198, 0.0657, 0.5180, 0.6611, 0.6990, 0.0419, 0.1918, 0.6958, 0.1473,\n",
            "        0.1427, 0.7205, 0.6085, 0.8671, 0.1857, 0.2711, 0.4303, 0.1676, 0.8581,\n",
            "        0.1670, 0.6997, 0.4145, 0.2817, 0.3470, 0.4743, 0.0353, 0.1835, 0.7437,\n",
            "        0.6848, 0.9999, 0.8509, 0.1720, 0.0157, 0.7130, 0.9486, 0.7615, 0.3937,\n",
            "        0.4633, 0.6472, 0.6312, 0.0748, 0.1699, 0.1785, 0.1929, 0.4677, 0.3576,\n",
            "        0.7964, 0.9324, 0.5533, 0.0866, 0.4046, 0.6761, 0.9840, 0.2255, 0.9726,\n",
            "        0.8682, 0.4002, 0.4598, 0.2912, 0.9640, 0.6811, 0.7585, 0.3378, 0.8987,\n",
            "        0.2581, 0.3140, 0.9754, 0.6725, 0.4491, 0.6794, 0.5017, 0.5209, 0.0848,\n",
            "        0.2092, 0.6475, 0.6538, 0.2059, 0.5017, 0.8939, 0.8245, 0.8690, 0.3112,\n",
            "        0.6884, 0.8372, 0.6861, 0.7125, 0.1372, 0.5736, 0.6393, 0.6599, 0.7706,\n",
            "        0.8619, 0.1385, 0.2967, 0.4895, 0.2039, 0.4638, 0.2602, 0.8873, 0.4075,\n",
            "        0.7523, 0.0128, 0.0714, 0.9518, 0.4605, 0.0606, 0.5562, 0.0598, 0.7564,\n",
            "        0.6651, 0.5147, 0.9202, 0.6866, 0.1628, 0.8090, 0.9071, 0.1203, 0.2307,\n",
            "        0.0051, 0.0034, 0.8492, 0.6338, 0.9114, 0.6983, 0.9200, 0.8447, 0.2421,\n",
            "        0.0974, 0.9469, 0.4740, 0.9894, 0.1872, 0.4341, 0.3997, 0.3306, 0.9834,\n",
            "        0.2919, 0.3851, 0.3298, 0.8648, 0.6031, 0.5180, 0.4976, 0.4052, 0.7414,\n",
            "        0.6350, 0.9565, 0.6327, 0.9450, 0.1956, 0.2356, 0.1775, 0.2849, 0.3739,\n",
            "        0.2358, 0.7494, 0.9698, 0.5141, 0.0740, 0.7314, 0.6823, 0.0189, 0.4018,\n",
            "        0.9273, 0.6000, 0.1384, 0.5124, 0.7032, 0.1136, 0.2197, 0.9664, 0.7539,\n",
            "        0.6645, 0.7657, 0.4921, 0.5110, 0.1535, 0.3586, 0.1731, 0.2322, 0.5659,\n",
            "        0.5588, 0.8508, 0.8301, 0.6135, 0.9385, 0.8811, 0.0167, 0.0295, 0.4470,\n",
            "        0.3276, 0.0635, 0.1369, 0.4833, 0.2821, 0.4114, 0.9632, 0.6963, 0.2293,\n",
            "        0.9480], grad_fn=<SplitBackward0>), tensor([4.8479e-01, 4.9826e-01, 9.4092e-01, 2.6808e-01, 4.5928e-01, 8.6421e-01,\n",
            "        9.4198e-01, 9.4701e-01, 8.5560e-01, 1.3416e-01, 2.9397e-01, 5.4966e-02,\n",
            "        6.1761e-01, 7.8101e-01, 6.2693e-01, 3.5769e-01, 2.3837e-01, 5.6589e-01,\n",
            "        3.8811e-01, 7.4947e-01, 1.2895e-01, 3.0728e-01, 1.1349e-03, 4.6321e-01,\n",
            "        8.0255e-01, 2.5109e-01, 1.1457e-01, 9.2028e-01, 8.5454e-01, 3.9331e-01,\n",
            "        1.8700e-01, 5.3838e-01, 1.9777e-01, 3.1394e-01, 8.4406e-01, 9.5560e-01,\n",
            "        5.6382e-01, 7.4648e-01, 7.8567e-01, 4.5551e-02, 7.7312e-01, 3.9096e-01,\n",
            "        3.2216e-04, 4.5141e-01, 8.0903e-01, 9.3456e-01, 5.0817e-01, 1.2702e-02,\n",
            "        5.8385e-01, 6.6904e-01, 6.7443e-01, 9.0805e-01, 6.7978e-02, 7.9913e-01,\n",
            "        2.2578e-01, 9.0585e-01, 1.4372e-01, 1.3708e-01, 9.9993e-02, 9.2490e-02,\n",
            "        5.4978e-01, 6.5654e-02, 8.6073e-01, 5.6146e-01, 6.4226e-02, 3.1952e-01,\n",
            "        2.7465e-02, 7.4690e-01, 9.3852e-01, 7.9741e-01, 9.0447e-01, 4.9722e-01,\n",
            "        8.3151e-01, 6.4283e-01, 3.5034e-02, 8.6679e-01, 6.4990e-01, 8.0591e-01,\n",
            "        6.4907e-01, 6.4164e-01, 5.4115e-01, 7.7794e-01, 3.9519e-01, 8.1281e-01,\n",
            "        5.4832e-01, 9.5260e-01, 4.2246e-01, 3.2469e-01, 2.1949e-01, 7.0406e-01,\n",
            "        4.2936e-01, 1.2200e-01, 2.9567e-01, 3.5619e-01, 4.9682e-01, 7.5332e-01,\n",
            "        8.0077e-01, 3.1716e-01, 1.9976e-01, 2.1753e-01, 5.2239e-01, 8.4341e-01,\n",
            "        4.8864e-01, 4.2983e-01, 8.5365e-01, 2.5858e-01, 7.8502e-01, 2.3480e-01,\n",
            "        9.6862e-02, 2.8361e-01, 8.8788e-01, 5.2974e-01, 5.6546e-01, 3.5000e-01,\n",
            "        9.6475e-01, 2.2466e-03, 5.5196e-01, 8.1587e-02, 5.7519e-01, 6.0342e-01,\n",
            "        4.4308e-02, 9.7913e-01, 2.9606e-01, 9.7511e-01, 5.5305e-01, 8.5304e-02,\n",
            "        4.9355e-01, 1.1671e-01, 6.7755e-01, 7.4788e-01, 7.9278e-01, 1.0014e-01,\n",
            "        6.5745e-02, 9.4509e-01, 6.9678e-01, 9.3386e-01, 4.3405e-01, 5.3912e-01,\n",
            "        9.4233e-01, 3.7913e-01, 3.3132e-01, 1.2859e-01, 6.8467e-01, 3.2581e-01,\n",
            "        5.3396e-01, 8.6748e-01, 2.6953e-02, 2.2319e-01, 6.4673e-01, 7.7733e-01,\n",
            "        4.6943e-02, 9.8338e-01, 3.3799e-01, 5.1780e-01, 8.1517e-01, 7.6583e-01,\n",
            "        7.9916e-01, 5.7755e-01, 5.3679e-01, 7.4768e-01, 8.9430e-01, 6.7939e-01,\n",
            "        4.2246e-01, 9.2229e-01, 9.4659e-01, 2.2613e-01, 5.9903e-01, 4.5431e-02,\n",
            "        4.4751e-01, 4.9238e-01, 7.5343e-02, 7.3493e-01, 3.4619e-01, 5.9806e-01,\n",
            "        3.2035e-01, 2.7505e-01, 8.0337e-02, 4.2889e-01, 1.6412e-01, 1.6740e-01,\n",
            "        8.3990e-01, 7.0550e-01, 4.2367e-02, 9.9886e-01, 7.0700e-01, 7.5668e-01,\n",
            "        4.7553e-01, 4.4189e-01, 1.7375e-01, 7.9605e-01, 7.6598e-01, 9.9710e-02,\n",
            "        8.1910e-01, 9.5326e-01, 9.6005e-01, 3.8626e-01, 8.2592e-01, 3.0304e-01,\n",
            "        3.3644e-01, 3.8012e-01, 7.8075e-01, 7.5054e-01, 9.8402e-02, 7.9223e-01,\n",
            "        6.0580e-01, 7.6700e-01, 7.5740e-01, 5.6702e-01, 9.1563e-01, 1.8614e-01,\n",
            "        2.1135e-01, 3.8251e-02, 8.4269e-01, 9.6452e-02, 3.9985e-01, 2.5841e-01,\n",
            "        3.9628e-02, 7.5929e-01, 8.0559e-01, 6.1514e-01, 7.4928e-01, 4.4909e-01,\n",
            "        7.7455e-01, 2.3213e-01, 4.3494e-01, 2.6262e-01, 7.9884e-01, 1.7748e-03,\n",
            "        8.0917e-01, 7.6228e-01, 7.0420e-01, 4.9755e-01, 8.1829e-01, 7.6615e-01,\n",
            "        6.1703e-01, 9.6541e-01, 4.3000e-01, 9.9750e-01, 8.4190e-01, 7.7156e-01,\n",
            "        2.4535e-01, 4.0286e-01, 9.5787e-01, 9.3282e-01, 2.1447e-01, 7.7977e-01,\n",
            "        2.7231e-01, 7.5144e-01, 6.9560e-01, 1.4662e-01, 1.3396e-01, 6.1710e-01,\n",
            "        7.1761e-03, 8.7128e-01, 1.4618e-01, 1.3776e-01, 7.8509e-01, 1.1167e-01,\n",
            "        5.0632e-01, 1.0703e-01, 3.8312e-01, 2.4225e-02, 8.0470e-01, 9.5690e-01,\n",
            "        1.4911e-01, 6.0544e-01, 4.8269e-01, 8.8442e-01, 3.2483e-01, 7.8276e-01,\n",
            "        5.6981e-01, 3.1307e-02, 6.0918e-01, 7.0098e-01, 6.4880e-01, 5.9168e-01,\n",
            "        1.9829e-01, 1.9495e-01, 9.5628e-01, 1.4723e-01, 3.1426e-01, 9.4725e-01,\n",
            "        1.2927e-01, 7.9151e-01, 1.9841e-01, 7.3995e-01, 1.9610e-01, 4.6023e-01,\n",
            "        5.7185e-01, 6.5380e-01, 3.9036e-01, 8.9641e-01, 6.6306e-01, 1.9410e-01,\n",
            "        8.0542e-01, 3.3279e-01, 8.9036e-01, 6.8236e-01, 6.5074e-01, 7.1079e-01,\n",
            "        3.5386e-01, 4.9641e-01, 4.1721e-01, 9.0395e-01, 2.9201e-01, 3.0288e-01,\n",
            "        1.1021e-01, 5.3137e-01, 7.7874e-02, 3.8439e-01, 3.5221e-01, 7.1725e-02,\n",
            "        2.8751e-01, 3.8410e-01, 8.4814e-01, 1.3558e-01, 6.8188e-01, 1.5435e-01,\n",
            "        5.5737e-01, 9.4858e-01, 3.9713e-01, 2.7448e-01, 2.6288e-01, 4.4065e-01,\n",
            "        7.1247e-01, 6.5336e-01, 6.1931e-02, 2.6052e-01, 9.9588e-01, 1.3753e-01,\n",
            "        4.7617e-01, 5.0279e-01, 9.7299e-01, 5.4630e-01, 6.8544e-02, 7.8087e-01,\n",
            "        5.5004e-01, 5.3288e-01, 6.5196e-01, 5.9963e-01, 7.2222e-01, 5.0478e-01,\n",
            "        7.7270e-01, 2.1887e-01, 3.1483e-02, 5.5212e-01, 8.9686e-01, 6.1597e-01,\n",
            "        4.7621e-01, 7.4818e-01, 4.6024e-01, 1.7997e-01, 1.7843e-01, 3.4631e-01,\n",
            "        3.6823e-01, 9.5984e-01, 9.6588e-01, 5.7053e-01, 6.8327e-01, 5.0279e-01,\n",
            "        6.7744e-02, 4.5441e-01, 3.3172e-01, 9.7989e-01, 7.6007e-02, 9.7214e-01,\n",
            "        2.4272e-01, 2.8650e-01, 1.1303e-01, 1.3039e-01, 9.5305e-01, 7.7595e-01,\n",
            "        9.8878e-01, 2.3676e-01, 4.6890e-01, 3.0656e-01, 8.8429e-01, 8.6145e-02,\n",
            "        7.3508e-01, 6.8238e-01, 3.7684e-01, 2.9246e-01, 5.3198e-02, 1.7806e-02,\n",
            "        2.3479e-02, 4.1270e-01, 1.9605e-01, 1.7382e-01, 2.4406e-01, 2.4330e-01,\n",
            "        9.2604e-01, 2.2611e-01, 3.6018e-01, 5.3570e-01, 3.3557e-01, 7.3523e-01,\n",
            "        1.8102e-01, 6.3368e-01, 3.9805e-02, 2.1747e-01, 4.3670e-02, 8.3413e-01,\n",
            "        6.4406e-01, 1.7543e-01, 8.7849e-01, 7.9939e-01, 8.9439e-01, 4.9036e-01,\n",
            "        9.4999e-01, 7.6234e-01, 7.9825e-01, 2.8678e-01, 1.6633e-01, 5.4620e-01,\n",
            "        4.7123e-01, 2.2416e-01, 2.7389e-02, 8.9342e-01, 8.4388e-02, 9.4414e-01,\n",
            "        2.2538e-01, 8.3198e-01, 5.9802e-01, 2.3642e-01, 4.9625e-01, 5.7386e-01,\n",
            "        7.5892e-01, 9.9330e-01, 5.7000e-01, 9.1957e-01, 6.3920e-01, 3.5427e-01,\n",
            "        4.9008e-01, 5.4820e-02, 9.6687e-01, 3.4287e-02, 4.6062e-01, 4.9825e-02,\n",
            "        8.8263e-01, 3.0180e-01, 3.4012e-01, 3.2927e-01, 6.0530e-01, 9.3532e-01,\n",
            "        1.3365e-01, 3.2458e-01, 9.8431e-01, 5.9971e-02, 5.1008e-01, 1.4078e-02,\n",
            "        1.3801e-01, 1.5851e-01, 4.4964e-01, 3.2132e-01, 1.1292e-01, 9.0465e-01,\n",
            "        2.1945e-01, 9.6592e-01, 1.3918e-02, 5.7801e-01, 8.1443e-01, 6.8878e-01,\n",
            "        7.0584e-01, 8.4727e-01, 7.4880e-01, 6.3058e-01, 2.0562e-01, 4.3877e-01,\n",
            "        6.7951e-01, 2.8690e-01, 1.2222e-02, 9.3790e-01, 1.8583e-02, 3.3339e-01,\n",
            "        7.9098e-01, 2.4069e-01, 3.4556e-01, 2.9531e-01, 7.6775e-01, 8.8297e-01,\n",
            "        5.2570e-01, 1.9829e-01, 9.6448e-01, 9.9233e-01, 9.2924e-02, 9.4103e-01,\n",
            "        5.5941e-01, 5.9967e-01, 3.2991e-01, 3.1476e-01, 7.9175e-01, 4.0516e-01,\n",
            "        1.0904e-01, 1.9616e-01, 6.0022e-01, 9.6659e-01, 9.9847e-01, 1.8690e-01,\n",
            "        1.6194e-01, 6.6264e-01, 9.1319e-01, 6.6516e-01, 2.1555e-01, 3.6638e-01,\n",
            "        9.1563e-01, 8.5833e-01, 3.6256e-01, 1.3983e-01, 7.6278e-01, 8.5279e-01,\n",
            "        9.8804e-02, 6.8041e-02, 7.5780e-01, 7.3199e-01, 5.4689e-01, 5.0734e-01,\n",
            "        7.9644e-01, 2.5740e-01, 7.1255e-01, 1.1535e-01, 5.3372e-02, 4.0052e-01,\n",
            "        6.3949e-01, 9.5744e-01, 4.2738e-01, 7.2927e-01, 9.2554e-01, 2.0681e-01,\n",
            "        8.6651e-01, 3.0783e-01, 6.2153e-01, 2.8038e-01, 2.6808e-01, 6.6529e-01,\n",
            "        4.3182e-02, 2.0386e-01, 6.3867e-01, 9.2827e-01, 5.3592e-01, 3.2902e-01,\n",
            "        1.9907e-01, 8.6094e-01, 9.1831e-01, 8.9264e-01, 9.7433e-01, 4.5956e-01,\n",
            "        4.5246e-01, 1.9012e-01, 5.8962e-01, 6.1523e-01, 6.9767e-01, 1.6242e-01,\n",
            "        3.8680e-01, 5.3552e-02, 2.7666e-01, 9.0296e-01, 7.3346e-01, 3.7637e-01,\n",
            "        5.2082e-01, 3.3092e-01, 8.9637e-01, 3.4293e-01, 3.7962e-01, 5.1370e-01,\n",
            "        2.2336e-01, 3.7463e-01, 4.2680e-02, 9.9327e-01, 9.2575e-01, 2.7942e-02,\n",
            "        7.2702e-01, 4.9704e-01, 9.2564e-01, 7.9813e-01, 5.8069e-01, 1.2597e-01,\n",
            "        5.2724e-01, 8.1385e-01, 3.4370e-01, 6.2276e-01, 6.3816e-01, 4.1442e-01,\n",
            "        2.4332e-01, 6.5091e-01, 7.2745e-01, 1.2206e-01, 9.6556e-01, 8.0154e-01,\n",
            "        6.5687e-01, 2.3345e-01, 9.5127e-02, 5.1103e-01, 3.1811e-01, 8.7330e-01,\n",
            "        2.0962e-01, 5.0804e-01, 2.3132e-01, 1.4049e-03, 3.9259e-01, 2.8347e-01,\n",
            "        6.8375e-01, 7.9826e-01, 8.6751e-02, 2.5440e-01, 7.9545e-01, 9.0219e-01,\n",
            "        6.3144e-01, 9.9737e-01, 1.7721e-01, 1.6019e-01, 3.6117e-01, 5.2511e-01,\n",
            "        2.6951e-02, 9.6765e-01, 5.2188e-01, 2.4214e-01, 7.3027e-01, 7.3800e-02,\n",
            "        3.1245e-01, 7.1373e-02, 7.3301e-01, 1.4437e-01, 8.5952e-01, 5.9697e-01,\n",
            "        3.0368e-01, 8.5653e-01, 1.4986e-01, 9.8376e-01, 4.0289e-01, 5.2460e-02,\n",
            "        4.2775e-02, 4.6466e-01, 6.5291e-01, 2.6331e-02, 3.7366e-01, 8.7726e-01,\n",
            "        1.9822e-01, 7.2522e-01, 7.9741e-01, 4.3052e-01, 7.8501e-01, 6.5580e-01,\n",
            "        7.1808e-01, 8.2346e-01, 8.5475e-01, 8.3525e-03, 1.2449e-01, 4.8014e-02,\n",
            "        8.1930e-01, 8.8922e-01, 5.4025e-01, 9.9387e-01, 4.8744e-01, 8.1807e-01,\n",
            "        8.1095e-01, 2.3452e-01, 2.7786e-01, 8.9725e-01, 4.9683e-01, 5.8522e-01,\n",
            "        9.4615e-01, 2.2929e-01, 2.9465e-02, 9.1192e-01, 2.4675e-01, 2.9890e-01,\n",
            "        3.3332e-01, 5.8322e-01, 7.0255e-01, 5.0594e-01, 5.2741e-01, 4.2983e-03,\n",
            "        7.0891e-01, 6.3202e-01, 8.7176e-01, 5.7532e-01, 5.6163e-01, 6.1857e-01,\n",
            "        4.5989e-02, 4.8280e-01, 5.4628e-01, 8.0059e-01, 1.5065e-02, 3.5627e-01,\n",
            "        1.8857e-01, 6.6414e-01, 4.1279e-01, 4.7666e-01, 4.7787e-01, 1.1170e-01,\n",
            "        8.3853e-01, 5.1609e-01, 8.1221e-01, 7.1672e-01, 9.8635e-01, 5.8541e-01,\n",
            "        5.3071e-01, 8.2138e-01, 4.0258e-01, 4.3267e-01, 5.6552e-01, 1.6892e-01,\n",
            "        5.3350e-01, 6.9862e-01, 4.3460e-01, 1.3715e-01, 3.3491e-01, 5.6058e-01,\n",
            "        5.5009e-01, 2.2163e-01, 2.6746e-01, 9.9727e-01, 3.1227e-01, 6.0110e-01,\n",
            "        2.7073e-01, 1.8113e-01, 1.3987e-01, 2.6390e-01, 3.8872e-01, 8.8901e-01,\n",
            "        7.2770e-01, 7.6015e-01, 1.0430e-01, 1.5300e-01, 8.5082e-02, 2.1256e-01,\n",
            "        9.1786e-01, 4.2089e-02, 5.0941e-02, 9.8202e-01, 1.8187e-01, 3.0679e-01,\n",
            "        5.4283e-01, 1.7673e-01, 8.1893e-01, 8.0387e-02, 3.5662e-01, 2.0124e-01,\n",
            "        8.3303e-01, 9.2167e-01, 9.3981e-01, 8.5830e-01, 9.9795e-01, 5.6615e-01,\n",
            "        7.8715e-01, 2.5185e-02, 3.3419e-02, 2.4817e-01, 1.0420e-01, 9.0182e-01,\n",
            "        9.0478e-01, 2.7089e-01, 3.6420e-01, 1.6933e-01, 3.3227e-01, 8.3975e-01,\n",
            "        7.0930e-01, 6.7258e-01, 4.2306e-01, 6.7543e-01, 5.7419e-01, 3.0084e-01,\n",
            "        3.8011e-01, 9.9356e-01, 6.4817e-01, 9.2769e-01, 7.1184e-01, 9.5319e-01,\n",
            "        1.8051e-01, 8.4953e-01, 7.3419e-01, 8.2479e-01, 8.8673e-01, 8.0268e-01,\n",
            "        7.0146e-01, 5.4028e-01, 8.7618e-01, 3.0273e-01, 4.8271e-01, 3.3262e-01,\n",
            "        4.8496e-01, 4.9719e-01, 3.8915e-02, 9.2921e-01, 5.7474e-01, 9.3452e-01,\n",
            "        8.4361e-01, 6.3232e-01, 4.5040e-01, 4.2313e-02, 3.8459e-01, 6.2492e-01,\n",
            "        8.9170e-01, 1.7097e-01, 6.1548e-01, 9.9948e-02, 4.3667e-01, 8.8708e-01,\n",
            "        5.6699e-01, 4.9302e-01, 5.2839e-01, 1.7900e-01, 4.8778e-01, 6.9859e-01,\n",
            "        7.2397e-01, 6.3030e-01, 6.5715e-01, 7.1580e-01, 6.9852e-01, 8.5789e-01,\n",
            "        5.0347e-01, 7.0462e-01, 2.2897e-01, 5.4579e-01, 8.8850e-01, 6.4097e-01,\n",
            "        7.2492e-01, 4.9668e-01, 7.9014e-01, 5.9602e-01, 5.2733e-01, 3.2481e-01,\n",
            "        1.3177e-01, 4.8120e-01, 9.1421e-01, 9.5086e-01, 9.3816e-01, 1.5796e-01,\n",
            "        7.9065e-01, 7.0202e-01, 6.7831e-01, 8.9066e-02, 2.6264e-01, 1.1521e-01,\n",
            "        1.9026e-01, 7.8404e-01, 7.5977e-01, 1.3189e-01, 1.6840e-02, 4.2169e-01,\n",
            "        4.4766e-01, 5.1235e-01, 9.4890e-01, 5.1871e-01, 3.7693e-01, 9.3952e-02,\n",
            "        5.9558e-01, 3.3767e-01, 6.0362e-01, 4.3812e-01, 5.5067e-01, 4.9775e-01,\n",
            "        9.2189e-01, 7.8141e-01, 4.9196e-01, 8.9101e-01, 5.2070e-01, 5.9016e-02,\n",
            "        7.5069e-03, 1.0156e-01, 2.1829e-01, 6.6751e-01, 4.8720e-01, 9.0542e-01,\n",
            "        6.1162e-01, 9.5806e-02, 8.8690e-01, 5.8120e-01, 3.8235e-01, 3.4639e-01,\n",
            "        7.5933e-01, 6.6610e-01, 9.4571e-01, 6.1946e-01, 5.7336e-01, 8.1399e-02,\n",
            "        7.1364e-01, 9.1249e-01, 5.2833e-01, 4.7660e-01, 4.7840e-01, 3.8672e-01,\n",
            "        6.3241e-01, 7.3595e-01, 7.8047e-01, 9.8161e-01, 3.6185e-01, 5.7544e-01,\n",
            "        8.8958e-01, 3.9833e-01, 4.2498e-01, 4.2312e-01, 8.5904e-01, 2.1444e-01,\n",
            "        1.2890e-02, 3.4320e-01, 2.2666e-01, 2.1467e-01, 4.6853e-01, 7.2066e-01,\n",
            "        5.1436e-01, 8.2016e-01, 8.9439e-01, 6.5510e-01, 8.0970e-01, 6.0300e-01,\n",
            "        9.2581e-01, 9.5590e-01, 6.5824e-01, 3.6490e-01, 9.9016e-01, 7.5524e-01,\n",
            "        3.8763e-01, 4.0942e-01, 4.3939e-01, 7.2636e-01, 9.0575e-01, 8.9669e-01,\n",
            "        8.3315e-01, 7.2828e-01, 8.2593e-01, 7.2663e-01, 5.1046e-01, 4.5371e-01,\n",
            "        1.2612e-01, 6.9454e-01, 4.1361e-01, 7.0012e-01, 7.6072e-01, 6.8639e-01,\n",
            "        5.5894e-01, 9.5172e-01, 7.0548e-01, 5.9847e-01, 7.5818e-01, 4.1427e-01,\n",
            "        5.6439e-01, 5.5083e-01, 4.6808e-01, 9.5903e-01, 5.9239e-01, 9.2462e-01,\n",
            "        7.6552e-01, 9.9831e-02, 8.6191e-03, 6.6220e-01, 2.7262e-01, 2.7831e-02,\n",
            "        2.3721e-01, 4.1173e-01, 2.1033e-01, 7.2234e-01, 3.0832e-01, 6.3059e-01,\n",
            "        3.1941e-02, 4.7393e-01, 4.6250e-01, 9.5436e-02, 7.7129e-01, 7.4153e-01,\n",
            "        5.4321e-02, 5.7795e-01, 8.7834e-01, 9.9339e-01, 9.4534e-01, 6.2758e-03,\n",
            "        3.5767e-01, 5.8512e-03, 2.9684e-01, 4.7580e-01, 2.4276e-01, 8.0879e-01,\n",
            "        7.9176e-01, 3.3484e-01, 8.4937e-01, 3.6776e-01, 1.1064e-01, 9.3514e-02,\n",
            "        3.5319e-01, 7.2673e-01, 7.4985e-01, 7.1560e-01, 4.0839e-01, 6.7377e-01,\n",
            "        9.6015e-01, 6.6975e-01, 2.0108e-01, 2.0230e-01, 6.8526e-01, 1.2930e-01,\n",
            "        1.0384e-01, 6.5251e-01, 7.9450e-01, 4.2201e-01, 7.5607e-01, 8.3206e-01,\n",
            "        7.7748e-01, 4.0329e-01, 9.1277e-01, 8.9079e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([0.6691, 0.9001, 0.0059, 0.7727, 0.5524, 0.7298, 0.5997, 0.7009, 0.5954,\n",
            "        0.2102, 0.2928, 0.3772, 0.9412, 0.4040, 0.5508, 0.4508, 0.3934, 0.4997,\n",
            "        0.0078, 0.3593, 0.8028, 0.8901, 0.8426, 0.2289, 0.5689, 0.1176, 0.5031,\n",
            "        0.1395, 0.8129, 0.2557, 0.7399, 0.7417, 0.6019, 0.2631, 0.1919, 0.5706,\n",
            "        0.9893, 0.1245, 0.9596, 0.8429, 0.0945, 0.4740, 0.7635, 0.1171, 0.4037,\n",
            "        0.2604, 0.6393, 0.8018, 0.6916, 0.6135, 0.9620, 0.4195, 0.9812, 0.2679,\n",
            "        0.0985, 0.5622, 0.7804, 0.1484, 0.9508, 0.6952, 0.7567, 0.8027, 0.3710,\n",
            "        0.8586, 0.4461, 0.7680, 0.1922, 0.7798, 0.5924, 0.3706, 0.3572, 0.0760,\n",
            "        0.3533, 0.4059, 0.0059, 0.0614, 0.5247, 0.7789, 0.0175, 0.6242, 0.7494,\n",
            "        0.2904, 0.5257, 0.3724, 0.3879, 0.5563, 0.2007, 0.7927, 0.8877, 0.6056,\n",
            "        0.3809, 0.5573, 0.0587, 0.2926, 0.5806, 0.7752, 0.3529, 0.5594, 0.6809,\n",
            "        0.9780, 0.7086, 0.5194, 0.4747, 0.6994, 0.9801, 0.4846, 0.4695, 0.8694,\n",
            "        0.4814, 0.6437, 0.3346, 0.8642, 0.5897, 0.0540, 0.1183, 0.8015, 0.6564,\n",
            "        0.5647, 0.1113, 0.6641, 0.4857, 0.8858, 0.7145, 0.4464, 0.1914, 0.4654,\n",
            "        0.6100, 0.9543, 0.5150, 0.0305, 0.6987, 0.6391, 0.0866, 0.7517, 0.7220,\n",
            "        0.0903, 0.9110, 0.2062, 0.5043, 0.4104, 0.5197, 0.7403, 0.3373, 0.9111,\n",
            "        0.7913, 0.5537, 0.6412, 0.4796, 0.7299, 0.4077, 0.4620, 0.9373, 0.0833,\n",
            "        0.4956, 0.7415, 0.5420, 0.8681, 0.5847, 0.4891, 0.4842, 0.0555, 0.5164,\n",
            "        0.4899, 0.8058, 0.8166, 0.0114, 0.0230, 0.8063, 0.5773, 0.4225, 0.3596,\n",
            "        0.4893, 0.4604, 0.2368, 0.1499, 0.3331, 0.9709, 0.5066, 0.5153, 0.8439,\n",
            "        0.7692, 0.6100, 0.1598, 0.8600, 0.0825, 0.6643, 0.1054, 0.5376, 0.4804,\n",
            "        0.1949, 0.9724, 0.8722, 0.8992, 0.7854, 0.8364, 0.0479, 0.8456, 0.6442,\n",
            "        0.8415, 0.9366, 0.3082, 0.7192, 0.5627, 0.6736, 0.2082, 0.1599, 0.1968,\n",
            "        0.0664, 0.3475, 0.5124, 0.1481, 0.9353, 0.2059, 0.6197, 0.1708, 0.0812,\n",
            "        0.6532, 0.3509, 0.9130, 0.3840, 0.6655, 0.3890, 0.2510, 0.6186, 0.8365,\n",
            "        0.8571, 0.3559, 0.0938, 0.2071, 0.9899, 0.6404, 0.4219, 0.2941, 0.1876,\n",
            "        0.0817, 0.8006, 0.2928, 0.5468, 0.1534, 0.4018, 0.6268, 0.7667, 0.5131,\n",
            "        0.1379, 0.8259, 0.7636, 0.4321, 0.9918, 0.6333, 0.0355, 0.5994, 0.6546,\n",
            "        0.1098, 0.0016, 0.8148, 0.4930, 0.1924, 0.9333, 0.3616, 0.3643, 0.2140,\n",
            "        0.5982, 0.0733, 0.6245, 0.0552, 0.5817, 0.6437, 0.3042, 0.9429, 0.6256,\n",
            "        0.8466, 0.0549, 0.9735, 0.5242, 0.4668, 0.2194, 0.9772, 0.2180, 0.9415,\n",
            "        0.0402, 0.2560, 0.6920, 0.8968, 0.9505, 0.9565, 0.9222, 0.6703, 0.6399,\n",
            "        0.0516, 0.7790, 0.7271, 0.8784, 0.5878, 0.4828, 0.9945, 0.4429, 0.9658,\n",
            "        0.8442, 0.2128, 0.4790, 0.2544, 0.5126, 0.9895, 0.5895, 0.5479, 0.0704,\n",
            "        0.7076, 0.4946, 0.6003, 0.7327, 0.4928, 0.6214, 0.4708, 0.1723, 0.0772,\n",
            "        0.3433, 0.6581, 0.9809, 0.5430, 0.5156, 0.4075, 0.8763, 0.5090, 0.4887,\n",
            "        0.0879, 0.0031, 0.1354, 0.9460, 0.7494, 0.8870, 0.2920, 0.0643, 0.3802,\n",
            "        0.6621, 0.4866, 0.8156, 0.6082, 0.1716, 0.7806, 0.6399, 0.9892, 0.0598,\n",
            "        0.0226, 0.4455, 0.1084, 0.1138, 0.2314, 0.5660, 0.6924, 0.9668, 0.4193,\n",
            "        0.2780, 0.5099, 0.2522, 0.0627, 0.5408, 0.8647, 0.9382, 0.1379, 0.2682,\n",
            "        0.7854, 0.1187, 0.4718, 0.2653, 0.8806, 0.8817, 0.0502, 0.2832, 0.1552,\n",
            "        0.0313, 0.0984, 0.2441, 0.4042, 0.2261, 0.6193, 0.9208, 0.0193, 0.6089,\n",
            "        0.2774, 0.4965, 0.8457, 0.6870, 0.0602, 0.5086, 0.8099, 0.3982, 0.2893,\n",
            "        0.0934, 0.9696, 0.0229, 0.7215, 0.0023, 0.6525, 0.4210, 0.7584, 0.6291,\n",
            "        0.8465, 0.6844, 0.6707, 0.4678, 0.2100, 0.7261, 0.1581, 0.3635, 0.8949,\n",
            "        0.0761, 0.8343, 0.4779, 0.7362, 0.9519, 0.3566, 0.1095, 0.1263, 0.3688,\n",
            "        0.7772, 0.0285, 0.5379, 0.4163, 0.9187, 0.6463, 0.3003, 0.0681, 0.8708,\n",
            "        0.4078, 0.8649, 0.3415, 0.3353, 0.0785, 0.1859, 0.5487, 0.1649, 0.8143,\n",
            "        0.3870, 0.4638, 0.0395, 0.5242, 0.0527, 0.8422, 0.5978, 0.9231, 0.7497,\n",
            "        0.5917, 0.2655, 0.0122, 0.5746, 0.0887, 0.9201, 0.2073, 0.2675, 0.0295,\n",
            "        0.9784, 0.7983, 0.8463, 0.1428, 0.4429, 0.0345, 0.4759, 0.9239, 0.8031,\n",
            "        0.8679, 0.8669, 0.1823, 0.0946, 0.8366, 0.4743, 0.7365, 0.7662, 0.2626,\n",
            "        0.7653, 0.5101, 0.1964, 0.2463, 0.3600, 0.3503, 0.1846, 0.0906, 0.5640,\n",
            "        0.8359, 0.8492, 0.9459, 0.8215, 0.9218, 0.6269, 0.2143, 0.6904, 0.5821,\n",
            "        0.2926, 0.1776, 0.4588, 0.3569, 0.0095, 0.9249, 0.0540, 0.2484, 0.3630,\n",
            "        0.3977, 0.7076, 0.9704, 0.0124, 0.0434, 0.9661, 0.6371, 0.3706, 0.2022,\n",
            "        0.5874, 0.7266, 0.5749, 0.2026, 0.0995, 0.1286, 0.9174, 0.8828, 0.8611,\n",
            "        0.5902, 0.1152, 0.2722, 0.2435, 0.0958, 0.5190, 0.0782, 0.6674, 0.7224,\n",
            "        0.2326, 0.7186, 0.1578, 0.3357, 0.4723, 0.7857, 0.7416, 0.7902, 0.5273,\n",
            "        0.1755, 0.0697, 0.2381, 0.2666, 0.7725, 0.4651, 0.1627, 0.3225, 0.4416,\n",
            "        0.6179, 0.7936, 0.3330, 0.3292, 0.9842, 0.5529, 0.0228, 0.7213, 0.3174,\n",
            "        0.5234, 0.5096, 0.5750, 0.9275, 0.4813, 0.1197, 0.5598, 0.1462, 0.9182,\n",
            "        0.0282, 0.0894, 0.5801, 0.6794, 0.3721, 0.6039, 0.4902, 0.3588, 0.4003,\n",
            "        0.2621, 0.8943, 0.0395, 0.9435, 0.3038, 0.1983, 0.2474, 0.2413, 0.5974,\n",
            "        0.9337, 0.0898, 0.7736, 0.3722, 0.7163, 0.6473, 0.2792, 0.8190, 0.8333,\n",
            "        0.7007, 0.0736, 0.8213, 0.9737, 0.4921, 0.0483, 0.5449, 0.4420, 0.2300,\n",
            "        0.6141, 0.1285, 0.8439, 0.1174, 0.3509, 0.7349, 0.5347, 0.8268, 0.6697,\n",
            "        0.0249, 0.9624, 0.0493, 0.7631, 0.7759, 0.0728, 0.5878, 0.9627, 0.4670,\n",
            "        0.5679, 0.1784, 0.2919, 0.9502, 0.1127, 0.5312, 0.5306, 0.0171, 0.1718,\n",
            "        0.3207, 0.2089, 0.2393, 0.3418, 0.2887, 0.4520, 0.5639, 0.7708, 0.4299,\n",
            "        0.5265, 0.8060, 0.2519, 0.3219, 0.8729, 0.9762, 0.9716, 0.3905, 0.0225,\n",
            "        0.2994, 0.2740, 0.9619, 0.8921, 0.7163, 0.5456, 0.5658, 0.1627, 0.4413,\n",
            "        0.7963, 0.0205, 0.9367, 0.8662, 0.7775, 0.1871, 0.0036, 0.3518, 0.5545,\n",
            "        0.0410, 0.1437, 0.8887, 0.5582, 0.8446, 0.2934, 0.3305, 0.1651, 0.4932,\n",
            "        0.5090, 0.7164, 0.3612, 0.9325, 0.0944, 0.3458, 0.6250, 0.6997, 0.9190,\n",
            "        0.8175, 0.9566, 0.0549, 0.1695, 0.7252, 0.0933, 0.9483, 0.7761, 0.4069,\n",
            "        0.6487, 0.9751, 0.0823, 0.5320, 0.4170, 0.6624, 0.5637, 0.9892, 0.3708,\n",
            "        0.1233, 0.6880, 0.3894, 0.3873, 0.2457, 0.4400, 0.9139, 0.3077, 0.0536,\n",
            "        0.0274, 0.6812, 0.5476, 0.2643, 0.9366, 0.5420, 0.1126, 0.1441, 0.0769,\n",
            "        0.7367, 0.1551, 0.2237, 0.2878, 0.4028, 0.3192, 0.0213, 0.4204, 0.6076,\n",
            "        0.1378, 0.9637, 0.2645, 0.0794, 0.3607, 0.0696, 0.1742, 0.0118, 0.5369,\n",
            "        0.8130, 0.2997, 0.1624, 0.6975, 0.5030, 0.1516, 0.0399, 0.3086, 0.4212,\n",
            "        0.1732, 0.7601, 0.1968, 0.6902, 0.6019, 0.5613, 0.5035, 0.5962, 0.8307,\n",
            "        0.1709, 0.1764, 0.0716, 0.1477, 0.8757, 0.6038, 0.5686, 0.1833, 0.5240,\n",
            "        0.9742, 0.8657, 0.6889, 0.9957, 0.3089, 0.7045, 0.1552, 0.5909, 0.4408,\n",
            "        0.5688, 0.3117, 0.0681, 0.7844, 0.1848, 0.6006, 0.8386, 0.6763, 0.0131,\n",
            "        0.3054, 0.5815, 0.3006, 0.5741, 0.5181, 0.4780, 0.8724, 0.9095, 0.1142,\n",
            "        0.0085, 0.0152, 0.9872, 0.0616, 0.0318, 0.6904, 0.1537, 0.9662, 0.0128,\n",
            "        0.1949, 0.1141, 0.8840, 0.1770, 0.7027, 0.2931, 0.0595, 0.1788, 0.7234,\n",
            "        0.3019, 0.7030, 0.4929, 0.8961, 0.9803, 0.6268, 0.7305, 0.0869, 0.7151,\n",
            "        0.3641, 0.9102, 0.3553, 0.9768, 0.7271, 0.9202, 0.5200, 0.3320, 0.8870,\n",
            "        0.0986, 0.6254, 0.9298, 0.5813, 0.8570, 0.9353, 0.3864, 0.1955, 0.6587,\n",
            "        0.5957, 0.8693, 0.9815, 0.7865, 0.0974, 0.4191, 0.6915, 0.8042, 0.9950,\n",
            "        0.9770, 0.9345, 0.5050, 0.4725, 0.1841, 0.3479, 0.9823, 0.8242, 0.6315,\n",
            "        0.8077, 0.8245, 0.9433, 0.8740, 0.2525, 0.3630, 0.9622, 0.4371, 0.8801,\n",
            "        0.1084, 0.4751, 0.4659, 0.3682, 0.9449, 0.2379, 0.1187, 0.6296, 0.5017,\n",
            "        0.7357, 0.0245, 0.6570, 0.3864, 0.9739, 0.4887, 0.9704, 0.8277, 0.6384,\n",
            "        0.3971, 0.3842, 0.4286, 0.6892, 0.1108, 0.0081, 0.7313, 0.3307, 0.6803,\n",
            "        0.4053, 0.8887, 0.7913, 0.1862, 0.1812, 0.9519, 0.7683, 0.0465, 0.9908,\n",
            "        0.8737, 0.9955, 0.7641, 0.7349, 0.4602, 0.2073, 0.1966, 0.6751, 0.5250,\n",
            "        0.0298, 0.8558, 0.7552, 0.6562, 0.0555, 0.5134, 0.4357, 0.2531, 0.0265,\n",
            "        0.9924, 0.4863, 0.1977, 0.4759, 0.2309, 0.2534, 0.7413, 0.3562, 0.9493,\n",
            "        0.6052, 0.1431, 0.9043, 0.1162, 0.5416, 0.5921, 0.7224, 0.0838, 0.6801,\n",
            "        0.5515, 0.6148, 0.2970, 0.2367, 0.8613, 0.7977, 0.2721, 0.4105, 0.4820,\n",
            "        0.5551, 0.5226, 0.0855, 0.4768, 0.3625, 0.7657, 0.9512, 0.4199, 0.7525,\n",
            "        0.5135, 0.9998, 0.8361, 0.8500, 0.0091, 0.4683, 0.0979, 0.8446, 0.2871,\n",
            "        0.3899, 0.3753, 0.4331, 0.9322, 0.4960, 0.6772, 0.3660, 0.5690, 0.3475,\n",
            "        0.1028, 0.6932, 0.9024, 0.4011, 0.2526, 0.6122, 0.2940, 0.3813, 0.3929,\n",
            "        0.2568, 0.2466, 0.3692, 0.4075, 0.3284, 0.1842, 0.9193, 0.7185, 0.4210,\n",
            "        0.7875, 0.3232, 0.2178, 0.8671, 0.1423, 0.2407, 0.2598, 0.5574, 0.3797,\n",
            "        0.2473, 0.6881, 0.2609, 0.6787, 0.7653, 0.6606, 0.5760, 0.7585, 0.4663,\n",
            "        0.4259], grad_fn=<SplitBackward0>), tensor([ 0.0680, -0.0439, -0.0869, -0.0808, -0.0229,  0.0532,  0.0594, -0.0275,\n",
            "        -0.0658, -0.0942, -0.0205, -0.0843, -0.0604,  0.0997,  0.0445,  0.0905,\n",
            "         0.0979, -0.0372, -0.0251,  0.0938,  0.0832, -0.0090,  0.0779,  0.0610,\n",
            "        -0.0801,  0.0922,  0.0098, -0.0119, -0.0167,  0.0256,  0.0973, -0.0709,\n",
            "         0.0827, -0.0640,  0.0623,  0.0003, -0.0216,  0.0437, -0.0109, -0.0489,\n",
            "        -0.0246,  0.0728, -0.0400, -0.0884, -0.0848,  0.0016,  0.0458, -0.0587,\n",
            "         0.0881, -0.0171,  0.0185,  0.0562,  0.0173, -0.0867,  0.0094, -0.0752,\n",
            "         0.0899, -0.0156, -0.0783,  0.0790, -0.0482, -0.0217, -0.0773,  0.0950,\n",
            "         0.0056, -0.0296,  0.0152,  0.0453,  0.0022,  0.0654, -0.0744,  0.0818,\n",
            "        -0.0614, -0.0218,  0.0317, -0.0403,  0.0891, -0.0293,  0.0360, -0.0529,\n",
            "         0.0725, -0.0317, -0.0115,  0.0686, -0.0921, -0.0679,  0.0372, -0.0591,\n",
            "        -0.0419,  0.0259, -0.0210, -0.0479,  0.0063, -0.0366,  0.0226, -0.0519,\n",
            "        -0.0444,  0.0570,  0.0509,  0.0292], grad_fn=<SplitBackward0>), tensor([0.1876, 0.8020, 0.0908, 0.6488, 0.0892, 0.2920, 0.2118, 0.4940, 0.5697,\n",
            "        0.7660, 0.4564, 0.2002, 0.1483, 0.7564, 0.5154, 0.5790, 0.9547, 0.7147,\n",
            "        0.1246, 0.6104, 0.8081, 0.1804, 0.2539, 0.0337, 0.1251, 0.4884, 0.4602,\n",
            "        0.8052, 0.9360, 0.8805, 0.4739, 0.2198, 0.2818, 0.5944, 0.7786, 0.2406,\n",
            "        0.2943, 0.5959, 0.9589, 0.8377, 0.1224, 0.2356, 0.0460, 0.0537, 0.9207,\n",
            "        0.3656, 0.6738, 0.9176, 0.9038, 0.7479, 0.9545, 0.5648, 0.7670, 0.7533,\n",
            "        0.8066, 0.2659, 0.8559, 0.8154, 0.1344, 0.8392, 0.6515, 0.7700, 0.3487,\n",
            "        0.3007, 0.2336, 0.6563, 0.5641, 0.6599, 0.0052, 0.6255, 0.3166, 0.6670,\n",
            "        0.1845, 0.0033, 0.4697, 0.3728, 0.2931, 0.6798, 0.8382, 0.6672, 0.7301,\n",
            "        0.8427, 0.1816, 0.8248, 0.7093, 0.4019, 0.6182, 0.7871, 0.4010, 0.7558,\n",
            "        0.2863, 0.3949, 0.1247, 0.2675, 0.4679, 0.3699, 0.6876, 0.6134, 0.2039,\n",
            "        0.5152], grad_fn=<SplitBackward0>), tensor([0.0516], grad_fn=<SplitBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert param_chunks to a numpy array\n",
        "param_chunks_np = np.concatenate([chunk.detach().numpy() for chunk in param_chunks])\n",
        "\n",
        "print(type(param_chunks_np))\n",
        "print(\"size:\", param_chunks_np.shape)"
      ],
      "metadata": {
        "id": "GS1CbLGJR9gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c981c7-a230-475f-d5c7-a76015f5bbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "size: (11301,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: learning a mapping from each chunk to an integer via VQ-VAE"
      ],
      "metadata": {
        "id": "Uo4o1jPJO08f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X6JeFrWrCi0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "\n",
        "from six.moves import xrange\n",
        "\n",
        "# import umap\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mce1mksSrCi0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0BgGzArCi0"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "using the weights of the above neural network as input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_variance = np.var(param_chunks_np / 255.0)\n",
        "\n",
        "print(data_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elD5efknVUrr",
        "outputId": "a56ebdbe-f5af-4a4f-a734-b344793ca24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3333508e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsAR4cQHrCi2"
      },
      "source": [
        "## Vector Quantizer Layer\n",
        "\n",
        "This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.\n",
        "\n",
        "The output tensor will have the same shape as the input.\n",
        "\n",
        "As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw84w9yPrCi3"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "        '''\n",
        "        # convert inputs from HW -> HW\n",
        "        inputs = inputs.permute(0, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 1).contiguous(), perplexity, encodings # (0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXrFrH_UrCi3"
      },
      "source": [
        "We will also implement a slightly modified version  which will use exponential moving averages to update the embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRx68eOsrCi3"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 1).contiguous() # (0, 2, 3, 1)\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 1).contiguous(), perplexity, encodings # (0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBBSF1p1rCi4"
      },
      "source": [
        "## Encoder & Decoder Architecture\n",
        "\n",
        "The encoder and decoder architecture is based on a ResNet and is implemented below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlplEblqrCi4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_features=in_channels,\n",
        "                      out_features=num_residual_hiddens),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_features=num_residual_hiddens,\n",
        "                      out_features=num_hiddens) # Replaced with nn.Linear\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "        # x = x.unsqueeze(2)  # Add an extra dimension\n",
        "        # output = x + self._block(x)\n",
        "        # return output.squeeze(2)  # Remove the extra dimension\n",
        "'''\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=in_channels, out_channels=num_residual_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)  # Add an extra dimension\n",
        "        output = x + self._block(x)\n",
        "        return output.squeeze(2)  # Remove the extra dimension\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUz-8SmDrCi4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._linear_1 = nn.Linear(in_channels*64, num_hiddens//2)\n",
        "        self._linear_2 = nn.Linear(num_hiddens//2, num_hiddens)\n",
        "\n",
        "\n",
        "        self._residual_stack = ResidualStack(in_channels=128, #num_hiddens,\n",
        "                                             num_hiddens=1,\n",
        "                                             num_residual_layers=1,\n",
        "                                             num_residual_hiddens=1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._linear_1(inputs.view(inputs.size(0), -1))\n",
        "        print(\"x=\", x.shape)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._linear_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return self._residual_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP6Yhd3MrCi5"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._linear_1 = nn.Linear(in_channels, num_hiddens)\n",
        "\n",
        "        self._residual_stack = ResidualStack(in_channels=128, #num_hiddens,\n",
        "                                             num_hiddens=1,\n",
        "                                             num_residual_layers=1,\n",
        "                                             num_residual_hiddens=1)\n",
        "\n",
        "        self._linear_2 = nn.Linear(num_hiddens, num_hiddens//2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._linear_1(inputs.view(inputs.size(0), -1))\n",
        "        print(\"x=\", x.shape)\n",
        "        x = self._residual_stack(x)\n",
        "\n",
        "        x = self._linear_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faiz3uawrCi5"
      },
      "source": [
        "## Train\n",
        "\n",
        "We use the hyperparameters from the author's code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IHhIF9crCi5"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_training_updates = 15000\n",
        "\n",
        "num_hiddens = 128\n",
        "num_residual_hiddens = 32\n",
        "num_residual_layers = 2\n",
        "\n",
        "embedding_dim = 64\n",
        "num_embeddings = 512\n",
        "\n",
        "commitment_cost = 0.25\n",
        "\n",
        "decay = 0.99\n",
        "\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW6vZgTorCi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "5076dc89-f6a4-4662-cca0-8db26f88635a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7af8352ffaf0>\n",
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7af8352fc0a0>\n",
            "ori_data= [tensor([0.4888, 0.0474, 0.2346, 0.6123, 0.8177, 0.4739, 0.9732, 0.6205, 0.1436,\n",
            "        0.0986, 0.5187, 0.0252, 0.7701, 0.9893, 0.9328, 0.4948, 0.3099, 0.2192,\n",
            "        0.5352, 0.1977, 0.8090, 0.8964, 0.4286, 0.2837, 0.0108, 0.5531, 0.9424,\n",
            "        0.3177, 0.3435, 0.4505, 0.4231, 0.5755, 0.7621, 0.2150, 0.4349, 0.4824,\n",
            "        0.1983, 0.1066, 0.3323, 0.1205, 0.9500, 0.1452, 0.6512, 0.8441, 0.4323,\n",
            "        0.8228, 0.2539, 0.3342, 0.7360, 0.8981, 0.3812, 0.7564, 0.3739, 0.6766,\n",
            "        0.5419, 0.0210, 0.5698, 0.5816, 0.9289, 0.0235, 0.7799, 0.8610, 0.0456,\n",
            "        0.4263])]\n",
            "<class 'list'>\n",
            "data= tensor([[ 0.0951,  0.6299,  0.4237,  0.3078,  0.7555, -0.1972,  0.6487,  0.7891,\n",
            "          0.2252,  0.3612,  0.5262,  0.2401,  0.6173,  0.0974,  0.2855,  0.2621,\n",
            "          0.8607,  0.5508,  0.4699,  0.1263,  0.2584,  0.4404,  0.1232,  0.3165,\n",
            "          0.8242,  0.8981,  0.5285,  0.6078,  0.2627,  0.3649,  0.4990,  0.4889,\n",
            "          0.9423,  0.7345,  0.6285,  0.9149,  0.7631,  0.1586,  0.3904,  0.0386,\n",
            "          0.8214,  0.6570,  0.0888,  0.4476,  0.9286,  0.2846,  0.2712,  0.4822,\n",
            "          0.9342,  0.6385,  0.7798,  0.4725,  0.9131, -0.0684,  0.1598,  0.0482,\n",
            "          0.9471,  0.5985,  0.1245,  0.9934,  0.3301,  0.3503,  0.1066,  0.3243]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor batch_idx, data in enumerate(training_loader):\\n    print(\"Batch Index:\", batch_idx)\\n    print(\"Data:\", data)\\n    print()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a TensorDataset from param_chunks_np\n",
        "dataset = TensorDataset(torch.from_numpy(param_chunks_np))\n",
        "\n",
        "# Set the batch size and other DataLoader parameters\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "pin_memory = True\n",
        "\n",
        "# Create the DataLoader using the custom dataset\n",
        "training_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
        "\n",
        "print(training_loader)\n",
        "\n",
        "print(iter(training_loader))\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "\n",
        "print(\"ori_data=\", data)\n",
        "\n",
        "print(type(next(iter(training_loader))))\n",
        "# print(data[0])\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "# for i in range(len(data)):\n",
        "#    data[i] = data[i].to(device)\n",
        "data = torch.stack(data).to(device)\n",
        "print(\"data=\", data)\n",
        "\n",
        "# There's no label in the NN weight dataset\n",
        "'''\n",
        "for batch_idx, data in enumerate(training_loader):\n",
        "    print(\"Batch Index:\", batch_idx)\n",
        "    print(\"Data:\", data)\n",
        "    print()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))\n",
        "\n",
        "cifar_training_loader = DataLoader(cifar_training_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True,\n",
        "                             pin_memory=True)\n",
        "\n",
        "print(cifar_training_loader)\n",
        "\n",
        "print(iter(cifar_training_loader))\n",
        "\n",
        "(data, _) = next(iter(cifar_training_loader))\n",
        "print(type(next(iter(cifar_training_loader))))\n",
        "print(type(data))\n",
        "print(type(_))\n",
        "print(\"ori_data=\", data)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "print(\"data=\", data)\n",
        "\n",
        "'''\n",
        "for batch_idx, (data, _) in enumerate(cifar_training_loader):\n",
        "    print(\"Batch Index:\", batch_idx)\n",
        "    # print(\"Data:\", data)\n",
        "    print(\"Label:\", _)\n",
        "    print()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yemq6mH1XYaz",
        "outputId": "a9dd1b07-de78-4d5c-c7b1-3012ef521854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7af835315120>\n",
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7af83fb9af50>\n",
            "<class 'list'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "ori_data= tensor([[[[-0.1863, -0.1824, -0.1706,  ..., -0.1863, -0.1980, -0.2098],\n",
            "          [-0.1627, -0.1549, -0.1431,  ..., -0.1745, -0.1863, -0.1980],\n",
            "          [-0.1510, -0.1431, -0.1314,  ..., -0.1667, -0.1784, -0.1863],\n",
            "          ...,\n",
            "          [-0.1471, -0.1431, -0.1353,  ..., -0.1471, -0.1588, -0.1706],\n",
            "          [-0.1353, -0.1314, -0.1275,  ..., -0.1510, -0.1588, -0.1706],\n",
            "          [-0.1510, -0.1431, -0.1275,  ..., -0.1510, -0.1588, -0.1667]],\n",
            "\n",
            "         [[-0.0647, -0.0569, -0.0451,  ..., -0.0843, -0.0961, -0.1078],\n",
            "          [-0.0490, -0.0412, -0.0255,  ..., -0.0725, -0.0843, -0.0961],\n",
            "          [-0.0412, -0.0333, -0.0216,  ..., -0.0647, -0.0765, -0.0843],\n",
            "          ...,\n",
            "          [-0.0294, -0.0255, -0.0176,  ..., -0.0529, -0.0647, -0.0765],\n",
            "          [-0.0333, -0.0294, -0.0216,  ..., -0.0569, -0.0647, -0.0765],\n",
            "          [-0.0569, -0.0490, -0.0333,  ..., -0.0569, -0.0647, -0.0725]],\n",
            "\n",
            "         [[-0.2804, -0.2725, -0.2608,  ..., -0.2765, -0.2843, -0.2961],\n",
            "          [-0.2608, -0.2529, -0.2373,  ..., -0.2647, -0.2725, -0.2843],\n",
            "          [-0.2529, -0.2451, -0.2294,  ..., -0.2529, -0.2647, -0.2725],\n",
            "          ...,\n",
            "          [-0.2725, -0.2765, -0.2804,  ..., -0.2804, -0.2922, -0.3039],\n",
            "          [-0.2647, -0.2686, -0.2686,  ..., -0.2843, -0.2922, -0.3039],\n",
            "          [-0.2569, -0.2569, -0.2529,  ..., -0.2843, -0.2922, -0.3000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4608,  0.4569,  0.4569,  ...,  0.0961,  0.0451, -0.1157],\n",
            "          [ 0.4961,  0.4804,  0.4843,  ..., -0.0922, -0.1118, -0.1588],\n",
            "          [ 0.5000,  0.4961,  0.5000,  ..., -0.0490, -0.1588, -0.1941],\n",
            "          ...,\n",
            "          [-0.0176, -0.0333, -0.0373,  ...,  0.1039,  0.1118,  0.1078],\n",
            "          [-0.0216, -0.0216, -0.0255,  ...,  0.0765,  0.0804,  0.0843],\n",
            "          [-0.0333, -0.0098, -0.0020,  ...,  0.0725,  0.0686,  0.0490]],\n",
            "\n",
            "         [[ 0.4608,  0.4569,  0.4569,  ...,  0.1275,  0.0608, -0.1000],\n",
            "          [ 0.4961,  0.4804,  0.4843,  ..., -0.0647, -0.0922, -0.1392],\n",
            "          [ 0.5000,  0.4961,  0.5000,  ..., -0.0412, -0.1392, -0.1784],\n",
            "          ...,\n",
            "          [-0.0255, -0.0412, -0.0451,  ...,  0.0686,  0.0804,  0.0725],\n",
            "          [-0.0333, -0.0333, -0.0373,  ...,  0.0451,  0.0490,  0.0529],\n",
            "          [-0.0490, -0.0294, -0.0216,  ...,  0.0412,  0.0333,  0.0294]],\n",
            "\n",
            "         [[ 0.4608,  0.4569,  0.4569,  ...,  0.0804, -0.0059, -0.2333],\n",
            "          [ 0.4961,  0.4765,  0.4843,  ..., -0.1980, -0.2647, -0.3039],\n",
            "          [ 0.5000,  0.4922,  0.4961,  ..., -0.0686, -0.2882, -0.3039],\n",
            "          ...,\n",
            "          [-0.0137, -0.0294, -0.0333,  ...,  0.0647,  0.0765,  0.0765],\n",
            "          [-0.0216, -0.0216, -0.0255,  ...,  0.0373,  0.0451,  0.0569],\n",
            "          [-0.0333, -0.0216, -0.0137,  ...,  0.0451,  0.0373,  0.0333]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3431,  0.3353,  0.3314,  ...,  0.3549,  0.3588,  0.3471],\n",
            "          [ 0.4882,  0.4843,  0.4882,  ...,  0.5000,  0.5000,  0.4961],\n",
            "          [ 0.4412,  0.4451,  0.4451,  ...,  0.4608,  0.4569,  0.4686],\n",
            "          ...,\n",
            "          [-0.0412, -0.0608, -0.0725,  ..., -0.1000, -0.0961, -0.0961],\n",
            "          [ 0.0059, -0.0176, -0.0412,  ..., -0.0765, -0.0804, -0.0725],\n",
            "          [-0.0451, -0.0608, -0.0765,  ..., -0.0922, -0.0922, -0.0882]],\n",
            "\n",
            "         [[ 0.2490,  0.2373,  0.2373,  ...,  0.2647,  0.2647,  0.2569],\n",
            "          [ 0.4412,  0.4412,  0.4451,  ...,  0.4647,  0.4608,  0.4569],\n",
            "          [ 0.4098,  0.4176,  0.4216,  ...,  0.4373,  0.4294,  0.4412],\n",
            "          ...,\n",
            "          [-0.0647, -0.0725, -0.0804,  ..., -0.1196, -0.1000, -0.1157],\n",
            "          [-0.0529, -0.0686, -0.0882,  ..., -0.1235, -0.1235, -0.1275],\n",
            "          [-0.1431, -0.1510, -0.1667,  ..., -0.1784, -0.1784, -0.1824]],\n",
            "\n",
            "         [[ 0.4922,  0.4961,  0.4961,  ...,  0.4961,  0.4961,  0.4961],\n",
            "          [ 0.4765,  0.4725,  0.4608,  ...,  0.4725,  0.4804,  0.4843],\n",
            "          [ 0.3588,  0.3431,  0.3275,  ...,  0.3549,  0.3588,  0.3784],\n",
            "          ...,\n",
            "          [-0.0373, -0.0529, -0.0569,  ..., -0.0961, -0.0843, -0.0647],\n",
            "          [ 0.1000,  0.0451,  0.0255,  ..., -0.0255, -0.0216,  0.0255],\n",
            "          [ 0.2216,  0.1627,  0.1431,  ...,  0.1157,  0.1196,  0.1706]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.4804,  0.4843,  0.4843,  ...,  0.4569,  0.4569,  0.4569],\n",
            "          [ 0.4765,  0.4608,  0.4529,  ...,  0.4608,  0.4569,  0.4608],\n",
            "          [ 0.4765,  0.4569,  0.4569,  ...,  0.4804,  0.4804,  0.4843],\n",
            "          ...,\n",
            "          [ 0.2216,  0.2176,  0.2333,  ...,  0.1902,  0.2529,  0.2529],\n",
            "          [ 0.2294,  0.2216,  0.2294,  ...,  0.2137,  0.2333,  0.2529],\n",
            "          [ 0.2608,  0.2412,  0.2333,  ...,  0.1627,  0.2647,  0.2725]],\n",
            "\n",
            "         [[ 0.4843,  0.4882,  0.4882,  ...,  0.4843,  0.4843,  0.4843],\n",
            "          [ 0.4804,  0.4647,  0.4569,  ...,  0.4765,  0.4725,  0.4804],\n",
            "          [ 0.4804,  0.4608,  0.4608,  ...,  0.4882,  0.4882,  0.4922],\n",
            "          ...,\n",
            "          [ 0.2569,  0.2529,  0.2686,  ...,  0.2176,  0.2843,  0.2804],\n",
            "          [ 0.2647,  0.2569,  0.2647,  ...,  0.2451,  0.2647,  0.2804],\n",
            "          [ 0.2961,  0.2765,  0.2686,  ...,  0.1941,  0.2922,  0.3000]],\n",
            "\n",
            "         [[ 0.4922,  0.4961,  0.4961,  ...,  0.5000,  0.5000,  0.5000],\n",
            "          [ 0.4882,  0.4725,  0.4647,  ...,  0.4882,  0.4882,  0.4922],\n",
            "          [ 0.4882,  0.4686,  0.4686,  ...,  0.4961,  0.4961,  0.5000],\n",
            "          ...,\n",
            "          [ 0.2882,  0.2804,  0.2961,  ...,  0.2255,  0.3000,  0.3078],\n",
            "          [ 0.2922,  0.2843,  0.2922,  ...,  0.2529,  0.2765,  0.3039],\n",
            "          [ 0.3235,  0.3039,  0.2961,  ...,  0.1980,  0.3118,  0.3275]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4882,  0.4333,  0.4373,  ...,  0.1902,  0.2176,  0.1118],\n",
            "          [ 0.4569,  0.4059,  0.4098,  ...,  0.2176,  0.2922,  0.1706],\n",
            "          [ 0.3745,  0.3745,  0.3824,  ...,  0.2098,  0.1588,  0.0216],\n",
            "          ...,\n",
            "          [ 0.1392,  0.1392,  0.1353,  ..., -0.0804, -0.1353, -0.1431],\n",
            "          [ 0.1353,  0.1392,  0.1471,  ..., -0.1314, -0.2020, -0.1902],\n",
            "          [ 0.1471,  0.1549,  0.1588,  ..., -0.0961, -0.1431, -0.1627]],\n",
            "\n",
            "         [[ 0.5000,  0.4490,  0.4490,  ...,  0.1431,  0.1353, -0.0137],\n",
            "          [ 0.4725,  0.4216,  0.4255,  ...,  0.1549,  0.1902,  0.0255],\n",
            "          [ 0.3902,  0.3863,  0.3980,  ...,  0.1039,  0.0333, -0.1392],\n",
            "          ...,\n",
            "          [ 0.1980,  0.1980,  0.1980,  ..., -0.1078, -0.1902, -0.2020],\n",
            "          [ 0.1824,  0.1863,  0.1941,  ..., -0.1157, -0.2098, -0.2098],\n",
            "          [ 0.1824,  0.1941,  0.1941,  ..., -0.0608, -0.1275, -0.1510]],\n",
            "\n",
            "         [[ 0.5000,  0.4608,  0.4608,  ...,  0.0922,  0.0569, -0.1588],\n",
            "          [ 0.4765,  0.4294,  0.4333,  ...,  0.0922,  0.1078, -0.1157],\n",
            "          [ 0.3980,  0.3980,  0.4098,  ...,  0.0059, -0.0686, -0.2725],\n",
            "          ...,\n",
            "          [ 0.2451,  0.2451,  0.2412,  ..., -0.1431, -0.2451, -0.2647],\n",
            "          [ 0.2216,  0.2255,  0.2294,  ..., -0.1039, -0.2216, -0.2216],\n",
            "          [ 0.2098,  0.2216,  0.2216,  ..., -0.0255, -0.1078, -0.1353]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2176,  0.2176,  0.2294,  ...,  0.2216,  0.2137,  0.2020],\n",
            "          [-0.1039, -0.1157, -0.1157,  ...,  0.2294,  0.2176,  0.2059],\n",
            "          [-0.4490, -0.4490, -0.4490,  ...,  0.2451,  0.2294,  0.2176],\n",
            "          ...,\n",
            "          [-0.1000, -0.0765,  0.0020,  ...,  0.2216,  0.2098,  0.2020],\n",
            "          [-0.1157, -0.0725, -0.0294,  ...,  0.2098,  0.2020,  0.1980],\n",
            "          [-0.1431, -0.0647,  0.0020,  ...,  0.1980,  0.1941,  0.1902]],\n",
            "\n",
            "         [[ 0.1980,  0.1980,  0.2059,  ...,  0.1941,  0.1863,  0.1745],\n",
            "          [-0.1000, -0.1118, -0.1118,  ...,  0.2020,  0.1902,  0.1784],\n",
            "          [-0.4412, -0.4412, -0.4412,  ...,  0.2176,  0.2020,  0.1902],\n",
            "          ...,\n",
            "          [-0.1941, -0.2098, -0.2725,  ...,  0.2255,  0.2059,  0.1863],\n",
            "          [-0.2059, -0.2098, -0.3118,  ...,  0.2137,  0.1980,  0.1824],\n",
            "          [-0.2294, -0.2059, -0.2882,  ...,  0.2020,  0.1902,  0.1745]],\n",
            "\n",
            "         [[ 0.1941,  0.1941,  0.2059,  ...,  0.1706,  0.1627,  0.1510],\n",
            "          [-0.1078, -0.1196, -0.1235,  ...,  0.1784,  0.1667,  0.1549],\n",
            "          [-0.4529, -0.4529, -0.4529,  ...,  0.1941,  0.1784,  0.1667],\n",
            "          ...,\n",
            "          [-0.2569, -0.2765, -0.3392,  ...,  0.1941,  0.1745,  0.1588],\n",
            "          [-0.2686, -0.2765, -0.3745,  ...,  0.1824,  0.1667,  0.1588],\n",
            "          [-0.2922, -0.2686, -0.3471,  ...,  0.1706,  0.1588,  0.1471]]]])\n",
            "data= tensor([[[[-0.1863, -0.1824, -0.1706,  ..., -0.1863, -0.1980, -0.2098],\n",
            "          [-0.1627, -0.1549, -0.1431,  ..., -0.1745, -0.1863, -0.1980],\n",
            "          [-0.1510, -0.1431, -0.1314,  ..., -0.1667, -0.1784, -0.1863],\n",
            "          ...,\n",
            "          [-0.1471, -0.1431, -0.1353,  ..., -0.1471, -0.1588, -0.1706],\n",
            "          [-0.1353, -0.1314, -0.1275,  ..., -0.1510, -0.1588, -0.1706],\n",
            "          [-0.1510, -0.1431, -0.1275,  ..., -0.1510, -0.1588, -0.1667]],\n",
            "\n",
            "         [[-0.0647, -0.0569, -0.0451,  ..., -0.0843, -0.0961, -0.1078],\n",
            "          [-0.0490, -0.0412, -0.0255,  ..., -0.0725, -0.0843, -0.0961],\n",
            "          [-0.0412, -0.0333, -0.0216,  ..., -0.0647, -0.0765, -0.0843],\n",
            "          ...,\n",
            "          [-0.0294, -0.0255, -0.0176,  ..., -0.0529, -0.0647, -0.0765],\n",
            "          [-0.0333, -0.0294, -0.0216,  ..., -0.0569, -0.0647, -0.0765],\n",
            "          [-0.0569, -0.0490, -0.0333,  ..., -0.0569, -0.0647, -0.0725]],\n",
            "\n",
            "         [[-0.2804, -0.2725, -0.2608,  ..., -0.2765, -0.2843, -0.2961],\n",
            "          [-0.2608, -0.2529, -0.2373,  ..., -0.2647, -0.2725, -0.2843],\n",
            "          [-0.2529, -0.2451, -0.2294,  ..., -0.2529, -0.2647, -0.2725],\n",
            "          ...,\n",
            "          [-0.2725, -0.2765, -0.2804,  ..., -0.2804, -0.2922, -0.3039],\n",
            "          [-0.2647, -0.2686, -0.2686,  ..., -0.2843, -0.2922, -0.3039],\n",
            "          [-0.2569, -0.2569, -0.2529,  ..., -0.2843, -0.2922, -0.3000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4608,  0.4569,  0.4569,  ...,  0.0961,  0.0451, -0.1157],\n",
            "          [ 0.4961,  0.4804,  0.4843,  ..., -0.0922, -0.1118, -0.1588],\n",
            "          [ 0.5000,  0.4961,  0.5000,  ..., -0.0490, -0.1588, -0.1941],\n",
            "          ...,\n",
            "          [-0.0176, -0.0333, -0.0373,  ...,  0.1039,  0.1118,  0.1078],\n",
            "          [-0.0216, -0.0216, -0.0255,  ...,  0.0765,  0.0804,  0.0843],\n",
            "          [-0.0333, -0.0098, -0.0020,  ...,  0.0725,  0.0686,  0.0490]],\n",
            "\n",
            "         [[ 0.4608,  0.4569,  0.4569,  ...,  0.1275,  0.0608, -0.1000],\n",
            "          [ 0.4961,  0.4804,  0.4843,  ..., -0.0647, -0.0922, -0.1392],\n",
            "          [ 0.5000,  0.4961,  0.5000,  ..., -0.0412, -0.1392, -0.1784],\n",
            "          ...,\n",
            "          [-0.0255, -0.0412, -0.0451,  ...,  0.0686,  0.0804,  0.0725],\n",
            "          [-0.0333, -0.0333, -0.0373,  ...,  0.0451,  0.0490,  0.0529],\n",
            "          [-0.0490, -0.0294, -0.0216,  ...,  0.0412,  0.0333,  0.0294]],\n",
            "\n",
            "         [[ 0.4608,  0.4569,  0.4569,  ...,  0.0804, -0.0059, -0.2333],\n",
            "          [ 0.4961,  0.4765,  0.4843,  ..., -0.1980, -0.2647, -0.3039],\n",
            "          [ 0.5000,  0.4922,  0.4961,  ..., -0.0686, -0.2882, -0.3039],\n",
            "          ...,\n",
            "          [-0.0137, -0.0294, -0.0333,  ...,  0.0647,  0.0765,  0.0765],\n",
            "          [-0.0216, -0.0216, -0.0255,  ...,  0.0373,  0.0451,  0.0569],\n",
            "          [-0.0333, -0.0216, -0.0137,  ...,  0.0451,  0.0373,  0.0333]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3431,  0.3353,  0.3314,  ...,  0.3549,  0.3588,  0.3471],\n",
            "          [ 0.4882,  0.4843,  0.4882,  ...,  0.5000,  0.5000,  0.4961],\n",
            "          [ 0.4412,  0.4451,  0.4451,  ...,  0.4608,  0.4569,  0.4686],\n",
            "          ...,\n",
            "          [-0.0412, -0.0608, -0.0725,  ..., -0.1000, -0.0961, -0.0961],\n",
            "          [ 0.0059, -0.0176, -0.0412,  ..., -0.0765, -0.0804, -0.0725],\n",
            "          [-0.0451, -0.0608, -0.0765,  ..., -0.0922, -0.0922, -0.0882]],\n",
            "\n",
            "         [[ 0.2490,  0.2373,  0.2373,  ...,  0.2647,  0.2647,  0.2569],\n",
            "          [ 0.4412,  0.4412,  0.4451,  ...,  0.4647,  0.4608,  0.4569],\n",
            "          [ 0.4098,  0.4176,  0.4216,  ...,  0.4373,  0.4294,  0.4412],\n",
            "          ...,\n",
            "          [-0.0647, -0.0725, -0.0804,  ..., -0.1196, -0.1000, -0.1157],\n",
            "          [-0.0529, -0.0686, -0.0882,  ..., -0.1235, -0.1235, -0.1275],\n",
            "          [-0.1431, -0.1510, -0.1667,  ..., -0.1784, -0.1784, -0.1824]],\n",
            "\n",
            "         [[ 0.4922,  0.4961,  0.4961,  ...,  0.4961,  0.4961,  0.4961],\n",
            "          [ 0.4765,  0.4725,  0.4608,  ...,  0.4725,  0.4804,  0.4843],\n",
            "          [ 0.3588,  0.3431,  0.3275,  ...,  0.3549,  0.3588,  0.3784],\n",
            "          ...,\n",
            "          [-0.0373, -0.0529, -0.0569,  ..., -0.0961, -0.0843, -0.0647],\n",
            "          [ 0.1000,  0.0451,  0.0255,  ..., -0.0255, -0.0216,  0.0255],\n",
            "          [ 0.2216,  0.1627,  0.1431,  ...,  0.1157,  0.1196,  0.1706]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.4804,  0.4843,  0.4843,  ...,  0.4569,  0.4569,  0.4569],\n",
            "          [ 0.4765,  0.4608,  0.4529,  ...,  0.4608,  0.4569,  0.4608],\n",
            "          [ 0.4765,  0.4569,  0.4569,  ...,  0.4804,  0.4804,  0.4843],\n",
            "          ...,\n",
            "          [ 0.2216,  0.2176,  0.2333,  ...,  0.1902,  0.2529,  0.2529],\n",
            "          [ 0.2294,  0.2216,  0.2294,  ...,  0.2137,  0.2333,  0.2529],\n",
            "          [ 0.2608,  0.2412,  0.2333,  ...,  0.1627,  0.2647,  0.2725]],\n",
            "\n",
            "         [[ 0.4843,  0.4882,  0.4882,  ...,  0.4843,  0.4843,  0.4843],\n",
            "          [ 0.4804,  0.4647,  0.4569,  ...,  0.4765,  0.4725,  0.4804],\n",
            "          [ 0.4804,  0.4608,  0.4608,  ...,  0.4882,  0.4882,  0.4922],\n",
            "          ...,\n",
            "          [ 0.2569,  0.2529,  0.2686,  ...,  0.2176,  0.2843,  0.2804],\n",
            "          [ 0.2647,  0.2569,  0.2647,  ...,  0.2451,  0.2647,  0.2804],\n",
            "          [ 0.2961,  0.2765,  0.2686,  ...,  0.1941,  0.2922,  0.3000]],\n",
            "\n",
            "         [[ 0.4922,  0.4961,  0.4961,  ...,  0.5000,  0.5000,  0.5000],\n",
            "          [ 0.4882,  0.4725,  0.4647,  ...,  0.4882,  0.4882,  0.4922],\n",
            "          [ 0.4882,  0.4686,  0.4686,  ...,  0.4961,  0.4961,  0.5000],\n",
            "          ...,\n",
            "          [ 0.2882,  0.2804,  0.2961,  ...,  0.2255,  0.3000,  0.3078],\n",
            "          [ 0.2922,  0.2843,  0.2922,  ...,  0.2529,  0.2765,  0.3039],\n",
            "          [ 0.3235,  0.3039,  0.2961,  ...,  0.1980,  0.3118,  0.3275]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4882,  0.4333,  0.4373,  ...,  0.1902,  0.2176,  0.1118],\n",
            "          [ 0.4569,  0.4059,  0.4098,  ...,  0.2176,  0.2922,  0.1706],\n",
            "          [ 0.3745,  0.3745,  0.3824,  ...,  0.2098,  0.1588,  0.0216],\n",
            "          ...,\n",
            "          [ 0.1392,  0.1392,  0.1353,  ..., -0.0804, -0.1353, -0.1431],\n",
            "          [ 0.1353,  0.1392,  0.1471,  ..., -0.1314, -0.2020, -0.1902],\n",
            "          [ 0.1471,  0.1549,  0.1588,  ..., -0.0961, -0.1431, -0.1627]],\n",
            "\n",
            "         [[ 0.5000,  0.4490,  0.4490,  ...,  0.1431,  0.1353, -0.0137],\n",
            "          [ 0.4725,  0.4216,  0.4255,  ...,  0.1549,  0.1902,  0.0255],\n",
            "          [ 0.3902,  0.3863,  0.3980,  ...,  0.1039,  0.0333, -0.1392],\n",
            "          ...,\n",
            "          [ 0.1980,  0.1980,  0.1980,  ..., -0.1078, -0.1902, -0.2020],\n",
            "          [ 0.1824,  0.1863,  0.1941,  ..., -0.1157, -0.2098, -0.2098],\n",
            "          [ 0.1824,  0.1941,  0.1941,  ..., -0.0608, -0.1275, -0.1510]],\n",
            "\n",
            "         [[ 0.5000,  0.4608,  0.4608,  ...,  0.0922,  0.0569, -0.1588],\n",
            "          [ 0.4765,  0.4294,  0.4333,  ...,  0.0922,  0.1078, -0.1157],\n",
            "          [ 0.3980,  0.3980,  0.4098,  ...,  0.0059, -0.0686, -0.2725],\n",
            "          ...,\n",
            "          [ 0.2451,  0.2451,  0.2412,  ..., -0.1431, -0.2451, -0.2647],\n",
            "          [ 0.2216,  0.2255,  0.2294,  ..., -0.1039, -0.2216, -0.2216],\n",
            "          [ 0.2098,  0.2216,  0.2216,  ..., -0.0255, -0.1078, -0.1353]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2176,  0.2176,  0.2294,  ...,  0.2216,  0.2137,  0.2020],\n",
            "          [-0.1039, -0.1157, -0.1157,  ...,  0.2294,  0.2176,  0.2059],\n",
            "          [-0.4490, -0.4490, -0.4490,  ...,  0.2451,  0.2294,  0.2176],\n",
            "          ...,\n",
            "          [-0.1000, -0.0765,  0.0020,  ...,  0.2216,  0.2098,  0.2020],\n",
            "          [-0.1157, -0.0725, -0.0294,  ...,  0.2098,  0.2020,  0.1980],\n",
            "          [-0.1431, -0.0647,  0.0020,  ...,  0.1980,  0.1941,  0.1902]],\n",
            "\n",
            "         [[ 0.1980,  0.1980,  0.2059,  ...,  0.1941,  0.1863,  0.1745],\n",
            "          [-0.1000, -0.1118, -0.1118,  ...,  0.2020,  0.1902,  0.1784],\n",
            "          [-0.4412, -0.4412, -0.4412,  ...,  0.2176,  0.2020,  0.1902],\n",
            "          ...,\n",
            "          [-0.1941, -0.2098, -0.2725,  ...,  0.2255,  0.2059,  0.1863],\n",
            "          [-0.2059, -0.2098, -0.3118,  ...,  0.2137,  0.1980,  0.1824],\n",
            "          [-0.2294, -0.2059, -0.2882,  ...,  0.2020,  0.1902,  0.1745]],\n",
            "\n",
            "         [[ 0.1941,  0.1941,  0.2059,  ...,  0.1706,  0.1627,  0.1510],\n",
            "          [-0.1078, -0.1196, -0.1235,  ...,  0.1784,  0.1667,  0.1549],\n",
            "          [-0.4529, -0.4529, -0.4529,  ...,  0.1941,  0.1784,  0.1667],\n",
            "          ...,\n",
            "          [-0.2569, -0.2765, -0.3392,  ...,  0.1941,  0.1745,  0.1588],\n",
            "          [-0.2686, -0.2765, -0.3745,  ...,  0.1824,  0.1667,  0.1588],\n",
            "          [-0.2922, -0.2686, -0.3471,  ...,  0.1706,  0.1588,  0.1471]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor batch_idx, (data, _) in enumerate(cifar_training_loader):\\n    print(\"Batch Index:\", batch_idx)\\n    # print(\"Data:\", data)\\n    print(\"Label:\", _)\\n    print()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTLi8nUrCi6"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Updated input size to [1, 64]\n",
        "        self._encoder = Encoder(1, num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "        print(self._encoder)\n",
        "\n",
        "        # Replaced with nn.Linear\n",
        "        self._pre_vq_linear = nn.Linear(num_hiddens, embedding_dim)\n",
        "\n",
        "        print(self._pre_vq_linear)\n",
        "\n",
        "        if decay > 0.0:\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                              commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
        "\n",
        "        print(self._vq_vae)\n",
        "\n",
        "        self._decoder = Decoder(embedding_dim,\n",
        "                                num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "        print(self._decoder)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.unsqueeze(0)  # Add an extra dimension for batch size\n",
        "        ## print(\"x=\", x)\n",
        "        ## print(x.shape)\n",
        "        z = self._encoder(x)\n",
        "        z = z.view(z.size(0), -1)  # Flatten the tensor\n",
        "        ## print(\"z0=\", z)\n",
        "        # Replaced self._pre_vq_conv with self._pre_vq_linear\n",
        "        z = self._pre_vq_linear(z)\n",
        "        ## print(\"z1=\", z)\n",
        "\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        ## print(\"quantized:\", quantized)\n",
        "\n",
        "        # # Reshape quantized before passing it to the decoder\n",
        "        # quantized = quantized.unsqueeze(2).unsqueeze(3)\n",
        "        # print(\"quantized_after:\", quantized)\n",
        "        x_recon = self._decoder(quantized)\n",
        "\n",
        "        # x_recon = 0\n",
        "        return loss, x_recon, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGDClXkCrCi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b14eb6-8600-46ea-962e-61d23baf8dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (_linear_1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (_linear_2): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (_residual_stack): ResidualStack(\n",
            "    (_layers): ModuleList(\n",
            "      (0): Residual(\n",
            "        (_block): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv1d(128, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Linear(in_features=128, out_features=64, bias=True)\n",
            "VectorQuantizerEMA(\n",
            "  (_embedding): Embedding(512, 64)\n",
            ")\n",
            "Decoder(\n",
            "  (_linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (_residual_stack): ResidualStack(\n",
            "    (_layers): ModuleList(\n",
            "      (0): Residual(\n",
            "        (_block): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv1d(128, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (_linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "              num_embeddings, embedding_dim,\n",
        "              commitment_cost, decay).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hRFXzlrCi6"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LeKJtXg-rCi7",
        "outputId": "143927c9-94cd-4d1b-d48a-a899261a02bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64])\n",
            "x= torch.Size([1, 64])\n",
            "x= torch.Size([1, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in ReluBackward0. Traceback of forward call that caused the error:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-167-cb412821bc76>\", line 15, in <cell line: 5>\n",
            "    vq_loss, data_recon, perplexity = model(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-164-51fffe4b7ce7>\", line 35, in forward\n",
            "    z = self._encoder(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-159-984169f64b4c>\", line 21, in forward\n",
            "    x = F.relu(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1457, in relu\n",
            "    result = torch.relu(input)\n",
            " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-cb412821bc76>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_error\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvq_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 128]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "train_res_recon_error = []\n",
        "train_res_perplexity = []\n",
        "\n",
        "for i in xrange(num_training_updates):\n",
        "    # (data, _) = next(iter(training_loader))\n",
        "    # data = data.to(device)\n",
        "    data = next(iter(training_loader))\n",
        "    data = torch.stack(data).to(device)\n",
        "    #for i in range(len(data)):\n",
        "    #  data[i] = data[i].to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print(data.shape)\n",
        "    vq_loss, data_recon, perplexity = model(data)\n",
        "    recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "    torch.autograd.set_detect_anomaly(True) # for debug\n",
        "    loss = recon_error + vq_loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_res_recon_error.append(recon_error.item())\n",
        "    train_res_perplexity.append(perplexity.item())\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "        print('%d iterations' % (i+1))\n",
        "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
        "        % print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*The reason of the error:*\n",
        "\n",
        "When using the ReLU activation function, if the input of a neuron is negative, it causes the neuron to output zero constantly, resulting in deactivation. Since the gradient is zero at this point, it cannot recover.\n",
        "\n"
      ],
      "metadata": {
        "id": "6-vxEsMNKzpW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szkLa9JcrCi7"
      },
      "source": [
        "## Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__AOkue0rCi7"
      },
      "outputs": [],
      "source": [
        "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
        "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sp67spXrCi7"
      },
      "outputs": [],
      "source": [
        "f = plt.figure(figsize=(16,8))\n",
        "ax = f.add_subplot(1,2,1)\n",
        "\n",
        "ax.plot(train_res_recon_error_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed NMSE.')\n",
        "ax.set_xlabel('iteration')\n",
        "'''\n",
        "ax = f.add_subplot(1,2,2)\n",
        "ax.plot(train_res_perplexity_smooth)\n",
        "ax.set_title('Smoothed Average codebook usage (perplexity).')\n",
        "ax.set_xlabel('iteration')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMbb0fPrCi7"
      },
      "source": [
        "## View Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnVCwvK8rCi8"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "train_originals = torch.stack(data).to(device)\n",
        "vq_output_eval = model._pre_vq_linear(model._encoder(train_originals))\n",
        "print(\"vq_output_eval=\", vq_output_eval)\n",
        "print(\"vq_output_eval.shape=\", vq_output_eval.shape)\n",
        "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
        "print(valid_quantize)\n",
        "print(\"valid_quantize=\", valid_quantize)\n",
        "print(\"valid_quantize.shape=\", valid_quantize.shape)\n",
        "# (train_originals, _) = next(iter(training_loader))\n",
        "# train_originals = train_originals.to(device)\n",
        "valid_reconstructions = model._decoder(valid_quantize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RF5noYrCi8"
      },
      "outputs": [],
      "source": [
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VAi9TanrCi8"
      },
      "outputs": [],
      "source": [
        "show(make_grid(valid_reconstructions.cpu().data)+0.5, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J620xHA6rCi8"
      },
      "outputs": [],
      "source": [
        "show(make_grid(train_originals.cpu()+0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqs594NmrCi8"
      },
      "source": [
        "## View Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW0ZlSrHrCi8"
      },
      "outputs": [],
      "source": [
        "! pip uninstall umap\n",
        "! pip install umap-learn\n",
        "\n",
        "import umap.umap_ as umap\n",
        "\n",
        "proj = umap.UMAP(n_neighbors=3,\n",
        "                 min_dist=0.1,\n",
        "                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNXVBcWCrCi9"
      },
      "outputs": [],
      "source": [
        "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWak2D-rCi9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX version"
      ],
      "metadata": {
        "id": "XdJN_axiTU07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-haiku==0.0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTBVrG_bVQ3t",
        "outputId": "e02e3d23-6782-4d43-8009-5c65d22536c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dm-haiku==0.0.9\n",
            "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku==0.0.9) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku==0.0.9)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku==0.0.9) (1.23.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku==0.0.9) (0.9.0)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.9 jmp-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Haiku implementation of VQ-VAE https://arxiv.org/abs/1711.00937.\"\"\"\n",
        "\n",
        "from typing import Any, Optional\n",
        "\n",
        "from haiku._src import base\n",
        "from haiku._src import initializers\n",
        "from haiku._src import module\n",
        "from haiku._src import moving_averages\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "# If you are forking replace this with `import haiku as hk`.\n",
        "# pylint: disable=invalid-name\n",
        "class hk:\n",
        "  get_parameter = base.get_parameter\n",
        "  get_state = base.get_state\n",
        "  set_state = base.set_state\n",
        "  initializers = initializers\n",
        "  ExponentialMovingAverage = moving_averages.ExponentialMovingAverage\n",
        "  Module = module.Module\n",
        "# pylint: enable=invalid-name\n",
        "del base, initializers, module, moving_averages\n",
        "\n",
        "\n",
        "class VectorQuantizer(hk.Module):\n",
        "  \"\"\"Haiku module representing the VQ-VAE layer.\n",
        "\n",
        "  Implements the algorithm presented in\n",
        "  \"Neural Discrete Representation Learning\" by van den Oord et al.\n",
        "  https://arxiv.org/abs/1711.00937\n",
        "\n",
        "  Input any tensor to be quantized. Last dimension will be used as space in\n",
        "  which to quantize. All other dimensions will be flattened and will be seen\n",
        "  as different examples to quantize.\n",
        "\n",
        "  The output tensor will have the same shape as the input.\n",
        "\n",
        "  For example a tensor with shape ``[16, 32, 32, 64]`` will be reshaped into\n",
        "  ``[16384, 64]`` and all ``16384`` vectors (each of ``64`` dimensions)  will be\n",
        "  quantized independently.\n",
        "\n",
        "  Attributes:\n",
        "    embedding_dim: integer representing the dimensionality of the tensors in the\n",
        "      quantized space. Inputs to the modules must be in this format as well.\n",
        "    num_embeddings: integer, the number of vectors in the quantized space.\n",
        "    commitment_cost: scalar which controls the weighting of the loss terms (see\n",
        "      equation 4 in the paper - this variable is Beta).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dim: int,\n",
        "      num_embeddings: int,\n",
        "      commitment_cost: float,\n",
        "      dtype: Any = jnp.float32,\n",
        "      name: Optional[str] = None,\n",
        "      cross_replica_axis: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Initializes a VQ-VAE module.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: dimensionality of the tensors in the quantized space.\n",
        "        Inputs to the modules must be in this format as well.\n",
        "      num_embeddings: number of vectors in the quantized space.\n",
        "      commitment_cost: scalar which controls the weighting of the loss terms\n",
        "        (see equation 4 in the paper - this variable is Beta).\n",
        "      dtype: dtype for the embeddings variable, defaults to ``float32``.\n",
        "      name: name of the module.\n",
        "      cross_replica_axis: If not ``None``, it should be a string representing\n",
        "        the axis name over which this module is being run within a\n",
        "        :func:`jax.pmap`. Supplying this argument means that perplexity is\n",
        "        calculated across all replicas on that axis.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.commitment_cost = commitment_cost\n",
        "    self.cross_replica_axis = cross_replica_axis\n",
        "\n",
        "    self._embedding_shape = [embedding_dim, num_embeddings]\n",
        "    self._embedding_dtype = dtype\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    initializer = hk.initializers.VarianceScaling(distribution=\"uniform\")\n",
        "    return hk.get_parameter(\n",
        "        \"embeddings\",\n",
        "        self._embedding_shape,\n",
        "        self._embedding_dtype,\n",
        "        init=initializer)\n",
        "\n",
        "  def __call__(self, inputs, is_training):\n",
        "    \"\"\"Connects the module to some inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Tensor, final dimension must be equal to ``embedding_dim``. All\n",
        "        other leading dimensions will be flattened and treated as a large batch.\n",
        "      is_training: boolean, whether this connection is to training data.\n",
        "\n",
        "    Returns:\n",
        "      dict: Dictionary containing the following keys and values:\n",
        "        * ``quantize``: Tensor containing the quantized version of the input.\n",
        "        * ``loss``: Tensor containing the loss to optimize.\n",
        "        * ``perplexity``: Tensor containing the perplexity of the encodings.\n",
        "        * ``encodings``: Tensor containing the discrete encodings, ie which\n",
        "          element of the quantized space each input element was mapped to.\n",
        "        * ``encoding_indices``: Tensor containing the discrete encoding indices,\n",
        "          ie which element of the quantized space each input element was mapped\n",
        "          to.\n",
        "    \"\"\"\n",
        "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
        "\n",
        "    distances = (\n",
        "        jnp.sum(jnp.square(flat_inputs), 1, keepdims=True) -\n",
        "        2 * jnp.matmul(flat_inputs, self.embeddings) +\n",
        "        jnp.sum(jnp.square(self.embeddings), 0, keepdims=True))\n",
        "\n",
        "    encoding_indices = jnp.argmax(-distances, 1)\n",
        "    encodings = jax.nn.one_hot(encoding_indices,\n",
        "                               self.num_embeddings,\n",
        "                               dtype=distances.dtype)\n",
        "\n",
        "    # NB: if your code crashes with a reshape error on the line below about a\n",
        "    # Tensor containing the wrong number of values, then the most likely cause\n",
        "    # is that the input passed in does not have a final dimension equal to\n",
        "    # self.embedding_dim. Ideally we would catch this with an Assert but that\n",
        "    # creates various other problems related to device placement / TPUs.\n",
        "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
        "    quantized = self.quantize(encoding_indices)\n",
        "\n",
        "    e_latent_loss = jnp.mean(\n",
        "        jnp.square(jax.lax.stop_gradient(quantized) - inputs))\n",
        "    q_latent_loss = jnp.mean(\n",
        "        jnp.square(quantized - jax.lax.stop_gradient(inputs)))\n",
        "    loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "    # Straight Through Estimator\n",
        "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
        "    avg_probs = jnp.mean(encodings, 0)\n",
        "    if self.cross_replica_axis:\n",
        "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
        "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
        "\n",
        "    return {\n",
        "        \"quantize\": quantized,\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"encodings\": encodings,\n",
        "        \"encoding_indices\": encoding_indices,\n",
        "        \"distances\": distances,\n",
        "    }\n",
        "\n",
        "  def quantize(self, encoding_indices):\n",
        "    \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
        "    w = self.embeddings.swapaxes(1, 0)\n",
        "    w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
        "    return w[(encoding_indices,)]\n",
        "\n",
        "\n",
        "class VectorQuantizerEMA(hk.Module):\n",
        "  r\"\"\"Haiku module representing the VQ-VAE layer.\n",
        "\n",
        "  Implements a slightly modified version of the algorithm presented in\n",
        "  \"Neural Discrete Representation Learning\" by van den Oord et al.\n",
        "  https://arxiv.org/abs/1711.00937\n",
        "\n",
        "  The difference between :class:`VectorQuantizerEMA` and\n",
        "  :class:`VectorQuantizer` is that this module uses\n",
        "  :class:`~haiku.ExponentialMovingAverage`\\ s to update the embedding vectors\n",
        "  instead of an auxiliary loss. This has the advantage that the embedding\n",
        "  updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac,\n",
        "  ...) used for the encoder, decoder and other parts of the architecture. For\n",
        "  most experiments the EMA version trains faster than the non-EMA version.\n",
        "\n",
        "  Input any tensor to be quantized. Last dimension will be used as space in\n",
        "  which to quantize. All other dimensions will be flattened and will be seen\n",
        "  as different examples to quantize.\n",
        "\n",
        "  The output tensor will have the same shape as the input.\n",
        "\n",
        "  For example a tensor with shape ``[16, 32, 32, 64]`` will be reshaped into\n",
        "  ``[16384, 64]`` and all ``16384`` vectors (each of 64 dimensions)  will be\n",
        "  quantized independently.\n",
        "\n",
        "  Attributes:\n",
        "    embedding_dim: integer representing the dimensionality of the tensors in\n",
        "      the quantized space. Inputs to the modules must be in this format as well.\n",
        "    num_embeddings: integer, the number of vectors in the quantized space.\n",
        "    commitment_cost: scalar which controls the weighting of the loss terms\n",
        "      (see equation 4 in the paper).\n",
        "    decay: float, decay for the moving averages.\n",
        "    epsilon: small float constant to avoid numerical instability.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dim,\n",
        "      num_embeddings,\n",
        "      commitment_cost,\n",
        "      decay,\n",
        "      epsilon: float = 1e-5,\n",
        "      dtype: Any = jnp.float32,\n",
        "      cross_replica_axis: Optional[str] = None,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Initializes a VQ-VAE EMA module.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: integer representing the dimensionality of the tensors in\n",
        "        the quantized space. Inputs to the modules must be in this format as\n",
        "        well.\n",
        "      num_embeddings: integer, the number of vectors in the quantized space.\n",
        "      commitment_cost: scalar which controls the weighting of the loss terms\n",
        "        (see equation 4 in the paper - this variable is Beta).\n",
        "      decay: float between 0 and 1, controls the speed of the Exponential Moving\n",
        "        Averages.\n",
        "      epsilon: small constant to aid numerical stability, default ``1e-5``.\n",
        "      dtype: dtype for the embeddings variable, defaults to ``float32``.\n",
        "      cross_replica_axis: If not ``None``, it should be a string representing\n",
        "        the axis name over which this module is being run within a\n",
        "        :func:`jax.pmap`. Supplying this argument means that cluster statistics\n",
        "        and the perplexity are calculated across all replicas on that axis.\n",
        "      name: name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    if not 0 <= decay <= 1:\n",
        "      raise ValueError(\"decay must be in range [0, 1]\")\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.decay = decay\n",
        "    self.commitment_cost = commitment_cost\n",
        "    self.epsilon = epsilon\n",
        "    self.cross_replica_axis = cross_replica_axis\n",
        "\n",
        "    self._embedding_shape = [embedding_dim, num_embeddings]\n",
        "    self._dtype = dtype\n",
        "\n",
        "    self._ema_cluster_size = hk.ExponentialMovingAverage(\n",
        "        decay=self.decay, name=\"ema_cluster_size\")\n",
        "    self._ema_dw = hk.ExponentialMovingAverage(decay=self.decay, name=\"ema_dw\")\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    initializer = hk.initializers.VarianceScaling(distribution=\"uniform\")\n",
        "    return hk.get_state(\n",
        "        \"embeddings\", self._embedding_shape, self._dtype, init=initializer)\n",
        "\n",
        "  @property\n",
        "  def ema_cluster_size(self):\n",
        "    self._ema_cluster_size.initialize([self.num_embeddings], self._dtype)\n",
        "    return self._ema_cluster_size\n",
        "\n",
        "  @property\n",
        "  def ema_dw(self):\n",
        "    self._ema_dw.initialize(self._embedding_shape, self._dtype)\n",
        "    return self._ema_dw\n",
        "\n",
        "  def __call__(self, inputs, is_training):\n",
        "    \"\"\"Connects the module to some inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Tensor, final dimension must be equal to ``embedding_dim``. All\n",
        "        other leading dimensions will be flattened and treated as a large batch.\n",
        "      is_training: boolean, whether this connection is to training data. When\n",
        "        this is set to ``False``, the internal moving average statistics will\n",
        "        not be updated.\n",
        "\n",
        "    Returns:\n",
        "      dict: Dictionary containing the following keys and values:\n",
        "        * ``quantize``: Tensor containing the quantized version of the input.\n",
        "        * ``loss``: Tensor containing the loss to optimize.\n",
        "        * ``perplexity``: Tensor containing the perplexity of the encodings.\n",
        "        * ``encodings``: Tensor containing the discrete encodings, ie which\n",
        "          element of the quantized space each input element was mapped to.\n",
        "        * ``encoding_indices``: Tensor containing the discrete encoding indices,\n",
        "          ie which element of the quantized space each input element was mapped\n",
        "          to.\n",
        "    \"\"\"\n",
        "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
        "    embeddings = self.embeddings\n",
        "\n",
        "    distances = (\n",
        "        jnp.sum(jnp.square(flat_inputs), 1, keepdims=True) -\n",
        "        2 * jnp.matmul(flat_inputs, embeddings) +\n",
        "        jnp.sum(jnp.square(embeddings), 0, keepdims=True))\n",
        "\n",
        "    encoding_indices = jnp.argmax(-distances, 1)\n",
        "    encodings = jax.nn.one_hot(encoding_indices,\n",
        "                               self.num_embeddings,\n",
        "                               dtype=distances.dtype)\n",
        "\n",
        "    # NB: if your code crashes with a reshape error on the line below about a\n",
        "    # Tensor containing the wrong number of values, then the most likely cause\n",
        "    # is that the input passed in does not have a final dimension equal to\n",
        "    # self.embedding_dim. Ideally we would catch this with an Assert but that\n",
        "    # creates various other problems related to device placement / TPUs.\n",
        "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
        "    quantized = self.quantize(encoding_indices)\n",
        "    e_latent_loss = jnp.mean(\n",
        "        jnp.square(jax.lax.stop_gradient(quantized) - inputs))\n",
        "\n",
        "    if is_training:\n",
        "      cluster_size = jnp.sum(encodings, axis=0)\n",
        "      if self.cross_replica_axis:\n",
        "        cluster_size = jax.lax.psum(\n",
        "            cluster_size, axis_name=self.cross_replica_axis)\n",
        "      updated_ema_cluster_size = self.ema_cluster_size(cluster_size)\n",
        "\n",
        "      dw = jnp.matmul(flat_inputs.T, encodings)\n",
        "      if self.cross_replica_axis:\n",
        "        dw = jax.lax.psum(dw, axis_name=self.cross_replica_axis)\n",
        "      updated_ema_dw = self.ema_dw(dw)\n",
        "\n",
        "      n = jnp.sum(updated_ema_cluster_size)\n",
        "      updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /\n",
        "                                  (n + self.num_embeddings * self.epsilon) * n)\n",
        "\n",
        "      normalised_updated_ema_w = (\n",
        "          updated_ema_dw / jnp.reshape(updated_ema_cluster_size, [1, -1]))\n",
        "\n",
        "      hk.set_state(\"embeddings\", normalised_updated_ema_w)\n",
        "      loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "    else:\n",
        "      loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "    # Straight Through Estimator\n",
        "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
        "    avg_probs = jnp.mean(encodings, 0)\n",
        "    if self.cross_replica_axis:\n",
        "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
        "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
        "\n",
        "    return {\n",
        "        \"quantize\": quantized,\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"encodings\": encodings,\n",
        "        \"encoding_indices\": encoding_indices,\n",
        "        \"distances\": distances,\n",
        "    }\n",
        "\n",
        "  def quantize(self, encoding_indices):\n",
        "    \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
        "    w = self.embeddings.swapaxes(1, 0)\n",
        "    w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
        "    return w[(encoding_indices,)]"
      ],
      "metadata": {
        "id": "uHsrsUu_TYny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    in_channels: int\n",
        "    num_hiddens: int\n",
        "    num_residual_hiddens: int\n",
        "\n",
        "    def setup(self):\n",
        "        self._block = nn.Sequential([\n",
        "            nn.relu,\n",
        "            nn.Conv(features=self.num_residual_hiddens,\n",
        "                    kernel_size=(1,),\n",
        "                    strides=(1,),\n",
        "                    use_bias=False)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = jnp.expand_dims(x, 2)  # Add an extra dimension\n",
        "        output = x + self._block(x)\n",
        "        return jnp.squeeze(output, 2)  # Remove the extra dimension\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    in_channels: int\n",
        "    num_hiddens: int\n",
        "    num_residual_layers: int\n",
        "    num_residual_hiddens: int\n",
        "\n",
        "    def setup(self):\n",
        "        self._layers = [\n",
        "            Residual(self.in_channels, self.num_hiddens, self.num_residual_hiddens)\n",
        "            for _ in range(self.num_residual_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self._layers:\n",
        "            x = layer(x)\n",
        "        return nn.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    in_channels: int\n",
        "    num_hiddens: int\n",
        "    num_residual_layers: int\n",
        "    num_residual_hiddens: int\n",
        "\n",
        "    def setup(self):\n",
        "        self._linear_1 = nn.Dense(self.num_hiddens // 2)\n",
        "        self._linear_2 = nn.Dense(self.num_hiddens)\n",
        "        self._residual_stack = ResidualStack(self.in_channels, self.num_hiddens,\n",
        "                                             self.num_residual_layers, self.num_residual_hiddens)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = self._linear_1(inputs.reshape((inputs.shape[0], -1)))\n",
        "        x = nn.relu(x)\n",
        "        x = self._linear_2(x)\n",
        "        x = nn.relu(x)\n",
        "        return self._residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    in_channels: int\n",
        "    num_hiddens: int\n",
        "    num_residual_layers: int\n",
        "    num_residual_hiddens: int\n",
        "\n",
        "    def setup(self):\n",
        "        self._linear_1 = nn.Dense(self.num_hiddens)\n",
        "        self._residual_stack = ResidualStack(self.in_channels, self.num_hiddens,\n",
        "                                             self.num_residual_layers, self.num_residual_hiddens)\n",
        "        self._linear_2 = nn.Dense(self.num_hiddens // 2)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = self._linear_1(inputs.reshape((inputs.shape[0], -1)))\n",
        "        x = self._residual_stack(x)\n",
        "        x = self._linear_2(x)\n",
        "        x = nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iqCQeyiqZroY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHMw7DL9WrPc",
        "outputId": "4c27dceb-b7ba-48de-c7ac-45978e69a7b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://pyenv.run | bash\n",
        "import os\n",
        "os.environ['PATH'] += \":/root/.pyenv/bin\"\n",
        "!pyenv install 3.8.0\n",
        "!pyenv global 3.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMMA9y7WWytB",
        "outputId": "ce5d86e3-2a03-467c-80c9-a40bbcd7f775"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   270  100   270    0     0   1357      0 --:--:-- --:--:-- --:--:--  1356\n",
            "Cloning into '/root/.pyenv'...\n",
            "remote: Enumerating objects: 1162, done.\u001b[K\n",
            "remote: Counting objects: 100% (1162/1162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (666/666), done.\u001b[K\n",
            "remote: Total 1162 (delta 674), reused 633 (delta 363), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1162/1162), 578.51 KiB | 13.45 MiB/s, done.\n",
            "Resolving deltas: 100% (674/674), done.\n",
            "Cloning into '/root/.pyenv/plugins/pyenv-doctor'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 11 (delta 1), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 38.72 KiB | 9.68 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Cloning into '/root/.pyenv/plugins/pyenv-update'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (10/10), done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Cloning into '/root/.pyenv/plugins/pyenv-virtualenv'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 63 (delta 11), reused 29 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (63/63), 40.54 KiB | 13.51 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "\n",
            "\u001b[1mWARNING\u001b[m: seems you still have not added 'pyenv' to the load path.\n",
            "\n",
            "# Load pyenv automatically by appending\n",
            "# the following to \n",
            "~/.bash_profile if it exists, otherwise ~/.profile (for login shells)\n",
            "and ~/.bashrc (for interactive shells) :\n",
            "\n",
            "export PYENV_ROOT=\"$HOME/.pyenv\"\n",
            "command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\n",
            "eval \"$(pyenv init -)\"\n",
            "\n",
            "# Restart your shell for the changes to take effect.\n",
            "\n",
            "# Load pyenv-virtualenv automatically by adding\n",
            "# the following to ~/.bashrc:\n",
            "\n",
            "eval \"$(pyenv virtualenv-init -)\"\n",
            "\n",
            "Downloading Python-3.8.0.tar.xz...\n",
            "-> https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tar.xz\n",
            "Installing Python-3.8.0...\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/root/.pyenv/versions/3.8.0/lib/python3.8/ctypes/__init__.py\", line 7, in <module>\n",
            "    from _ctypes import Union, Structure, Array\n",
            "ModuleNotFoundError: No module named '_ctypes'\n",
            "\u001b[1mWARNING\u001b[m: The Python ctypes extension was not compiled. Missing the libffi lib?\n",
            "Installed Python-3.8.0 to /root/.pyenv/versions/3.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pyenv global 3.8.0"
      ],
      "metadata": {
        "id": "h4HLgIVXX4bd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XgrWkxDXz0O",
        "outputId": "784d8cdc-63b1-44bf-e7b2-985b5c6ecee8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for haiku._src.nets.vqvae.\"\"\"\n",
        "\n",
        "import functools\n",
        "\n",
        "from absl.testing import absltest\n",
        "from absl.testing import parameterized\n",
        "\n",
        "from haiku._src import stateful\n",
        "from haiku._src import test_utils\n",
        "from haiku._src import transform\n",
        "from haiku._src.nets import vqvae\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VqvaeTest(parameterized.TestCase):\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testConstruct(self, constructor, kwargs):\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    # Batch of input vectors to quantize\n",
        "    inputs_np = np.random.randn(100, kwargs['embedding_dim']).astype(np.float32)\n",
        "    inputs = jnp.array(inputs_np)\n",
        "\n",
        "    # Set is_training to False, otherwise for the EMA case just evaluating the\n",
        "    # forward pass will change the embeddings, meaning that some of our computed\n",
        "    # closest embeddings will be incorrect.\n",
        "    vq_output = vqvae_module(inputs, is_training=False)\n",
        "\n",
        "    # Output shape is correct\n",
        "    self.assertEqual(vq_output['quantize'].shape, inputs.shape)\n",
        "\n",
        "    vq_output_np = jax.tree_util.tree_map(lambda t: t, vq_output)\n",
        "    embeddings_np = vqvae_module.embeddings\n",
        "\n",
        "    self.assertEqual(embeddings_np.shape,\n",
        "                     (kwargs['embedding_dim'], kwargs['num_embeddings']))\n",
        "\n",
        "    # Check that each input was assigned to the embedding it is closest to.\n",
        "    distances = (jnp.square(inputs_np).sum(axis=1, keepdims=True) -\n",
        "                 2 * np.dot(inputs_np, embeddings_np) +\n",
        "                 jnp.square(embeddings_np).sum(axis=0, keepdims=True))\n",
        "    closest_index = np.argmax(-distances, axis=1)\n",
        "    # On TPU, distances can be different by ~1% due to precision. This can cause\n",
        "    # the distanc to the closest embedding to flip, leading to a difference\n",
        "    # in the encoding indices tensor. First we check that the continuous\n",
        "    # distances are reasonably close, and then we only allow N differences in\n",
        "    # the encodings. For batch of 100, N == 3 seems okay (passed 1000x tests).\n",
        "    np.testing.assert_allclose(distances, vq_output_np['distances'], atol=5e-2)\n",
        "    num_differences_in_encodings = (closest_index !=\n",
        "                                    vq_output_np['encoding_indices']).sum()\n",
        "    num_differences_allowed = 3\n",
        "    self.assertLessEqual(num_differences_in_encodings, num_differences_allowed)\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testShapeChecking(self, constructor, kwargs):\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    wrong_shape_input = np.random.randn(100, kwargs['embedding_dim'] * 2)\n",
        "    with self.assertRaisesRegex(TypeError, 'total size must be unchanged'):\n",
        "      vqvae_module(\n",
        "          jnp.array(wrong_shape_input.astype(np.float32)), is_training=False)\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testNoneBatch(self, constructor, kwargs):\n",
        "    \"\"\"Check that vqvae can be built on input with a None batch dimension.\"\"\"\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    inputs = jnp.zeros([0, 5, 5, kwargs['embedding_dim']])\n",
        "    vqvae_module(inputs, is_training=False)\n",
        "\n",
        "  @parameterized.parameters({'use_jit': True, 'dtype': jnp.float32},\n",
        "                            {'use_jit': True, 'dtype': jnp.float64},\n",
        "                            {'use_jit': False, 'dtype': jnp.float32},\n",
        "                            {'use_jit': False, 'dtype': jnp.float64})\n",
        "  @test_utils.transform_and_run\n",
        "  def testEmaUpdating(self, use_jit, dtype):\n",
        "    if jax.local_devices()[0].platform == 'tpu' and dtype == jnp.float64:\n",
        "      self.skipTest('F64 not supported by TPU')\n",
        "\n",
        "    embedding_dim = 6\n",
        "    np_dtype = np.float64 if dtype is jnp.float64 else np.float32\n",
        "    decay = np.array(0.1, dtype=np_dtype)\n",
        "    vqvae_module = vqvae.VectorQuantizerEMA(\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_embeddings=7,\n",
        "        commitment_cost=0.5,\n",
        "        decay=decay,\n",
        "        dtype=dtype)\n",
        "\n",
        "    if use_jit:\n",
        "      vqvae_f = stateful.jit(vqvae_module, static_argnums=1)\n",
        "    else:\n",
        "      vqvae_f = vqvae_module\n",
        "\n",
        "    batch_size = 16\n",
        "\n",
        "    prev_embeddings = vqvae_module.embeddings\n",
        "\n",
        "    # Embeddings should change with every forwards pass if is_training == True.\n",
        "    for _ in range(10):\n",
        "      inputs = np.random.rand(batch_size, embedding_dim).astype(dtype)\n",
        "      vqvae_f(inputs, True)\n",
        "      current_embeddings = vqvae_module.embeddings\n",
        "      self.assertFalse((prev_embeddings == current_embeddings).all())\n",
        "      prev_embeddings = current_embeddings\n",
        "\n",
        "    # Forward passes with is_training == False don't change anything\n",
        "    for _ in range(10):\n",
        "      inputs = np.random.rand(batch_size, embedding_dim).astype(dtype)\n",
        "      vqvae_f(inputs, False)\n",
        "      current_embeddings = vqvae_module.embeddings\n",
        "      self.assertTrue((current_embeddings == prev_embeddings).all())\n",
        "\n",
        "  def testEmaCrossReplica(self):\n",
        "    embedding_dim = 6\n",
        "    batch_size = 16\n",
        "    inputs = np.random.rand(jax.local_device_count(), batch_size, embedding_dim)\n",
        "    embeddings = {}\n",
        "    perplexities = {}\n",
        "\n",
        "    for axis_name in [None, 'i']:\n",
        "      def my_function(x, axis_name):\n",
        "        decay = np.array(0.9, dtype=np.float32)\n",
        "        vqvae_module = vqvae.VectorQuantizerEMA(\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_embeddings=7,\n",
        "            commitment_cost=0.5,\n",
        "            decay=decay,\n",
        "            cross_replica_axis=axis_name,\n",
        "            dtype=jnp.float32)\n",
        "\n",
        "        outputs = vqvae_module(x, is_training=True)\n",
        "        return vqvae_module.embeddings, outputs['perplexity']\n",
        "\n",
        "      vqvae_f = transform.transform_with_state(\n",
        "          functools.partial(my_function, axis_name=axis_name))\n",
        "\n",
        "      rng = jax.random.PRNGKey(42)\n",
        "      rng = jnp.broadcast_to(rng, (jax.local_device_count(), *rng.shape))\n",
        "\n",
        "      params, state = jax.pmap(\n",
        "          vqvae_f.init, axis_name='i')(rng, inputs)\n",
        "      update_fn = jax.pmap(vqvae_f.apply, axis_name='i')\n",
        "\n",
        "      for _ in range(10):\n",
        "        outputs, state = update_fn(params, state, None, inputs)\n",
        "      embeddings[axis_name], perplexities[axis_name] = outputs\n",
        "\n",
        "    # In the single-device case, specifying a cross_replica_axis should have\n",
        "    # no effect. Otherwise, it should!\n",
        "    if jax.device_count() == 1:\n",
        "      # Have to use assert_allclose here rather than checking exact matches to\n",
        "      # make the test pass on GPU, presumably because of nondeterministic\n",
        "      # reductions.\n",
        "      np.testing.assert_allclose(\n",
        "          embeddings[None], embeddings['i'], rtol=1e-6, atol=1e-6)\n",
        "      np.testing.assert_allclose(\n",
        "          perplexities[None], perplexities['i'], rtol=1e-6, atol=1e-6)\n",
        "    else:\n",
        "      self.assertFalse((embeddings[None] == embeddings['i']).all())\n",
        "      self.assertFalse((perplexities[None] == perplexities['i']).all())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  absltest.main()"
      ],
      "metadata": {
        "id": "z_srkCXLTiGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f857cdab-df1d-43a6-eb2d-2188dd6e7dbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running tests under Python 3.10.12: /usr/bin/python3\n",
            "FATAL Flags parsing error: Unknown command line flag 'f'\n",
            "Pass --helpshort or --helpfull to see help on flags.\n",
            "E0829 03:38:20.981939 138459361918976 ultratb.py:152] Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 156, in parse_flags_with_usage\n",
            "    return FLAGS(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/flags/_flagvalues.py\", line 652, in __call__\n",
            "    raise _exceptions.UnrecognizedFlagError(\n",
            "absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'f'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-14-323b34bb46c9>\", line 195, in <cell line: 194>\n",
            "    absltest.main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/testing/absltest.py\", line 2060, in main\n",
            "    _run_in_app(run_tests, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/testing/absltest.py\", line 2165, in _run_in_app\n",
            "    app.run(main=main_function)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 300, in run\n",
            "    args = _run_init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 369, in _run_init\n",
            "    args = _register_and_parse_flags_with_usage(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 216, in _register_and_parse_flags_with_usage\n",
            "    args_to_main = flags_parser(original_argv)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 166, in parse_flags_with_usage\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mparse_flags_with_usage\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0m\u001b[1;32m    653\u001b[0m           name, value, suggestions=suggestions)\n",
            "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-323b34bb46c9>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mabsltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/testing/absltest.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m   \u001b[0mprint_python_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2060\u001b[0;31m   \u001b[0m_run_in_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_tests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/testing/absltest.py\u001b[0m in \u001b[0;36m_run_in_app\u001b[0;34m(function, args, kwargs)\u001b[0m\n\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2165\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     args = _run_init(\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_init\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    368\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_absl_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m   args = _register_and_parse_flags_with_usage(\n\u001b[0m\u001b[1;32m    370\u001b[0m       \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_register_and_parse_flags_with_usage\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0moriginal_argv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m   \u001b[0margs_to_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_argv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mparse_flags_with_usage\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pass --helpshort or --helpfull to see help on flags.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}