{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan0v0/nninn/blob/master/vq_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypctnKd-rCiy"
      },
      "outputs": [],
      "source": [
        "#!pip3 install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Splitting up neural net params into chunks"
      ],
      "metadata": {
        "id": "mMmijt45OkJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network architecture\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "        # Initialize the weights to be non-negative\n",
        "        nn.init.uniform_(self.fc1.weight, a=0, b=1)\n",
        "        nn.init.uniform_(self.fc2.weight, a=0, b=1)\n",
        "        nn.init.uniform_(self.fc3.weight, a=0, b=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the neural network\n",
        "net = NeuralNetwork()\n",
        "\n",
        "# Split up the neural network parameters into chunks\n",
        "chunk_size = 1000\n",
        "param_chunks = []\n",
        "for param in net.parameters():\n",
        "    flattened_param = param.view(-1)\n",
        "    chunks = torch.split(flattened_param, chunk_size)\n",
        "    param_chunks.extend(chunks)\n",
        "\n",
        "# Print the number of parameter chunks\n",
        "print(\"Number of parameter chunks:\", len(param_chunks))\n",
        "print(\"Parameter chunks:\", param_chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8gb85iH90wU",
        "outputId": "1fdbdb21-5fe9-4148-c7fc-985905e39d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameter chunks: 15\n",
            "Parameter chunks: [tensor([7.1851e-01, 6.2237e-01, 6.9191e-01, 2.4712e-02, 5.0765e-01, 4.4440e-01,\n",
            "        2.1151e-01, 2.2879e-01, 3.0134e-01, 2.5301e-01, 5.6201e-01, 6.9394e-01,\n",
            "        9.9110e-01, 8.7780e-01, 9.8581e-01, 3.4031e-01, 1.9593e-01, 7.3739e-01,\n",
            "        6.1132e-01, 2.4401e-01, 5.3588e-01, 4.8000e-01, 3.4889e-01, 3.0824e-02,\n",
            "        8.9073e-01, 7.3190e-01, 2.0004e-01, 7.9498e-01, 8.4673e-01, 4.3924e-02,\n",
            "        6.5294e-01, 1.5892e-01, 6.8574e-01, 1.7451e-01, 3.5812e-01, 8.5558e-01,\n",
            "        7.7970e-01, 5.2774e-01, 9.2514e-01, 3.0153e-01, 3.6376e-01, 7.7111e-01,\n",
            "        1.6636e-01, 9.1859e-01, 5.0898e-01, 6.4640e-01, 5.5042e-02, 1.6410e-02,\n",
            "        3.8419e-01, 3.9570e-01, 4.6773e-01, 7.2047e-01, 7.9951e-01, 9.2785e-01,\n",
            "        1.8951e-02, 6.7706e-02, 1.1297e-01, 8.3276e-01, 5.4782e-01, 9.1999e-01,\n",
            "        4.7233e-01, 1.8927e-01, 4.1096e-01, 8.5869e-01, 6.9278e-01, 4.7377e-01,\n",
            "        3.8494e-01, 4.9596e-01, 6.9117e-01, 4.9564e-01, 2.0846e-01, 4.9548e-02,\n",
            "        3.0927e-01, 1.2821e-01, 1.5827e-01, 1.8840e-01, 6.6387e-01, 2.1406e-01,\n",
            "        5.7504e-01, 8.3139e-02, 2.7301e-01, 8.1935e-02, 7.7992e-01, 8.9336e-01,\n",
            "        3.5128e-01, 5.1153e-01, 5.2662e-01, 2.1878e-01, 3.4748e-01, 5.4592e-01,\n",
            "        1.9981e-01, 1.6125e-01, 3.3030e-01, 3.3073e-01, 3.9054e-01, 2.9321e-01,\n",
            "        1.0510e-01, 6.7318e-02, 2.6691e-02, 5.2634e-01, 1.4566e-01, 8.5066e-01,\n",
            "        9.6519e-01, 2.4232e-01, 9.8109e-01, 3.3716e-02, 6.0302e-01, 2.9575e-01,\n",
            "        7.2499e-01, 4.1346e-01, 7.1874e-01, 7.0437e-01, 9.2086e-01, 4.3031e-01,\n",
            "        7.4055e-01, 9.1750e-01, 6.5841e-02, 9.8306e-01, 3.6744e-01, 9.4919e-01,\n",
            "        8.4514e-01, 1.8772e-01, 5.8817e-02, 8.9317e-01, 7.8139e-01, 2.7894e-01,\n",
            "        5.5830e-01, 1.5875e-01, 9.4017e-01, 9.1500e-02, 8.6719e-02, 2.8383e-01,\n",
            "        8.5233e-01, 9.3461e-01, 3.3801e-01, 3.9047e-01, 9.4061e-01, 4.1073e-01,\n",
            "        3.7417e-01, 7.6714e-01, 9.2437e-01, 8.2793e-01, 4.9631e-01, 6.8336e-01,\n",
            "        5.6175e-01, 7.9390e-01, 7.0647e-01, 4.1747e-01, 7.7783e-01, 8.1842e-01,\n",
            "        9.6161e-01, 6.0207e-01, 6.9551e-01, 5.2585e-02, 1.7600e-01, 6.1679e-01,\n",
            "        8.9044e-01, 2.8020e-01, 2.5687e-01, 7.1974e-01, 7.4784e-03, 4.4433e-01,\n",
            "        8.1780e-01, 5.0577e-01, 6.7291e-01, 8.1996e-01, 7.1736e-01, 5.8169e-01,\n",
            "        6.6110e-01, 5.0065e-02, 5.8155e-01, 2.6029e-01, 2.5752e-01, 6.3260e-01,\n",
            "        1.3939e-01, 1.8699e-01, 3.6760e-01, 5.2771e-01, 9.3937e-01, 7.2335e-01,\n",
            "        6.4663e-01, 4.7832e-01, 6.4434e-01, 6.7731e-01, 1.5203e-02, 8.7434e-01,\n",
            "        8.2042e-01, 2.9935e-01, 5.4188e-01, 2.7177e-01, 6.5238e-01, 9.4600e-01,\n",
            "        8.4357e-01, 2.6041e-01, 6.8334e-01, 2.7607e-01, 5.6633e-03, 4.3209e-01,\n",
            "        6.2810e-01, 4.0443e-01, 9.2602e-01, 5.8693e-01, 1.2218e-02, 2.2287e-01,\n",
            "        1.4874e-01, 7.9196e-01, 4.7755e-01, 7.2321e-01, 6.8520e-02, 8.4201e-01,\n",
            "        5.1655e-01, 6.0905e-01, 5.4016e-01, 1.2559e-01, 8.1779e-01, 2.0887e-01,\n",
            "        2.7490e-02, 6.9865e-01, 8.8931e-01, 6.0927e-01, 1.6271e-01, 5.5214e-01,\n",
            "        9.4883e-01, 6.1328e-02, 9.6410e-01, 3.3180e-01, 6.2491e-01, 7.1959e-01,\n",
            "        6.2599e-01, 8.8895e-01, 1.0633e-01, 6.8666e-02, 7.9625e-01, 4.7355e-01,\n",
            "        1.5507e-01, 7.5117e-01, 1.3133e-01, 3.0076e-01, 4.9180e-02, 2.3728e-01,\n",
            "        6.9516e-02, 4.1849e-01, 7.0692e-01, 1.3387e-01, 5.6591e-01, 3.7917e-02,\n",
            "        7.0295e-01, 8.3750e-01, 8.2447e-01, 3.8295e-02, 7.3733e-01, 6.0670e-01,\n",
            "        3.2864e-01, 9.5310e-03, 5.5799e-01, 9.8967e-01, 2.9433e-01, 7.0381e-01,\n",
            "        3.2119e-01, 6.1120e-01, 1.9352e-01, 8.8813e-01, 2.4457e-01, 6.8135e-01,\n",
            "        1.8300e-01, 3.8703e-01, 6.3083e-01, 9.5927e-01, 1.5637e-01, 9.9242e-01,\n",
            "        2.8628e-01, 7.7072e-01, 8.5816e-01, 5.4098e-01, 5.4665e-02, 5.2130e-01,\n",
            "        7.1737e-01, 3.2575e-01, 7.6860e-01, 6.7567e-01, 1.2264e-01, 9.6488e-01,\n",
            "        2.1739e-01, 5.9714e-01, 9.4182e-01, 1.5313e-01, 3.0384e-01, 1.4370e-02,\n",
            "        3.0718e-01, 8.4669e-01, 1.8841e-01, 8.0892e-01, 1.5685e-01, 2.6649e-01,\n",
            "        1.6620e-01, 1.5472e-01, 1.3258e-01, 8.8347e-01, 1.7496e-01, 2.7267e-01,\n",
            "        7.9298e-01, 7.8558e-01, 3.2105e-01, 3.6365e-01, 2.4822e-01, 6.9717e-01,\n",
            "        6.2849e-01, 6.2001e-01, 1.3798e-01, 9.8130e-01, 9.9211e-01, 2.5277e-01,\n",
            "        3.8589e-01, 5.8248e-01, 4.4354e-01, 7.0145e-01, 9.5162e-01, 5.2142e-01,\n",
            "        5.8543e-01, 9.7416e-01, 2.7994e-01, 1.1646e-01, 2.7014e-01, 5.4433e-01,\n",
            "        7.9896e-01, 3.8459e-02, 8.2357e-01, 9.1578e-01, 7.2400e-01, 7.9299e-01,\n",
            "        2.6434e-01, 4.3453e-01, 4.7960e-01, 7.4482e-01, 5.1406e-01, 8.2782e-01,\n",
            "        5.8403e-01, 7.9394e-01, 8.3062e-01, 5.1378e-02, 9.5962e-01, 5.3088e-01,\n",
            "        6.8000e-01, 3.3641e-01, 3.1004e-01, 8.6268e-01, 7.7742e-01, 7.3216e-01,\n",
            "        7.5964e-01, 1.8279e-02, 4.9403e-01, 4.5597e-01, 5.7900e-01, 3.1461e-01,\n",
            "        1.5163e-01, 6.6985e-01, 1.9511e-01, 5.3075e-01, 5.9990e-01, 4.6851e-01,\n",
            "        2.4321e-01, 9.2579e-01, 4.5913e-01, 5.6340e-01, 7.5725e-01, 3.1465e-01,\n",
            "        8.1292e-01, 4.3293e-01, 2.5549e-01, 9.4586e-01, 6.3191e-01, 6.7063e-02,\n",
            "        9.6120e-01, 6.4711e-01, 4.5706e-02, 9.1647e-01, 7.8521e-02, 7.0188e-01,\n",
            "        6.8131e-01, 2.2368e-01, 9.7413e-01, 4.7805e-01, 8.3284e-02, 7.7567e-01,\n",
            "        7.9710e-01, 3.2464e-01, 1.4459e-01, 2.9912e-01, 8.1622e-01, 2.9025e-01,\n",
            "        1.0605e-01, 4.7352e-01, 1.1021e-01, 5.6391e-01, 5.5185e-01, 6.8174e-01,\n",
            "        9.8878e-01, 8.6057e-01, 1.8923e-01, 6.7548e-01, 6.0834e-01, 4.5304e-02,\n",
            "        8.4263e-01, 1.4344e-01, 7.2172e-01, 6.7992e-01, 1.6251e-01, 4.7057e-03,\n",
            "        2.6179e-01, 5.2015e-02, 2.5895e-01, 3.7758e-01, 5.6186e-01, 7.9514e-01,\n",
            "        1.6657e-02, 6.8110e-01, 6.8902e-02, 1.5004e-01, 3.4923e-01, 3.8169e-01,\n",
            "        9.2140e-01, 4.0202e-01, 7.7023e-02, 2.2425e-01, 6.5497e-01, 1.7687e-01,\n",
            "        4.3573e-02, 5.8003e-01, 9.1204e-01, 6.1823e-01, 2.1369e-01, 7.7910e-01,\n",
            "        5.0022e-01, 6.1696e-01, 5.7706e-01, 4.0567e-01, 5.8060e-01, 1.0981e-02,\n",
            "        4.7395e-01, 5.3014e-01, 8.5981e-01, 5.3754e-01, 7.8729e-02, 1.7093e-01,\n",
            "        2.0166e-01, 2.9678e-02, 3.7366e-01, 9.5490e-01, 1.7320e-01, 7.9864e-01,\n",
            "        6.3506e-01, 8.6585e-01, 5.4844e-01, 6.4243e-01, 6.1337e-01, 9.9545e-01,\n",
            "        5.3981e-01, 1.0669e-01, 4.0718e-01, 9.0153e-01, 4.3302e-01, 6.3625e-01,\n",
            "        1.4736e-01, 5.3698e-01, 2.1433e-01, 8.8748e-02, 6.3579e-01, 9.1841e-01,\n",
            "        4.5934e-01, 6.1142e-02, 1.2574e-01, 8.1361e-01, 7.9230e-01, 3.6873e-01,\n",
            "        8.4192e-01, 9.2888e-02, 5.8226e-02, 5.3579e-01, 9.8283e-01, 8.2990e-01,\n",
            "        8.6506e-01, 7.3479e-01, 8.3080e-01, 7.4402e-01, 3.1221e-04, 8.8467e-01,\n",
            "        4.1824e-01, 7.7319e-01, 2.1411e-01, 1.4980e-01, 9.0968e-01, 3.3390e-01,\n",
            "        5.3836e-01, 1.2786e-01, 1.8518e-01, 8.8290e-01, 6.6689e-01, 6.3483e-01,\n",
            "        3.7605e-01, 1.8546e-01, 8.5519e-01, 3.4481e-01, 7.7741e-01, 3.6470e-02,\n",
            "        9.3023e-01, 3.9807e-01, 3.6821e-02, 4.5766e-01, 7.7193e-01, 6.8929e-01,\n",
            "        9.6256e-01, 4.4669e-01, 1.9174e-01, 9.3734e-01, 6.5481e-01, 8.9772e-02,\n",
            "        3.8211e-01, 8.4239e-01, 8.2476e-01, 1.3283e-01, 7.2657e-01, 6.3709e-02,\n",
            "        3.9585e-01, 6.3709e-01, 6.7977e-01, 8.9111e-01, 9.8083e-01, 5.9663e-01,\n",
            "        3.9601e-01, 6.6985e-01, 1.9979e-01, 2.9639e-01, 5.1180e-02, 5.6057e-01,\n",
            "        1.4742e-01, 9.0079e-01, 6.9750e-01, 3.4716e-01, 2.3805e-01, 2.4268e-01,\n",
            "        3.3754e-01, 5.5258e-01, 8.6824e-01, 6.8735e-01, 2.5125e-01, 6.0449e-01,\n",
            "        4.7551e-01, 6.1004e-01, 3.6345e-01, 5.4507e-01, 4.5012e-01, 8.7751e-01,\n",
            "        3.9174e-01, 6.7557e-01, 9.1086e-01, 3.7340e-01, 5.1850e-01, 5.8724e-01,\n",
            "        3.1162e-01, 8.4483e-01, 8.5884e-01, 2.6542e-01, 2.0583e-01, 6.8394e-01,\n",
            "        5.5928e-01, 8.5368e-01, 5.0314e-01, 2.5693e-01, 2.6159e-01, 6.3596e-01,\n",
            "        3.7448e-01, 5.1726e-01, 9.1491e-01, 5.0998e-01, 1.5060e-02, 9.7702e-01,\n",
            "        8.6818e-01, 1.9980e-01, 1.8719e-02, 3.0271e-01, 1.8201e-01, 7.9881e-01,\n",
            "        1.0232e-01, 2.2443e-01, 5.3398e-01, 9.7893e-01, 8.5754e-01, 1.3050e-01,\n",
            "        6.8322e-01, 6.6076e-01, 6.5710e-01, 3.4510e-01, 8.3954e-01, 6.9797e-01,\n",
            "        6.0436e-01, 9.1461e-01, 3.5107e-01, 9.8586e-01, 1.9781e-01, 2.0055e-01,\n",
            "        2.7575e-01, 4.5577e-01, 6.9435e-01, 9.2025e-01, 4.5787e-01, 3.7796e-01,\n",
            "        5.5707e-02, 3.1802e-01, 1.5798e-01, 8.4693e-01, 2.2075e-01, 7.1044e-01,\n",
            "        1.8860e-01, 4.0374e-01, 8.8155e-01, 9.4550e-01, 2.7116e-01, 2.0489e-01,\n",
            "        4.0004e-01, 1.1074e-01, 6.4686e-01, 5.8279e-01, 8.5756e-01, 9.5302e-02,\n",
            "        8.8509e-01, 5.0088e-02, 2.5380e-01, 6.2414e-01, 9.7535e-01, 2.0734e-01,\n",
            "        2.0619e-01, 9.8178e-02, 7.5612e-01, 7.8646e-01, 4.6678e-01, 6.0771e-01,\n",
            "        9.8752e-01, 5.5283e-01, 2.6425e-01, 5.7771e-01, 1.0744e-01, 9.0059e-01,\n",
            "        1.8218e-01, 8.4207e-01, 6.2992e-01, 6.5135e-01, 7.3205e-01, 9.9901e-01,\n",
            "        2.0824e-01, 7.7823e-01, 8.9935e-01, 2.4743e-01, 7.9091e-01, 9.4036e-01,\n",
            "        6.1581e-01, 8.4553e-01, 9.2637e-01, 1.5984e-01, 5.4038e-01, 4.1241e-01,\n",
            "        6.5124e-01, 9.7032e-01, 2.6916e-01, 7.1742e-01, 9.7075e-01, 9.6119e-01,\n",
            "        2.1445e-01, 7.3791e-01, 1.8160e-01, 8.5037e-01, 2.2421e-01, 4.3183e-01,\n",
            "        3.5768e-01, 3.2363e-01, 1.3375e-02, 5.7898e-02, 5.5851e-01, 4.0690e-01,\n",
            "        3.3412e-01, 4.1335e-02, 2.7882e-01, 6.2189e-01, 6.3005e-01, 8.7581e-01,\n",
            "        2.7161e-02, 7.7151e-01, 7.5662e-01, 9.9877e-01, 4.8066e-01, 9.4892e-01,\n",
            "        1.0672e-01, 2.0350e-01, 6.4392e-01, 7.3210e-01, 1.8203e-01, 2.7208e-01,\n",
            "        6.1767e-02, 5.8966e-01, 4.6271e-01, 6.7804e-01, 7.0491e-01, 7.1638e-01,\n",
            "        3.6305e-01, 3.2259e-02, 7.4385e-01, 1.1144e-01, 4.1624e-01, 5.3685e-01,\n",
            "        4.7099e-01, 3.3553e-01, 6.6533e-01, 7.2051e-02, 5.4392e-01, 6.8425e-01,\n",
            "        6.6375e-02, 7.5946e-01, 5.0777e-01, 2.8344e-01, 8.9747e-01, 3.0547e-02,\n",
            "        6.7204e-01, 8.4806e-01, 5.2640e-01, 7.3719e-01, 2.1437e-01, 1.2272e-01,\n",
            "        4.2979e-02, 1.8052e-03, 3.8392e-01, 3.7235e-01, 4.6192e-01, 8.2404e-01,\n",
            "        1.3434e-01, 5.5094e-01, 4.2757e-01, 9.0662e-01, 3.3570e-01, 9.2911e-01,\n",
            "        6.6313e-01, 1.4240e-01, 4.9077e-02, 2.9830e-01, 4.3570e-01, 7.9970e-02,\n",
            "        1.4629e-01, 4.9728e-01, 5.4743e-01, 2.0201e-01, 5.9219e-01, 2.1026e-01,\n",
            "        6.8532e-02, 8.0334e-01, 2.4961e-01, 3.1054e-01, 3.4216e-01, 2.9417e-01,\n",
            "        1.8305e-01, 2.0285e-02, 5.1424e-01, 1.8781e-01, 2.3372e-02, 4.3171e-02,\n",
            "        3.8842e-01, 3.3659e-01, 9.3428e-01, 8.0701e-01, 1.0906e-01, 7.8879e-01,\n",
            "        9.9275e-01, 3.7010e-01, 7.7465e-01, 4.3840e-01, 5.7786e-01, 8.2327e-01,\n",
            "        8.4905e-01, 6.5599e-01, 9.2537e-01, 9.2480e-01, 6.3584e-01, 3.3566e-01,\n",
            "        8.9039e-01, 7.9331e-01, 7.9332e-01, 7.8986e-01, 6.4442e-01, 2.3270e-01,\n",
            "        5.8905e-01, 6.7251e-01, 8.7729e-01, 8.9370e-01, 4.0357e-01, 7.8493e-01,\n",
            "        2.1419e-01, 5.4020e-01, 5.4753e-01, 2.0301e-01, 9.7066e-01, 9.1551e-01,\n",
            "        4.6706e-02, 7.6869e-02, 8.3571e-01, 5.2812e-01, 7.6128e-01, 1.0922e-01,\n",
            "        6.6464e-01, 2.6008e-01, 1.9067e-01, 6.5207e-01, 2.0088e-02, 6.3293e-01,\n",
            "        7.8500e-01, 2.0378e-01, 5.1071e-01, 8.5716e-01, 1.3554e-01, 7.1503e-01,\n",
            "        3.2785e-01, 9.1960e-02, 2.5858e-01, 6.0225e-02, 7.8641e-01, 6.3760e-01,\n",
            "        2.5901e-01, 6.6150e-01, 4.3038e-01, 6.9126e-01, 1.8581e-01, 4.9026e-01,\n",
            "        2.8483e-01, 3.5547e-01, 2.0618e-01, 2.6014e-02, 2.4703e-01, 2.5395e-01,\n",
            "        8.1332e-01, 1.8073e-01, 7.0127e-01, 4.6326e-01, 9.8723e-01, 6.3784e-01,\n",
            "        9.4240e-01, 6.7164e-01, 3.6314e-01, 9.9047e-01, 8.1108e-01, 1.6631e-01,\n",
            "        8.8325e-01, 5.7249e-01, 6.9708e-01, 4.1687e-01, 7.4220e-01, 2.9760e-01,\n",
            "        4.7148e-01, 9.9105e-01, 8.6629e-01, 7.5386e-01, 1.0041e-01, 6.7376e-02,\n",
            "        5.9820e-01, 6.7271e-02, 3.0073e-01, 5.1289e-01, 9.6838e-01, 9.2316e-01,\n",
            "        9.5972e-01, 1.8636e-02, 7.8873e-01, 4.5092e-01, 3.9417e-01, 5.3943e-01,\n",
            "        3.8657e-01, 5.3300e-01, 9.1756e-01, 4.8957e-01, 7.9902e-01, 3.6393e-01,\n",
            "        5.3700e-01, 4.8478e-01, 3.3314e-01, 2.3803e-01, 3.7979e-01, 1.4379e-01,\n",
            "        1.5353e-01, 9.0196e-01, 2.2069e-02, 8.3262e-01, 4.2380e-01, 8.2858e-01,\n",
            "        6.5556e-02, 3.6198e-01, 3.0974e-01, 4.9527e-02, 5.9219e-01, 3.8056e-01,\n",
            "        6.2965e-01, 2.7767e-01, 2.3212e-02, 4.7672e-01, 2.4448e-01, 8.3988e-01,\n",
            "        2.7317e-02, 1.2664e-01, 4.8928e-01, 6.2437e-03, 9.6513e-01, 1.3706e-01,\n",
            "        8.2637e-01, 7.6014e-01, 6.2864e-01, 1.2398e-04, 8.7085e-01, 2.6029e-01,\n",
            "        4.7028e-01, 6.5095e-01, 6.5453e-01, 6.8838e-01, 8.1016e-03, 1.7813e-01,\n",
            "        2.5411e-01, 2.3802e-01, 9.5921e-01, 3.1823e-01, 9.8262e-01, 5.7199e-01,\n",
            "        2.3356e-01, 4.1799e-01, 3.4981e-01, 5.1149e-01, 7.6648e-01, 2.9611e-01,\n",
            "        2.9930e-01, 1.4687e-01, 9.5776e-01, 7.9009e-01, 3.1366e-01, 4.7341e-01,\n",
            "        7.6074e-01, 8.5156e-01, 3.7133e-01, 6.9540e-02, 1.8291e-01, 7.0460e-01,\n",
            "        1.4739e-01, 6.5433e-01, 4.6016e-01, 2.7072e-01, 2.2476e-01, 8.0105e-01,\n",
            "        7.5721e-01, 7.1566e-02, 2.5912e-01, 6.8265e-04, 7.8255e-01, 8.9276e-01,\n",
            "        8.6231e-01, 2.7649e-01, 1.0230e-02, 9.8484e-01, 7.1525e-01, 9.8299e-01,\n",
            "        6.4238e-01, 4.6119e-01, 7.9641e-01, 6.4462e-01, 8.8930e-01, 1.9893e-01,\n",
            "        5.7448e-01, 9.4851e-01, 8.9983e-01, 5.0566e-02, 7.4350e-01, 8.7233e-01,\n",
            "        2.5947e-01, 8.8656e-01, 8.1533e-01, 6.9762e-02, 4.4835e-01, 2.4082e-02,\n",
            "        6.0136e-01, 7.9753e-01, 6.5099e-01, 4.1614e-01, 5.4683e-01, 2.5202e-01,\n",
            "        4.2235e-01, 5.4214e-01, 8.2302e-01, 5.8376e-02, 9.7698e-01, 7.3223e-01,\n",
            "        5.6602e-01, 8.6835e-01, 5.5667e-01, 7.2233e-01, 3.5576e-01, 4.0902e-01,\n",
            "        3.6319e-01, 1.4219e-01, 5.8384e-01, 6.8280e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([-0.2429,  0.1842, -0.2232,  0.1838,  0.2903,  0.2078,  0.0706,  0.2429,\n",
            "         0.2399,  0.2431,  0.2598,  0.1205,  0.0241, -0.2513,  0.0777,  0.2633,\n",
            "        -0.0851, -0.1293, -0.2973, -0.1347,  0.1438, -0.0698, -0.2777, -0.2176,\n",
            "         0.2170, -0.0287, -0.0382,  0.0102, -0.2991,  0.2127,  0.2836, -0.0951,\n",
            "        -0.2331,  0.0357,  0.1301, -0.0468,  0.2135,  0.0585, -0.1652, -0.0108,\n",
            "         0.0686,  0.0034, -0.0806,  0.0014, -0.1149, -0.1134, -0.1962, -0.1889,\n",
            "        -0.0008,  0.0727, -0.2628, -0.2111,  0.1722, -0.0670,  0.2663,  0.0160,\n",
            "         0.0107,  0.2658,  0.2594,  0.1686,  0.0113, -0.1374, -0.3113, -0.1983,\n",
            "         0.0267, -0.0707,  0.2545,  0.0034, -0.1953, -0.2447, -0.2305,  0.2144,\n",
            "        -0.2950,  0.1018, -0.2452,  0.2137,  0.0737, -0.0702,  0.2627,  0.2317,\n",
            "         0.1071, -0.3138, -0.1234,  0.1316, -0.0370,  0.0757, -0.2101, -0.0751,\n",
            "        -0.1672,  0.0487, -0.2531, -0.1590, -0.2785, -0.2227, -0.2763,  0.2161,\n",
            "        -0.1037,  0.2592,  0.2448, -0.2551], grad_fn=<SplitBackward0>), tensor([4.5293e-01, 6.0065e-01, 9.1847e-01, 9.8325e-01, 1.0589e-01, 7.3885e-01,\n",
            "        6.1236e-01, 5.5183e-01, 6.6493e-01, 4.8812e-01, 8.9739e-01, 4.8950e-01,\n",
            "        9.3765e-01, 3.2071e-01, 4.8021e-01, 4.5993e-02, 3.3670e-01, 4.9801e-01,\n",
            "        6.6505e-01, 7.8945e-01, 8.5628e-01, 8.4176e-01, 5.5199e-01, 1.2301e-01,\n",
            "        9.0488e-01, 2.2372e-01, 8.8813e-01, 9.6235e-01, 8.5817e-01, 8.8051e-01,\n",
            "        6.9618e-01, 7.7823e-01, 1.7925e-01, 5.9681e-01, 4.8195e-02, 1.9175e-01,\n",
            "        3.1679e-01, 6.8368e-01, 4.2647e-03, 1.2854e-02, 8.6053e-01, 9.8838e-01,\n",
            "        6.2023e-01, 5.1285e-01, 8.2401e-01, 6.9020e-01, 5.3647e-01, 1.7826e-01,\n",
            "        6.1952e-01, 2.9499e-02, 4.8798e-01, 4.8325e-01, 4.7512e-02, 2.6716e-01,\n",
            "        1.4255e-01, 4.9911e-01, 1.7082e-01, 3.8247e-01, 2.8428e-01, 3.5506e-01,\n",
            "        5.1367e-01, 1.7918e-01, 8.5431e-01, 4.3538e-01, 8.1285e-02, 9.4750e-01,\n",
            "        5.0129e-01, 3.3365e-01, 8.7590e-01, 5.5580e-01, 1.2896e-01, 3.9377e-01,\n",
            "        5.1770e-01, 2.1263e-01, 4.9648e-01, 2.0913e-01, 9.1594e-01, 5.3225e-01,\n",
            "        6.0302e-01, 5.7471e-02, 7.2603e-01, 5.9112e-01, 3.4202e-01, 7.8639e-01,\n",
            "        3.1343e-01, 7.1161e-01, 4.1508e-01, 4.2596e-02, 7.7693e-01, 7.6247e-01,\n",
            "        6.8404e-01, 7.1212e-01, 7.7392e-01, 2.7978e-01, 4.3561e-01, 2.9434e-01,\n",
            "        1.1662e-01, 3.4387e-02, 3.7070e-01, 8.2206e-01, 1.0333e-01, 2.9495e-01,\n",
            "        8.9102e-01, 9.0813e-01, 6.1105e-01, 8.0418e-01, 8.7791e-01, 1.1784e-01,\n",
            "        6.3556e-01, 3.9073e-01, 3.5571e-02, 1.9592e-02, 8.0611e-02, 7.7332e-01,\n",
            "        1.1026e-01, 3.5251e-01, 2.9434e-01, 4.0355e-01, 5.1293e-01, 8.9184e-01,\n",
            "        9.6454e-01, 7.7832e-01, 6.3462e-01, 4.4199e-01, 9.9641e-01, 9.8198e-01,\n",
            "        7.4604e-02, 8.9494e-02, 1.4897e-01, 4.5392e-01, 8.5416e-01, 9.3114e-01,\n",
            "        7.1304e-01, 1.4921e-01, 6.7920e-01, 4.9508e-01, 8.9354e-01, 3.3541e-01,\n",
            "        5.3627e-01, 8.0032e-02, 9.2313e-02, 1.3876e-01, 1.0301e-01, 4.6899e-02,\n",
            "        8.9009e-01, 7.9345e-01, 6.7059e-01, 6.8311e-01, 6.4212e-01, 9.1662e-01,\n",
            "        3.3124e-02, 9.4048e-01, 8.4104e-02, 3.7574e-01, 1.1463e-01, 5.4122e-01,\n",
            "        8.3099e-01, 9.5612e-01, 7.4401e-01, 4.3740e-01, 8.0385e-01, 7.0864e-02,\n",
            "        1.3303e-01, 9.6841e-01, 4.0866e-01, 5.6489e-02, 4.5642e-01, 6.8576e-01,\n",
            "        4.1392e-01, 6.2195e-01, 8.1685e-01, 9.3491e-01, 2.0353e-01, 6.2103e-01,\n",
            "        3.5033e-01, 3.3847e-01, 5.7744e-01, 4.4708e-01, 7.8855e-01, 4.1436e-01,\n",
            "        5.5299e-01, 3.8045e-01, 4.9043e-01, 6.6847e-01, 6.9048e-01, 1.9670e-01,\n",
            "        4.6024e-01, 1.3521e-01, 4.9908e-01, 2.5610e-01, 4.6936e-01, 1.4600e-01,\n",
            "        3.6095e-01, 2.6675e-02, 1.4450e-01, 8.9789e-01, 4.4196e-01, 4.9364e-01,\n",
            "        2.5109e-01, 3.8989e-01, 4.2680e-01, 4.6510e-01, 1.5068e-01, 9.3869e-01,\n",
            "        1.2960e-01, 7.6656e-01, 2.4034e-02, 4.2740e-01, 9.0066e-01, 2.5165e-01,\n",
            "        7.6417e-01, 5.6255e-02, 6.6304e-02, 7.4109e-01, 6.4512e-01, 2.3354e-01,\n",
            "        3.9924e-01, 6.8443e-02, 4.3975e-01, 8.0991e-01, 5.7155e-01, 2.0176e-01,\n",
            "        1.5309e-02, 4.6272e-01, 8.0572e-01, 6.0166e-01, 2.1953e-03, 2.0739e-01,\n",
            "        5.7339e-01, 7.7811e-01, 2.4733e-01, 4.3309e-01, 8.6482e-01, 7.7582e-02,\n",
            "        1.6686e-02, 4.3541e-01, 7.7432e-01, 5.8074e-01, 2.7758e-01, 2.2026e-02,\n",
            "        8.2983e-01, 5.5824e-02, 8.9522e-01, 7.5304e-01, 5.6882e-01, 8.8575e-02,\n",
            "        1.8612e-01, 4.0208e-01, 7.1519e-02, 1.3532e-01, 7.4559e-01, 4.1557e-01,\n",
            "        3.5039e-01, 8.2176e-01, 2.6325e-01, 2.0824e-01, 3.6090e-01, 2.6332e-01,\n",
            "        3.6909e-01, 5.9824e-02, 9.0490e-01, 7.9669e-01, 7.6252e-01, 9.3739e-01,\n",
            "        8.6618e-01, 2.7731e-01, 1.2292e-01, 2.4657e-01, 6.4688e-01, 8.1323e-02,\n",
            "        3.1025e-01, 5.9917e-01, 4.1841e-01, 5.6015e-01, 8.9413e-01, 5.5700e-02,\n",
            "        5.6997e-01, 6.9125e-01, 5.4641e-02, 4.0224e-01, 1.1635e-01, 1.4776e-01,\n",
            "        2.0561e-01, 2.2248e-01, 5.2639e-01, 1.7022e-01, 2.9890e-02, 9.8615e-01,\n",
            "        8.6741e-02, 6.6087e-02, 2.4294e-01, 3.2637e-01, 9.8967e-01, 9.8345e-01,\n",
            "        9.5553e-01, 7.6461e-01, 2.2867e-01, 8.4764e-01, 1.1109e-01, 1.0803e-01,\n",
            "        2.8015e-01, 2.3033e-01, 1.6069e-01, 1.4715e-01, 5.2616e-01, 9.5254e-01,\n",
            "        7.6563e-01, 8.8644e-01, 9.9476e-01, 5.1192e-01, 9.8142e-01, 1.1135e-03,\n",
            "        2.5240e-01, 7.4975e-01, 1.9028e-01, 5.1190e-01, 6.0255e-01, 6.0435e-01,\n",
            "        8.0369e-01, 8.1647e-02, 3.3620e-01, 6.3016e-01, 4.8723e-01, 7.4500e-01,\n",
            "        8.9771e-02, 7.3154e-01, 4.6786e-01, 2.3372e-01, 9.2989e-01, 9.7816e-01,\n",
            "        8.2577e-01, 2.1835e-01, 7.1920e-01, 7.0287e-01, 3.8153e-01, 4.6805e-01,\n",
            "        2.6237e-01, 2.7875e-02, 1.6754e-01, 6.7664e-01, 9.6213e-02, 9.7171e-01,\n",
            "        6.2833e-01, 6.6572e-01, 2.9031e-01, 2.3863e-01, 8.7963e-01, 4.3582e-01,\n",
            "        8.7522e-01, 7.1255e-01, 5.6100e-01, 7.8815e-01, 1.7334e-01, 1.6627e-02,\n",
            "        2.1619e-02, 9.7912e-01, 2.1338e-01, 9.0790e-02, 3.0632e-01, 6.8544e-01,\n",
            "        1.4560e-01, 8.6769e-01, 4.2040e-01, 1.1828e-01, 9.5205e-01, 9.5555e-01,\n",
            "        2.8403e-01, 2.9047e-02, 9.8021e-01, 9.4211e-01, 4.5095e-01, 9.5967e-01,\n",
            "        3.3262e-01, 1.0352e-01, 9.4220e-01, 6.1194e-01, 8.5178e-01, 1.8833e-01,\n",
            "        3.4379e-01, 1.2862e-01, 1.5646e-01, 4.2721e-01, 5.0362e-01, 3.1442e-01,\n",
            "        5.6822e-01, 4.5527e-01, 4.4944e-01, 6.2987e-02, 3.2612e-02, 6.0439e-01,\n",
            "        1.8637e-01, 7.3667e-01, 9.8029e-01, 9.3903e-01, 5.7126e-03, 6.6003e-01,\n",
            "        9.0456e-01, 6.3063e-01, 2.1953e-01, 8.5755e-01, 6.2163e-01, 6.2239e-01,\n",
            "        8.8783e-01, 7.6784e-01, 3.6629e-02, 1.3705e-01, 8.3679e-01, 4.2012e-01,\n",
            "        6.8607e-01, 2.9542e-01, 1.1378e-01, 7.0547e-01, 4.6045e-01, 6.3203e-02,\n",
            "        8.7131e-01, 6.9925e-01, 5.9976e-01, 7.3825e-01, 4.9910e-01, 5.2990e-01,\n",
            "        8.7025e-01, 7.4852e-01, 3.9249e-01, 3.8381e-01, 1.8662e-01, 4.1033e-01,\n",
            "        9.9064e-01, 6.6839e-01, 4.8496e-01, 8.5105e-01, 2.5943e-02, 6.0063e-01,\n",
            "        6.4765e-01, 2.3998e-01, 4.3556e-01, 2.0086e-01, 4.1219e-01, 4.0565e-02,\n",
            "        9.4400e-01, 3.6366e-01, 8.3732e-01, 3.4346e-01, 8.4704e-01, 3.7680e-01,\n",
            "        1.3783e-02, 7.4329e-01, 7.8205e-01, 1.8043e-01, 3.4796e-02, 9.8440e-01,\n",
            "        8.1599e-01, 8.2231e-01, 3.5297e-01, 2.5119e-01, 6.8205e-01, 6.1272e-01,\n",
            "        6.7916e-01, 4.8247e-01, 9.3785e-01, 2.7324e-01, 2.9855e-01, 1.0940e-01,\n",
            "        8.7591e-02, 9.8480e-01, 4.1913e-01, 9.4922e-01, 6.1103e-01, 4.4678e-01,\n",
            "        2.6470e-01, 4.6536e-01, 7.7463e-01, 1.7720e-01, 1.1473e-01, 5.8171e-02,\n",
            "        9.5700e-01, 8.2517e-01, 5.8488e-01, 8.2711e-01, 2.4827e-01, 6.1990e-01,\n",
            "        3.0380e-01, 5.8708e-01, 3.0719e-02, 5.8081e-01, 8.0040e-01, 8.1032e-01,\n",
            "        6.6590e-01, 2.0116e-01, 4.2584e-01, 4.3219e-01, 6.3650e-01, 7.0441e-02,\n",
            "        6.9370e-01, 7.6175e-01, 5.5439e-01, 5.6476e-01, 3.5991e-01, 7.6254e-01,\n",
            "        4.9375e-01, 8.3854e-01, 4.1117e-02, 4.5234e-01, 6.3949e-01, 4.8249e-01,\n",
            "        5.7396e-01, 8.5924e-01, 9.4136e-01, 6.6195e-01, 1.2595e-01, 6.4002e-01,\n",
            "        6.0609e-01, 3.4214e-01, 2.1116e-01, 4.8641e-02, 7.9687e-01, 3.2223e-01,\n",
            "        7.1485e-01, 5.5697e-01, 2.7989e-01, 9.0739e-01, 1.5284e-01, 1.2243e-01,\n",
            "        2.7254e-01, 2.3396e-01, 4.3297e-01, 4.2620e-01, 5.9637e-01, 7.0602e-01,\n",
            "        7.3882e-01, 1.9007e-01, 2.7660e-01, 5.6532e-01, 4.8266e-01, 9.4239e-01,\n",
            "        4.0850e-01, 9.4114e-01, 5.8939e-01, 8.7021e-01, 3.3971e-01, 9.3539e-02,\n",
            "        4.2727e-01, 7.8660e-01, 8.2029e-01, 8.3154e-01, 7.3175e-01, 7.2683e-01,\n",
            "        3.7397e-02, 2.2219e-02, 2.5842e-01, 4.7670e-01, 3.4681e-01, 5.7874e-01,\n",
            "        7.7645e-02, 3.8737e-01, 3.6246e-01, 5.7899e-01, 6.7260e-01, 7.8537e-01,\n",
            "        9.6234e-01, 5.9194e-01, 1.2580e-01, 3.3594e-01, 7.0273e-01, 7.6467e-01,\n",
            "        9.2692e-01, 2.2971e-01, 6.4917e-01, 9.3158e-01, 3.8004e-01, 4.1978e-01,\n",
            "        9.0687e-02, 8.4017e-01, 8.2181e-01, 7.1529e-01, 3.4054e-01, 3.3773e-01,\n",
            "        9.2528e-01, 3.4475e-01, 2.8118e-01, 4.6863e-02, 8.1032e-01, 5.7540e-02,\n",
            "        6.4092e-01, 7.0593e-02, 7.0172e-01, 1.6016e-01, 4.9174e-01, 6.9176e-01,\n",
            "        7.8618e-01, 6.0420e-01, 1.9804e-01, 5.5955e-01, 4.2587e-01, 7.6669e-01,\n",
            "        1.7886e-01, 3.3884e-01, 6.9020e-01, 1.5831e-01, 6.3612e-01, 2.4093e-01,\n",
            "        7.7711e-01, 9.9392e-02, 5.3055e-01, 6.7335e-01, 7.3628e-01, 8.8191e-01,\n",
            "        9.1726e-01, 4.3388e-01, 1.7384e-01, 1.2505e-01, 9.7929e-01, 3.0380e-01,\n",
            "        3.4553e-01, 7.1813e-01, 7.7943e-01, 4.5779e-01, 3.4809e-01, 1.6059e-01,\n",
            "        5.7033e-01, 8.7187e-01, 6.7759e-01, 3.5455e-01, 2.3208e-01, 6.4101e-01,\n",
            "        8.8646e-01, 9.1191e-01, 5.9692e-01, 6.0936e-01, 5.8283e-01, 5.2487e-01,\n",
            "        9.4643e-01, 3.3899e-02, 2.8848e-01, 7.9191e-01, 2.6949e-01, 4.9613e-01,\n",
            "        4.4753e-01, 9.5960e-01, 5.6873e-01, 1.4493e-01, 1.5741e-01, 6.5137e-01,\n",
            "        5.3346e-01, 1.1533e-04, 8.5678e-02, 1.7741e-01, 4.6375e-01, 6.7529e-01,\n",
            "        3.2468e-01, 4.2047e-01, 3.9865e-01, 3.5237e-01, 5.3080e-01, 9.4186e-01,\n",
            "        5.3536e-01, 4.1257e-01, 7.2979e-01, 2.9141e-01, 3.6041e-02, 4.2549e-01,\n",
            "        6.7648e-01, 3.7491e-01, 7.6045e-01, 2.9561e-01, 9.0382e-01, 8.8950e-01,\n",
            "        2.2288e-01, 4.5988e-01, 6.9447e-01, 8.0806e-02, 9.5128e-01, 4.3090e-01,\n",
            "        3.9034e-01, 7.3096e-01, 7.6624e-01, 4.5036e-01, 8.3188e-01, 7.8960e-01,\n",
            "        1.7234e-01, 3.5519e-02, 4.7791e-01, 8.4504e-02, 2.4303e-01, 8.4509e-01,\n",
            "        1.8788e-01, 9.9639e-01, 8.0707e-01, 7.8171e-01, 5.4331e-01, 6.4797e-01,\n",
            "        5.2917e-01, 1.6947e-01, 7.6488e-01, 6.5937e-01, 9.0480e-01, 9.2899e-01,\n",
            "        3.0326e-01, 8.8760e-01, 2.7162e-01, 5.2229e-03, 3.2780e-01, 3.2374e-01,\n",
            "        9.0291e-01, 1.9305e-01, 9.0766e-01, 9.5963e-02, 7.1140e-02, 8.2047e-01,\n",
            "        7.9658e-01, 5.3813e-02, 7.4867e-01, 7.0658e-01, 4.0883e-01, 9.9761e-01,\n",
            "        5.0302e-01, 9.0654e-01, 2.3421e-01, 6.6647e-01, 5.2286e-01, 4.9593e-01,\n",
            "        1.9660e-01, 1.3084e-01, 5.3072e-01, 9.7348e-02, 4.3109e-01, 1.3208e-01,\n",
            "        9.0689e-01, 7.9571e-01, 8.8805e-01, 5.3523e-01, 2.5143e-01, 6.6178e-01,\n",
            "        3.3424e-01, 9.3081e-01, 6.3937e-01, 1.3320e-01, 3.8932e-01, 2.3628e-02,\n",
            "        5.3198e-01, 7.0883e-01, 1.5275e-01, 4.1065e-01, 9.4503e-01, 5.1654e-01,\n",
            "        1.6495e-01, 7.8589e-01, 7.5305e-01, 9.5403e-01, 7.0414e-02, 4.6519e-01,\n",
            "        8.5542e-01, 6.5464e-01, 1.2614e-02, 9.1241e-01, 3.0673e-01, 3.4951e-01,\n",
            "        1.1231e-01, 5.1396e-01, 5.3476e-01, 8.2022e-01, 4.1907e-01, 9.8889e-01,\n",
            "        1.5810e-01, 2.1958e-01, 1.3171e-01, 6.5169e-01, 6.8643e-01, 2.8689e-01,\n",
            "        5.4830e-01, 7.1280e-01, 1.2308e-01, 1.4962e-01, 1.5875e-02, 2.9586e-01,\n",
            "        6.5793e-01, 9.3921e-01, 7.6564e-01, 4.1435e-01, 1.3865e-01, 1.7933e-01,\n",
            "        2.7493e-01, 6.0508e-01, 1.0220e-01, 3.2572e-01, 3.3568e-01, 6.6342e-01,\n",
            "        7.2847e-01, 8.3137e-01, 9.0796e-01, 5.5037e-01, 4.2813e-01, 1.6322e-02,\n",
            "        9.2906e-01, 3.4535e-01, 4.4033e-01, 6.8219e-01, 6.4316e-01, 1.8675e-01,\n",
            "        9.2110e-01, 9.0436e-01, 1.9257e-01, 5.0273e-01, 5.1613e-01, 6.3825e-03,\n",
            "        8.0513e-01, 5.6203e-01, 8.5306e-01, 8.9337e-01, 1.0281e-01, 1.3000e-02,\n",
            "        1.2041e-01, 9.6622e-01, 2.1246e-01, 9.8639e-01, 9.1385e-01, 3.0302e-01,\n",
            "        7.8732e-02, 9.7782e-01, 5.4672e-01, 5.3248e-02, 5.8178e-01, 9.4199e-01,\n",
            "        1.2386e-01, 8.7979e-02, 2.4510e-02, 4.1127e-01, 7.1789e-01, 4.4366e-01,\n",
            "        5.7643e-01, 6.8484e-01, 1.9174e-01, 2.0662e-01, 2.3579e-01, 8.6648e-01,\n",
            "        8.5762e-01, 4.5564e-01, 8.9704e-01, 7.0070e-01, 6.6777e-01, 4.6316e-03,\n",
            "        5.7459e-01, 2.9797e-01, 4.4784e-01, 4.6564e-01, 6.0291e-01, 4.2273e-01,\n",
            "        8.1636e-01, 2.2904e-02, 5.4359e-01, 4.9264e-01, 9.3910e-01, 8.7443e-01,\n",
            "        9.3460e-01, 6.1653e-01, 1.4437e-01, 6.4443e-01, 2.9421e-01, 2.3165e-01,\n",
            "        2.4792e-01, 2.1981e-01, 4.3506e-01, 6.7613e-01, 6.0256e-01, 2.0107e-01,\n",
            "        6.4663e-02, 5.7220e-02, 6.3477e-01, 9.5277e-01, 2.8832e-01, 4.0623e-01,\n",
            "        1.7032e-01, 5.7339e-01, 2.2010e-01, 9.1200e-01, 3.1625e-01, 6.5132e-01,\n",
            "        5.4677e-01, 8.0669e-02, 8.8956e-01, 6.6980e-01, 4.2272e-01, 1.4895e-01,\n",
            "        7.2740e-01, 7.7161e-01, 8.4883e-01, 9.9138e-01, 5.4285e-01, 2.0395e-01,\n",
            "        7.3185e-01, 1.2361e-01, 1.1101e-01, 1.0715e-01, 1.2637e-01, 6.6584e-01,\n",
            "        1.4212e-01, 5.6834e-01, 1.3325e-01, 4.6888e-01, 7.9978e-01, 2.8562e-01,\n",
            "        9.2013e-01, 6.9223e-01, 1.4176e-01, 8.2992e-01, 1.7445e-01, 3.8035e-01,\n",
            "        4.2083e-01, 4.1032e-01, 9.7543e-01, 3.3152e-01, 9.9944e-01, 7.0966e-01,\n",
            "        4.6805e-01, 7.4637e-01, 9.4696e-01, 3.5238e-01, 9.0876e-01, 8.8085e-01,\n",
            "        4.5461e-01, 6.3159e-01, 8.2889e-01, 3.2516e-01, 6.0343e-01, 3.5645e-01,\n",
            "        7.3247e-01, 3.4387e-02, 7.6275e-01, 4.2056e-01, 2.4536e-01, 4.7283e-01,\n",
            "        2.3791e-01, 5.6697e-01, 7.9955e-01, 5.7784e-01, 8.5945e-01, 7.6882e-01,\n",
            "        9.7967e-01, 1.6786e-01, 9.5356e-01, 2.3239e-01, 3.5695e-01, 8.6321e-01,\n",
            "        3.0798e-01, 6.7945e-01, 3.1305e-01, 2.5109e-01, 2.4883e-01, 3.5010e-01,\n",
            "        8.9865e-01, 2.9434e-01, 1.7774e-01, 1.7140e-01, 1.5750e-01, 7.7808e-01,\n",
            "        9.8669e-01, 5.8000e-02, 2.4452e-01, 7.4498e-01, 2.2053e-01, 5.9626e-01,\n",
            "        2.8350e-01, 9.2336e-01, 5.4269e-01, 7.0027e-01, 8.4125e-01, 3.7898e-01,\n",
            "        4.9702e-02, 5.9885e-02, 8.7787e-01, 7.1942e-01, 4.3235e-01, 9.8830e-01,\n",
            "        4.1313e-01, 4.5440e-01, 5.3116e-01, 1.4875e-01, 4.3982e-01, 5.4865e-02,\n",
            "        1.3503e-01, 9.0356e-01, 1.8823e-01, 8.6458e-01, 3.4769e-01, 6.7705e-01,\n",
            "        5.6982e-01, 8.6851e-01, 9.0702e-01, 3.2850e-02, 3.1945e-01, 5.2358e-01,\n",
            "        5.6512e-01, 8.2613e-01, 8.5934e-01, 4.1649e-01, 4.4974e-01, 3.0000e-01,\n",
            "        5.5728e-01, 6.1326e-01, 8.2572e-01, 7.9064e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([0.6784, 0.6235, 0.3644, 0.5346, 0.8762, 0.1512, 0.9387, 0.8266, 0.7671,\n",
            "        0.2086, 0.8562, 0.6592, 0.9538, 0.5671, 0.5645, 0.1272, 0.8472, 0.9039,\n",
            "        0.3185, 0.1376, 0.3115, 0.9183, 0.1401, 0.8064, 0.3869, 0.9762, 0.6068,\n",
            "        0.9238, 0.9314, 0.8920, 0.2160, 0.8796, 0.0300, 0.9451, 0.4539, 0.5226,\n",
            "        0.7018, 0.3351, 0.3861, 0.5240, 0.9145, 0.8909, 0.0218, 0.7369, 0.5930,\n",
            "        0.7698, 0.9222, 0.8113, 0.6002, 0.1444, 0.6534, 0.6218, 0.7484, 0.2697,\n",
            "        0.0510, 0.7733, 0.9146, 0.3274, 0.0217, 0.7928, 0.8934, 0.5487, 0.5490,\n",
            "        0.0112, 0.1062, 0.4493, 0.6437, 0.2014, 0.1633, 0.8354, 0.9063, 0.7693,\n",
            "        0.4945, 0.5268, 0.4780, 0.8288, 0.9561, 0.0592, 0.8292, 0.3826, 0.3066,\n",
            "        0.4982, 0.1973, 0.4271, 0.2626, 0.2767, 0.5322, 0.0442, 0.1270, 0.4047,\n",
            "        0.0723, 0.5764, 0.6422, 0.2080, 0.4140, 0.8863, 0.0453, 0.2546, 0.1195,\n",
            "        0.2526, 0.9359, 0.5794, 0.5372, 0.3185, 0.1364, 0.4512, 0.5802, 0.1803,\n",
            "        0.7030, 0.3526, 0.9624, 0.8647, 0.7072, 0.5164, 0.8746, 0.0858, 0.3540,\n",
            "        0.8458, 0.9013, 0.4322, 0.7936, 0.5698, 0.6698, 0.4092, 0.5376, 0.7359,\n",
            "        0.4487, 0.0528, 0.6604, 0.8987, 0.0057, 0.2441, 0.0078, 0.2910, 0.3824,\n",
            "        0.6257, 0.1289, 0.9749, 0.7382, 0.9773, 0.5775, 0.9949, 0.5780, 0.7808,\n",
            "        0.0836, 0.8420, 0.3558, 0.8872, 0.4979, 0.6963, 0.3681, 0.2167, 0.4053,\n",
            "        0.6558, 0.3691, 0.6452, 0.7730, 0.2068, 0.3302, 0.8997, 0.5018, 0.2274,\n",
            "        0.3907, 0.2705, 0.2460, 0.9772, 0.8734, 0.7198, 0.7483, 0.9134, 0.2015,\n",
            "        0.7112, 0.0524, 0.3575, 0.1930, 0.7456, 0.3311, 0.5462, 0.1780, 0.4823,\n",
            "        0.3539, 0.4562, 0.7112, 0.0159, 0.8835, 0.1672, 0.0680, 0.6817, 0.9121,\n",
            "        0.7550, 0.5744, 0.1999, 0.0018, 0.7685, 0.9104, 0.0927, 0.2094, 0.8321,\n",
            "        0.8867, 0.0675, 0.1637, 0.3621, 0.4640, 0.2503, 0.0311, 0.2549, 0.9090,\n",
            "        0.7342, 0.0924, 0.1217, 0.3217, 0.0627, 0.1213, 0.4474, 0.3529, 0.0309,\n",
            "        0.4219, 0.9554, 0.0163, 0.9318, 0.5835, 0.9554, 0.7403, 0.5334, 0.1167,\n",
            "        0.4132, 0.9867, 0.3201, 0.9092, 0.0653, 0.0542, 0.1168, 0.7949, 0.9580,\n",
            "        0.7024, 0.4977, 0.7286, 0.4343, 0.4850, 0.4538, 0.8126, 0.7253, 0.4467,\n",
            "        0.3670, 0.1800, 0.1205, 0.9694, 0.6593, 0.3647, 0.9707, 0.4891, 0.8182,\n",
            "        0.6913, 0.2587, 0.4151, 0.3259, 0.7044, 0.0496, 0.4261, 0.5147, 0.6358,\n",
            "        0.7421, 0.7011, 0.5036, 0.9075, 0.6022, 0.4341, 0.4760, 0.9012, 0.4937,\n",
            "        0.1925, 0.1053, 0.7142, 0.2562, 0.5194, 0.1092, 0.6304, 0.1187, 0.7631,\n",
            "        0.6623, 0.2020, 0.7485, 0.3977, 0.2826, 0.4837, 0.4110, 0.6162, 0.0844,\n",
            "        0.5064, 0.3938, 0.8653, 0.9417, 0.1107, 0.1411, 0.8886, 0.5084, 0.6218,\n",
            "        0.7476, 0.5636, 0.5904, 0.2139, 0.5116, 0.8946, 0.2359, 0.8748, 0.5551,\n",
            "        0.3832, 0.7703, 0.6916, 0.7292, 0.0194, 0.4008, 0.8561, 0.6467, 0.8020,\n",
            "        0.7440, 0.6605, 0.4693, 0.3017, 0.5773, 0.4007, 0.7508, 0.1562, 0.5287,\n",
            "        0.0509, 0.3874, 0.3898, 0.7904, 0.5228, 0.4741, 0.4149, 0.4427, 0.7151,\n",
            "        0.5981, 0.7247, 0.8232, 0.4037, 0.4420, 0.7169, 0.1125, 0.7332, 0.9874,\n",
            "        0.2912, 0.0027, 0.9890, 0.0125, 0.5286, 0.0031, 0.5644, 0.3879, 0.5343,\n",
            "        0.5566, 0.5104, 0.7170, 0.7225, 0.2686, 0.5240, 0.2758, 0.6046, 0.4614,\n",
            "        0.4604, 0.9738, 0.4617, 0.5121, 0.7719, 0.3504, 0.8815, 0.7221, 0.0528,\n",
            "        0.4156, 0.8869, 0.5834, 0.2289, 0.8352, 0.2379, 0.2948, 0.5596, 0.9436,\n",
            "        0.5886, 0.3951, 0.6245, 0.7499, 0.8436, 0.9987, 0.2009, 0.0714, 0.3889,\n",
            "        0.7434, 0.2623, 0.1543, 0.7750, 0.4522, 0.2781, 0.2573, 0.9410, 0.2503,\n",
            "        0.3438, 0.3238, 0.9693, 0.5902, 0.6587, 0.6868, 0.9484, 0.1933, 0.5222,\n",
            "        0.3838, 0.3818, 0.2327, 0.1621, 0.6169, 0.1535, 0.5111, 0.7503, 0.8230,\n",
            "        0.2499, 0.9388, 0.1447, 0.0104, 0.6580, 0.3508, 0.1926, 0.0972, 0.9835,\n",
            "        0.0683, 0.8489, 0.2573, 0.6734, 0.3819, 0.5286, 0.3777, 0.7838, 0.6649,\n",
            "        0.8159, 0.8775, 0.8150, 0.9931, 0.4992, 0.1905, 0.2055, 0.7019, 0.9372,\n",
            "        0.0606, 0.4172, 0.1006, 0.7459, 0.0271, 0.9087, 0.1648, 0.3555, 0.1223,\n",
            "        0.3219, 0.5779, 0.3058, 0.6183, 0.4141, 0.3372, 0.1454, 0.2594, 0.7887,\n",
            "        0.1095, 0.0474, 0.8610, 0.3973, 0.0759, 0.0113, 0.6526, 0.5097, 0.2263,\n",
            "        0.7900, 0.8344, 0.6277, 0.9329, 0.1163, 0.9068, 0.8280, 0.9719, 0.8307,\n",
            "        0.2270, 0.0650, 0.9199, 0.5802, 0.9391, 0.5322, 0.6071, 0.9160, 0.5065,\n",
            "        0.0437, 0.6603, 0.1655, 0.7871, 0.1045, 0.5643, 0.3734, 0.8770, 0.2047,\n",
            "        0.2980, 0.2422, 0.7242, 0.5071, 0.2726, 0.6061, 0.3848, 0.3121, 0.0590,\n",
            "        0.4238, 0.4080, 0.9657, 0.6014, 0.2466, 0.2269, 0.5701, 0.1768, 0.1604,\n",
            "        0.7826, 0.8111, 0.3427, 0.0274, 0.4983, 0.9527, 0.6263, 0.4814, 0.8488,\n",
            "        0.6438, 0.6112, 0.4058, 0.8898, 0.9083, 0.2872, 0.6543, 0.1740, 0.8500,\n",
            "        0.2558, 0.6668, 0.3124, 0.3289, 0.3891, 0.9560, 0.8345, 0.0223, 0.5751,\n",
            "        0.6082, 0.4903, 0.9619, 0.5106, 0.6112, 0.5984, 0.2798, 0.5524, 0.4952,\n",
            "        0.8869, 0.9611, 0.9338, 0.9605, 0.1111, 0.8514, 0.7727, 0.4308, 0.3270,\n",
            "        0.3250, 0.8456, 0.5924, 0.8536, 0.8897, 0.8206, 0.0200, 0.3415, 0.2269,\n",
            "        0.3082, 0.7101, 0.6375, 0.0944, 0.4214, 0.8807, 0.0697, 0.6873, 0.8778,\n",
            "        0.9397, 0.2247, 0.9129, 0.8671, 0.5559, 0.5302, 0.0189, 0.4168, 0.7971,\n",
            "        0.8624, 0.0118, 0.9259, 0.9880, 0.6660, 0.8555, 0.2535, 0.3288, 0.3859,\n",
            "        0.6715, 0.2032, 0.7296, 0.5646, 0.0551, 0.7685, 0.2179, 0.1681, 0.4663,\n",
            "        0.2552, 0.9724, 0.0640, 0.8370, 0.4380, 0.5169, 0.8231, 0.6409, 0.0764,\n",
            "        0.6243, 0.8370, 0.4843, 0.2462, 0.8289, 0.5002, 0.8351, 0.4174, 0.6754,\n",
            "        0.1526, 0.1580, 0.6297, 0.0736, 0.5920, 0.3881, 0.8016, 0.6022, 0.9510,\n",
            "        0.4736, 0.8752, 0.9132, 0.5675, 0.8055, 0.5851, 0.8392, 0.6475, 0.0495,\n",
            "        0.0607, 0.7811, 0.2044, 0.4126, 0.5425, 0.3536, 0.1742, 0.4089, 0.1388,\n",
            "        0.4559, 0.9799, 0.2273, 0.6208, 0.1976, 0.4923, 0.3316, 0.1445, 0.0432,\n",
            "        0.3566, 0.0904, 0.4052, 0.6535, 0.4887, 0.0096, 0.3738, 0.8941, 0.6255,\n",
            "        0.8328, 0.8164, 0.2083, 0.0793, 0.0547, 0.2770, 0.6138, 0.6106, 0.2490,\n",
            "        0.6924, 0.9344, 0.1558, 0.3176, 0.9337, 0.5435, 0.0406, 0.2434, 0.6333,\n",
            "        0.2574, 0.2470, 0.5054, 0.5794, 0.2819, 0.1226, 0.1448, 0.0291, 0.4543,\n",
            "        0.2454, 0.1485, 0.8469, 0.4227, 0.1175, 0.6544, 0.1777, 0.0352, 0.6873,\n",
            "        0.2225, 0.7071, 0.1905, 0.4637, 0.2479, 0.8905, 0.7420, 0.2025, 0.6009,\n",
            "        0.2935, 0.6847, 0.9576, 0.7228, 0.1666, 0.7178, 0.2049, 0.0804, 0.8376,\n",
            "        0.1339, 0.5214, 0.4127, 0.7224, 0.2331, 0.5606, 0.0073, 0.0965, 0.2787,\n",
            "        0.1915, 0.1789, 0.9156, 0.8532, 0.6550, 0.2981, 0.7357, 0.8234, 0.4441,\n",
            "        0.0547, 0.7239, 0.3292, 0.8827, 0.9777, 0.1098, 0.7931, 0.6833, 0.5602,\n",
            "        0.9389, 0.1616, 0.2441, 0.6098, 0.0973, 0.4033, 0.8104, 0.2545, 0.9614,\n",
            "        0.2753, 0.2853, 0.5830, 0.6953, 0.9253, 0.4933, 0.2856, 0.5483, 0.9308,\n",
            "        0.7291, 0.5458, 0.2816, 0.9579, 0.6998, 0.0682, 0.6947, 0.8433, 0.4905,\n",
            "        0.1175, 0.0944, 0.5828, 0.2389, 0.9869, 0.0428, 0.3501, 0.0689, 0.6785,\n",
            "        0.7162, 0.7547, 0.8297, 0.0531, 0.8075, 0.8172, 0.4299, 0.8959, 0.8981,\n",
            "        0.9611, 0.6520, 0.6387, 0.4589, 0.4702, 0.8186, 0.0884, 0.2388, 0.7697,\n",
            "        0.5064, 0.9995, 0.8973, 0.0747, 0.5404, 0.2934, 0.1641, 0.8675, 0.0636,\n",
            "        0.5422, 0.4655, 0.9391, 0.1254, 0.9925, 0.1589, 0.5341, 0.9741, 0.2394,\n",
            "        0.5703, 0.5233, 0.8000, 0.3480, 0.4624, 0.7854, 0.4184, 0.4972, 0.5166,\n",
            "        0.6737, 0.1397, 0.7896, 0.6571, 0.3644, 0.1902, 0.1926, 0.3663, 0.2238,\n",
            "        0.4606, 0.2655, 0.4102, 0.7391, 0.8903, 0.1795, 0.9364, 0.2622, 0.0063,\n",
            "        0.6190, 0.7052, 0.7932, 0.5491, 0.6236, 0.2927, 0.0539, 0.6418, 0.3545,\n",
            "        0.2990, 0.2510, 0.3221, 0.1140, 0.2374, 0.0641, 0.9235, 0.0989, 0.7280,\n",
            "        0.6237, 0.9506, 0.0421, 0.6003, 0.8941, 0.8271, 0.9215, 0.2286, 0.4598,\n",
            "        0.2721, 0.6081, 0.7006, 0.1549, 0.5088, 0.0689, 0.0965, 0.1635, 0.5103,\n",
            "        0.0021, 0.3611, 0.2607, 0.0068, 0.2297, 0.4275, 0.4297, 0.9545, 0.3591,\n",
            "        0.0976, 0.4203, 0.8318, 0.2503, 0.7218, 0.4533, 0.1315, 0.9975, 0.0168,\n",
            "        0.2349, 0.8517, 0.5562, 0.1791, 0.2338, 0.4919, 0.8969, 0.5179, 0.4551,\n",
            "        0.0349, 0.2030, 0.8356, 0.3203, 0.9853, 0.2371, 0.0041, 0.4920, 0.9822,\n",
            "        0.4637, 0.8543, 0.8141, 0.8193, 0.0426, 0.9087, 0.6693, 0.2612, 0.2949,\n",
            "        0.0716, 0.1854, 0.4819, 0.8853, 0.6361, 0.5899, 0.7988, 0.6045, 0.4937,\n",
            "        0.6731, 0.4470, 0.9444, 0.1046, 0.4949, 0.8251, 0.9772, 0.3799, 0.6868,\n",
            "        0.3921, 0.8826, 0.0287, 0.2196, 0.7621, 0.6823, 0.0864, 0.9105, 0.6591,\n",
            "        0.2117, 0.1259, 0.9008, 0.8223, 0.6678, 0.1490, 0.8478, 0.3287, 0.3705,\n",
            "        0.9632, 0.7270, 0.6723, 0.4530, 0.1526, 0.6607, 0.5637, 0.6514, 0.7250,\n",
            "        0.6115, 0.7712, 0.2941, 0.4751, 0.5888, 0.8770, 0.7896, 0.1156, 0.4534,\n",
            "        0.5883, 0.5729, 0.4322, 0.4413, 0.7544, 0.2949, 0.7415, 0.5195, 0.3550,\n",
            "        0.4569, 0.0239, 0.2978, 0.2781, 0.5084, 0.8156, 0.5358, 0.3403, 0.2442,\n",
            "        0.1564], grad_fn=<SplitBackward0>), tensor([4.9897e-01, 9.3550e-02, 5.9775e-01, 2.8223e-01, 7.2959e-01, 8.9217e-01,\n",
            "        4.1602e-01, 8.5832e-01, 6.3417e-01, 1.4630e-01, 6.8132e-01, 8.3708e-01,\n",
            "        1.1358e-01, 6.2575e-01, 9.1784e-01, 1.9449e-01, 2.0303e-02, 8.3208e-01,\n",
            "        8.6660e-01, 6.8410e-01, 4.4963e-01, 9.5010e-01, 4.6290e-01, 4.1132e-01,\n",
            "        6.9675e-01, 6.3008e-01, 2.3722e-01, 4.7748e-01, 7.9723e-01, 6.7180e-01,\n",
            "        9.5825e-01, 1.5681e-01, 6.6735e-01, 7.0714e-01, 7.8035e-01, 6.7844e-01,\n",
            "        1.9963e-01, 3.0200e-01, 8.8409e-01, 1.3480e-01, 9.7470e-01, 3.1436e-02,\n",
            "        3.7964e-01, 3.7119e-01, 5.6985e-01, 5.5072e-01, 7.5057e-01, 8.3129e-01,\n",
            "        6.2157e-01, 8.9082e-01, 7.5425e-01, 3.2632e-01, 2.9005e-01, 3.0883e-01,\n",
            "        5.9652e-02, 8.3640e-01, 5.0492e-01, 2.4181e-01, 4.3072e-01, 4.6250e-01,\n",
            "        4.4980e-01, 4.3856e-01, 5.2948e-01, 5.1483e-01, 6.9089e-01, 5.1807e-01,\n",
            "        7.1287e-01, 6.3639e-01, 5.1127e-01, 3.0640e-01, 7.0313e-01, 7.4435e-01,\n",
            "        8.4996e-01, 2.0808e-02, 2.9864e-01, 7.1027e-01, 6.2882e-01, 6.7440e-01,\n",
            "        8.3738e-02, 4.3211e-01, 8.3076e-01, 6.9613e-01, 2.6379e-01, 8.8182e-01,\n",
            "        7.4881e-01, 9.3097e-01, 1.0688e-01, 6.9609e-01, 6.6534e-01, 3.5723e-01,\n",
            "        8.7087e-01, 1.5072e-01, 9.6459e-01, 3.8339e-01, 4.9717e-01, 6.7336e-01,\n",
            "        7.6889e-01, 4.8029e-01, 5.3199e-01, 6.8221e-01, 9.0454e-02, 8.6113e-01,\n",
            "        8.7131e-01, 8.2339e-01, 3.3991e-01, 1.6725e-01, 3.0240e-01, 9.1846e-01,\n",
            "        1.7425e-01, 7.0048e-01, 1.0814e-01, 4.4965e-01, 2.0366e-01, 5.6545e-01,\n",
            "        4.7989e-01, 7.3158e-01, 3.6927e-01, 6.1686e-01, 8.4225e-01, 7.5561e-01,\n",
            "        1.6890e-01, 6.1289e-02, 5.6956e-01, 2.2233e-01, 3.5498e-01, 3.3679e-01,\n",
            "        9.7243e-01, 3.3957e-01, 3.2381e-01, 8.8945e-02, 2.8353e-01, 7.5170e-01,\n",
            "        1.2775e-01, 6.7966e-01, 8.5907e-01, 6.2300e-01, 7.1813e-01, 3.6590e-01,\n",
            "        7.0139e-03, 8.8972e-01, 2.5359e-01, 7.1157e-01, 2.5990e-01, 3.3874e-01,\n",
            "        2.1336e-01, 5.2969e-01, 1.5829e-01, 4.8685e-01, 1.7752e-01, 9.5912e-01,\n",
            "        7.3567e-01, 5.7182e-01, 8.8492e-01, 2.1295e-01, 9.3820e-01, 8.6894e-01,\n",
            "        1.8606e-01, 8.0784e-01, 1.3252e-01, 4.9562e-01, 4.1517e-01, 8.9751e-01,\n",
            "        1.9321e-01, 9.2567e-01, 6.6673e-01, 1.1065e-01, 5.2828e-01, 7.4912e-01,\n",
            "        5.2301e-01, 4.8537e-01, 9.5414e-01, 1.9961e-01, 1.8055e-01, 2.3797e-01,\n",
            "        5.0848e-01, 8.5150e-01, 8.2652e-01, 8.9714e-01, 6.6766e-01, 9.7697e-01,\n",
            "        6.5833e-01, 1.2635e-01, 4.7735e-01, 6.1193e-01, 8.6727e-01, 6.2576e-01,\n",
            "        3.3706e-01, 2.3546e-01, 5.5535e-01, 6.6797e-01, 9.5641e-01, 7.0091e-01,\n",
            "        2.7622e-01, 2.2979e-01, 6.1472e-01, 1.7207e-01, 6.6446e-01, 5.6984e-01,\n",
            "        7.4740e-01, 3.3120e-01, 6.5624e-01, 1.9493e-01, 1.7285e-01, 8.3884e-01,\n",
            "        9.6924e-01, 1.1440e-01, 3.3586e-01, 4.1856e-01, 5.6216e-01, 7.4768e-01,\n",
            "        9.2618e-01, 1.3616e-01, 8.3140e-01, 2.4746e-02, 3.7165e-01, 3.3009e-01,\n",
            "        1.1306e-01, 1.8881e-02, 1.7554e-01, 7.1052e-01, 2.1405e-01, 1.5585e-01,\n",
            "        6.0679e-01, 1.3002e-01, 3.7295e-01, 1.5548e-01, 7.3193e-02, 3.6875e-01,\n",
            "        7.6480e-01, 4.6573e-01, 5.5196e-01, 3.6423e-01, 2.3882e-01, 6.1283e-01,\n",
            "        4.5613e-01, 6.3874e-01, 7.7812e-01, 9.8347e-01, 9.4823e-01, 2.8409e-02,\n",
            "        2.6871e-02, 6.3989e-01, 1.5320e-01, 8.9401e-01, 5.3062e-01, 6.2987e-01,\n",
            "        5.4405e-01, 1.2033e-01, 3.1180e-01, 5.6756e-01, 3.2982e-01, 7.5466e-02,\n",
            "        1.5438e-01, 6.5344e-01, 9.0794e-01, 7.0679e-01, 2.9606e-01, 1.6475e-01,\n",
            "        2.7403e-01, 5.3017e-02, 7.6942e-01, 9.1241e-01, 5.9387e-01, 4.5628e-01,\n",
            "        8.1321e-01, 1.7375e-01, 5.7341e-01, 1.8417e-01, 3.6172e-01, 2.4227e-01,\n",
            "        8.2128e-01, 2.0827e-01, 8.9465e-01, 4.6781e-01, 4.1162e-01, 3.8388e-02,\n",
            "        3.2884e-03, 4.4264e-01, 5.1133e-01, 9.0593e-02, 5.2111e-01, 4.2921e-01,\n",
            "        7.2660e-01, 3.7249e-01, 3.8657e-01, 2.1492e-01, 5.1223e-01, 3.6171e-01,\n",
            "        4.5559e-01, 5.9269e-01, 9.0499e-01, 8.3753e-01, 7.2913e-01, 9.4887e-01,\n",
            "        6.1090e-01, 6.5057e-01, 3.6996e-01, 4.9045e-01, 3.5924e-02, 9.7028e-02,\n",
            "        8.8906e-01, 3.3238e-01, 6.4767e-02, 4.3174e-01, 9.6052e-01, 5.7105e-01,\n",
            "        1.8507e-02, 2.7568e-03, 3.8360e-01, 5.2727e-01, 5.0807e-01, 1.4922e-02,\n",
            "        9.3680e-01, 5.3973e-01, 2.5381e-01, 7.4977e-01, 2.4444e-01, 6.6788e-02,\n",
            "        4.2666e-02, 4.8767e-01, 8.6603e-01, 2.7216e-01, 7.3116e-01, 9.9074e-01,\n",
            "        3.3956e-01, 6.3644e-01, 7.3435e-01, 7.0695e-01, 8.7949e-01, 1.2254e-01,\n",
            "        4.2955e-01, 9.7072e-01, 8.0874e-01, 9.6024e-01, 7.5808e-01, 3.8134e-01,\n",
            "        1.5548e-01, 4.3464e-01, 3.4109e-03, 3.2718e-01, 1.6937e-01, 5.7084e-01,\n",
            "        7.7280e-02, 1.4832e-01, 7.3959e-01, 6.1365e-02, 9.7138e-01, 3.2690e-01,\n",
            "        4.5288e-02, 9.0063e-01, 3.4088e-01, 7.3342e-02, 6.0693e-02, 2.0110e-01,\n",
            "        2.8548e-01, 2.7786e-01, 7.0564e-01, 6.4133e-01, 2.6932e-01, 9.4246e-01,\n",
            "        2.4286e-01, 1.5797e-01, 9.7353e-01, 2.9803e-01, 7.8545e-01, 3.1105e-01,\n",
            "        9.8211e-02, 3.8942e-03, 8.9279e-01, 3.9287e-01, 2.4052e-01, 3.2774e-02,\n",
            "        5.3495e-01, 1.9997e-01, 3.1780e-01, 1.3816e-01, 2.8105e-01, 9.5719e-01,\n",
            "        4.6305e-01, 8.8826e-01, 1.1081e-01, 2.8759e-01, 2.0737e-01, 1.1292e-01,\n",
            "        2.6144e-01, 6.2015e-01, 7.8866e-01, 7.0837e-01, 7.1214e-01, 6.8440e-02,\n",
            "        6.1297e-02, 7.7005e-01, 5.1821e-01, 4.7276e-01, 2.5533e-01, 8.0316e-01,\n",
            "        2.1472e-01, 9.8112e-01, 3.1123e-01, 8.1006e-01, 1.6868e-01, 7.9173e-01,\n",
            "        3.0913e-01, 8.2266e-01, 4.4089e-01, 8.7030e-01, 3.0798e-01, 8.9323e-01,\n",
            "        2.3782e-02, 6.5556e-01, 8.7252e-01, 1.7065e-01, 9.5346e-01, 4.1463e-01,\n",
            "        7.8001e-01, 5.9763e-01, 9.8451e-01, 7.2576e-01, 8.0382e-01, 4.0419e-02,\n",
            "        3.3302e-01, 6.0241e-01, 4.0855e-01, 5.6617e-01, 2.9976e-01, 9.0999e-01,\n",
            "        1.4063e-01, 7.4923e-01, 1.0432e-01, 7.6670e-01, 2.9346e-02, 5.3804e-01,\n",
            "        8.7650e-01, 1.9900e-01, 7.2370e-01, 1.1499e-01, 3.2689e-01, 6.0428e-01,\n",
            "        6.0487e-01, 1.9460e-01, 1.5993e-01, 6.9998e-01, 5.5835e-01, 5.2910e-01,\n",
            "        3.8582e-01, 1.4916e-01, 5.4237e-01, 9.4768e-01, 1.4836e-01, 6.9897e-01,\n",
            "        2.7626e-01, 5.1195e-01, 4.1370e-01, 2.2569e-01, 5.7926e-01, 9.4724e-01,\n",
            "        8.8419e-01, 2.0478e-01, 2.8510e-01, 9.1672e-01, 7.7806e-01, 4.4203e-01,\n",
            "        5.0711e-01, 3.2467e-01, 4.1798e-01, 4.8289e-01, 7.7302e-01, 6.6018e-01,\n",
            "        9.6840e-01, 6.6520e-01, 4.7027e-01, 8.3788e-01, 4.7894e-01, 7.1797e-01,\n",
            "        9.1563e-01, 2.1769e-01, 6.4527e-01, 9.4470e-01, 7.1260e-01, 4.1022e-01,\n",
            "        4.1247e-01, 3.1249e-01, 6.1989e-01, 5.2536e-01, 2.7386e-01, 7.1770e-01,\n",
            "        2.5927e-01, 4.5757e-01, 3.9795e-01, 8.9872e-02, 8.1706e-01, 2.3607e-01,\n",
            "        5.2611e-01, 9.1414e-01, 6.2230e-01, 4.3293e-01, 2.9467e-01, 8.2304e-01,\n",
            "        4.4434e-01, 4.6412e-01, 4.2179e-01, 8.1382e-01, 6.8050e-01, 4.5623e-01,\n",
            "        8.4351e-01, 8.6277e-01, 4.1495e-01, 1.2512e-01, 3.1980e-01, 1.5475e-01,\n",
            "        4.9980e-01, 5.0232e-02, 5.7077e-01, 5.6970e-01, 3.9841e-01, 9.9933e-01,\n",
            "        3.2011e-01, 8.9657e-01, 6.5678e-01, 9.9389e-01, 7.4601e-01, 4.1144e-01,\n",
            "        9.0710e-01, 9.5463e-01, 3.8234e-01, 1.4236e-02, 4.9328e-01, 6.5925e-01,\n",
            "        8.6607e-01, 3.9497e-01, 1.2718e-01, 6.5246e-01, 6.2771e-01, 2.9158e-01,\n",
            "        3.9437e-01, 9.6199e-01, 2.9375e-01, 5.0944e-01, 9.1290e-01, 6.2756e-01,\n",
            "        9.2404e-01, 9.8053e-01, 5.8597e-01, 2.6179e-01, 2.4468e-01, 4.2914e-01,\n",
            "        5.7544e-01, 5.9412e-01, 7.9613e-02, 1.6358e-01, 7.5098e-03, 5.1251e-01,\n",
            "        3.7160e-01, 8.8354e-01, 1.0198e-01, 8.5112e-01, 9.6707e-01, 4.2072e-01,\n",
            "        5.8424e-01, 7.4442e-02, 3.6983e-01, 6.8914e-01, 6.8738e-01, 4.9556e-01,\n",
            "        3.8239e-01, 3.7052e-01, 9.7651e-01, 9.2905e-01, 5.9595e-01, 6.5923e-01,\n",
            "        9.0633e-01, 4.8227e-02, 3.3091e-01, 5.8486e-01, 3.0747e-01, 1.4867e-01,\n",
            "        9.8029e-01, 5.1548e-01, 6.2893e-01, 8.7491e-01, 6.4743e-01, 7.9656e-01,\n",
            "        5.8338e-01, 2.1125e-01, 8.1497e-01, 4.7107e-01, 2.1287e-01, 9.8632e-01,\n",
            "        9.4516e-01, 1.7979e-01, 8.4380e-03, 8.0363e-02, 5.2806e-01, 4.9691e-01,\n",
            "        2.4608e-01, 2.0598e-01, 8.1014e-01, 1.4876e-01, 5.7862e-01, 1.5150e-02,\n",
            "        3.8490e-01, 9.0468e-01, 1.4782e-01, 6.6358e-01, 1.8280e-01, 5.2359e-01,\n",
            "        8.2532e-01, 7.9836e-01, 1.7448e-01, 3.4710e-01, 5.4160e-01, 2.0538e-01,\n",
            "        8.7765e-01, 9.6356e-01, 8.1887e-01, 1.9429e-01, 1.4799e-01, 4.2041e-01,\n",
            "        9.7601e-01, 3.8495e-01, 5.2614e-01, 6.8584e-01, 9.9246e-01, 8.8821e-02,\n",
            "        5.3856e-01, 8.4284e-01, 2.1474e-01, 4.7992e-01, 7.8122e-01, 3.2776e-01,\n",
            "        2.0178e-01, 8.6818e-01, 6.2442e-01, 3.2373e-01, 4.9024e-01, 1.8795e-01,\n",
            "        8.3290e-01, 2.3421e-01, 8.4262e-01, 2.0459e-01, 4.4546e-01, 6.7238e-01,\n",
            "        8.4564e-01, 1.8709e-01, 5.7745e-01, 6.1146e-01, 5.9882e-01, 6.3122e-01,\n",
            "        6.5251e-01, 3.4462e-01, 3.5580e-01, 7.5753e-01, 4.0720e-01, 1.9171e-01,\n",
            "        7.4212e-01, 6.0503e-01, 7.5763e-01, 9.9350e-01, 2.6158e-01, 6.4085e-01,\n",
            "        5.5951e-01, 4.0491e-01, 7.6049e-01, 1.2270e-01, 5.3361e-01, 4.0520e-01,\n",
            "        9.2525e-01, 4.6704e-01, 1.6191e-01, 7.0219e-01, 7.4595e-02, 4.9674e-01,\n",
            "        8.6784e-02, 7.9572e-01, 2.4907e-01, 9.8375e-01, 6.8308e-01, 2.8495e-01,\n",
            "        9.2843e-01, 1.5998e-01, 4.8101e-01, 8.6501e-01, 5.3319e-01, 8.3637e-01,\n",
            "        4.2679e-01, 2.4588e-02, 5.6712e-02, 5.4364e-01, 1.4186e-01, 8.2067e-01,\n",
            "        1.9082e-01, 7.6055e-01, 9.3199e-01, 3.3392e-01, 4.9773e-01, 2.5288e-01,\n",
            "        4.3329e-01, 9.0051e-01, 1.4192e-01, 4.8519e-01, 5.7262e-01, 3.1149e-01,\n",
            "        7.9501e-01, 1.7713e-01, 2.2023e-01, 7.7003e-01, 8.3212e-01, 4.1076e-01,\n",
            "        6.2146e-01, 1.1976e-01, 7.9575e-01, 2.1365e-01, 6.5779e-01, 5.3089e-01,\n",
            "        1.4711e-01, 5.3384e-02, 3.8571e-01, 5.8543e-01, 7.8282e-01, 8.9652e-01,\n",
            "        1.8992e-01, 6.7658e-01, 7.7046e-01, 1.2839e-01, 3.1164e-01, 5.2359e-01,\n",
            "        2.9934e-01, 9.6928e-01, 1.6828e-01, 2.4492e-01, 4.9566e-01, 2.8849e-01,\n",
            "        2.8862e-01, 4.1163e-01, 4.2999e-01, 5.2070e-01, 5.9449e-01, 9.2308e-01,\n",
            "        8.1158e-01, 7.5692e-02, 8.3887e-01, 9.9695e-01, 5.7500e-02, 6.9108e-01,\n",
            "        5.1916e-01, 3.3434e-01, 3.2008e-01, 4.7345e-01, 6.7317e-01, 6.1963e-01,\n",
            "        1.9667e-01, 3.7334e-01, 5.3616e-01, 5.9494e-01, 2.7460e-03, 9.9912e-01,\n",
            "        7.6169e-01, 9.8701e-01, 9.6010e-01, 6.9313e-01, 2.2588e-02, 3.0511e-01,\n",
            "        6.2286e-01, 2.0862e-01, 2.3937e-01, 6.7305e-01, 6.7784e-01, 7.0781e-01,\n",
            "        8.5003e-01, 2.8431e-01, 1.0306e-04, 5.4888e-01, 2.0153e-01, 8.4904e-01,\n",
            "        2.1625e-01, 3.3213e-03, 4.1871e-01, 9.1230e-01, 5.9141e-01, 8.2795e-01,\n",
            "        1.0577e-01, 1.8536e-01, 1.1685e-02, 1.3514e-01, 8.8837e-01, 6.4342e-01,\n",
            "        1.5459e-01, 1.1910e-01, 4.7899e-01, 8.5570e-01, 7.3431e-01, 2.8567e-01,\n",
            "        8.8398e-01, 7.3202e-01, 9.2890e-01, 6.3927e-01, 9.0551e-01, 5.3307e-01,\n",
            "        6.6966e-01, 3.5249e-01, 3.1462e-01, 6.1645e-01, 5.2370e-01, 8.7695e-01,\n",
            "        8.1104e-01, 6.5683e-01, 2.4650e-01, 7.4834e-01, 5.9458e-01, 5.7492e-01,\n",
            "        9.0009e-01, 8.5930e-01, 8.1553e-01, 6.2674e-01, 7.1527e-01, 7.4759e-01,\n",
            "        1.8512e-01, 1.9401e-01, 6.1865e-01, 2.0065e-01, 4.2596e-01, 3.9876e-01,\n",
            "        5.8816e-01, 9.8626e-01, 8.5215e-02, 9.1408e-01, 4.9228e-01, 7.7126e-01,\n",
            "        7.5660e-01, 4.2464e-01, 7.0517e-01, 8.3160e-02, 3.8719e-01, 9.3351e-01,\n",
            "        5.8513e-01, 5.0153e-01, 4.1136e-01, 8.9255e-02, 5.3152e-01, 4.1371e-01,\n",
            "        4.4311e-01, 3.3961e-01, 1.7112e-04, 1.7165e-01, 7.0077e-01, 2.8173e-01,\n",
            "        8.9776e-01, 9.8203e-01, 5.1153e-01, 3.9136e-01, 5.8208e-02, 6.3981e-01,\n",
            "        4.9927e-01, 3.4119e-01, 4.0816e-01, 1.4764e-01, 5.0349e-01, 3.9677e-01,\n",
            "        9.8744e-01, 3.6894e-01, 2.6844e-01, 1.7296e-01, 1.9989e-02, 7.0174e-01,\n",
            "        9.0124e-01, 7.3673e-01, 5.7301e-01, 6.8155e-01, 1.7594e-01, 4.8872e-01,\n",
            "        1.2508e-01, 1.9755e-01, 5.7051e-01, 5.5209e-01, 2.3446e-01, 6.3493e-01,\n",
            "        3.9903e-01, 7.9030e-01, 7.6471e-01, 4.3394e-01, 4.2062e-01, 2.8234e-01,\n",
            "        4.5327e-01, 1.1776e-01, 7.4075e-03, 4.6976e-02, 9.6068e-01, 5.4188e-01,\n",
            "        2.0156e-01, 6.4283e-01, 6.4297e-01, 4.0948e-01, 9.7490e-01, 6.7712e-01,\n",
            "        1.1402e-01, 7.6749e-01, 6.8717e-01, 4.2238e-02, 6.4078e-01, 8.2980e-01,\n",
            "        2.6558e-01, 4.0751e-01, 4.0030e-01, 4.1450e-01, 7.3697e-01, 6.1703e-01,\n",
            "        6.6259e-01, 5.5028e-01, 1.7214e-01, 3.4268e-01, 9.0566e-01, 3.8243e-01,\n",
            "        6.7678e-02, 5.8170e-01, 4.0139e-01, 3.8811e-01, 7.4091e-03, 4.9674e-01,\n",
            "        5.2545e-01, 6.9010e-01, 5.8318e-01, 1.3695e-01, 8.6735e-01, 6.0423e-01,\n",
            "        4.3400e-01, 9.6586e-01, 8.4442e-02, 5.4187e-01, 3.0506e-02, 8.4402e-01,\n",
            "        6.3963e-01, 5.0863e-01, 3.6637e-01, 7.7403e-01, 6.9378e-02, 9.3161e-01,\n",
            "        3.3381e-01, 6.1384e-02, 2.9942e-01, 9.4664e-01, 8.9316e-01, 2.9595e-01,\n",
            "        1.6364e-01, 2.3120e-02, 5.2272e-01, 1.4250e-01, 3.4006e-02, 3.7432e-01,\n",
            "        8.5571e-01, 8.5512e-01, 1.1776e-01, 8.1826e-01, 1.7590e-01, 3.4020e-01,\n",
            "        8.5842e-01, 1.7369e-01, 8.0088e-01, 5.2113e-01, 3.0036e-02, 1.0451e-01,\n",
            "        2.7224e-01, 4.2000e-01, 8.1745e-01, 9.3220e-01, 9.4466e-01, 8.9076e-01,\n",
            "        2.1497e-01, 7.8180e-02, 9.7219e-01, 3.5241e-01, 5.6511e-01, 8.3140e-01,\n",
            "        2.2687e-02, 2.5531e-01, 5.0104e-01, 6.0865e-02, 4.5688e-01, 7.2153e-01,\n",
            "        2.2468e-01, 8.4133e-01, 3.9873e-01, 1.1805e-01, 9.4800e-01, 8.3534e-01,\n",
            "        1.3636e-01, 7.5339e-01, 9.7753e-01, 1.6560e-01, 2.4716e-01, 2.5907e-01,\n",
            "        8.3584e-01, 5.9024e-01, 3.8829e-01, 8.2126e-01, 4.5555e-01, 7.7851e-01,\n",
            "        1.3679e-01, 2.8473e-01, 5.3698e-01, 7.3921e-02],\n",
            "       grad_fn=<SplitBackward0>), tensor([1.2014e-01, 8.5831e-01, 9.3417e-01, 1.3410e-01, 6.7212e-02, 2.8860e-01,\n",
            "        7.3039e-01, 4.2367e-01, 9.2523e-01, 9.0372e-01, 6.0289e-01, 1.9721e-01,\n",
            "        2.9759e-01, 1.5527e-01, 8.8208e-02, 1.1785e-01, 5.5948e-01, 3.6465e-02,\n",
            "        8.9993e-01, 3.5701e-01, 8.0443e-01, 7.2564e-01, 4.4397e-01, 9.2585e-01,\n",
            "        1.9234e-01, 8.4044e-01, 3.5462e-01, 4.3362e-01, 6.5951e-01, 7.0493e-02,\n",
            "        6.8421e-01, 5.5629e-02, 5.2031e-01, 2.2210e-01, 6.6015e-01, 8.6598e-02,\n",
            "        9.7256e-01, 2.0629e-01, 3.6704e-01, 7.8428e-01, 3.2930e-01, 2.8164e-01,\n",
            "        1.0489e-01, 3.1802e-01, 8.0531e-02, 2.2598e-01, 1.4804e-01, 1.8677e-01,\n",
            "        6.1236e-01, 7.4559e-01, 4.7885e-01, 7.2234e-01, 7.6611e-01, 2.7117e-01,\n",
            "        6.7577e-02, 4.9627e-01, 6.5370e-01, 8.9288e-01, 5.4537e-01, 7.0726e-01,\n",
            "        7.1517e-01, 1.8056e-01, 6.8532e-01, 3.3643e-01, 6.3834e-01, 6.6115e-01,\n",
            "        3.3606e-01, 3.9235e-01, 7.7834e-01, 6.6842e-01, 5.8630e-01, 2.9639e-01,\n",
            "        3.6529e-01, 8.8180e-01, 7.5631e-01, 6.4240e-01, 4.5340e-01, 9.2708e-01,\n",
            "        4.2855e-01, 8.6230e-01, 8.7021e-01, 1.5396e-04, 4.1538e-01, 6.5634e-01,\n",
            "        1.0720e-03, 1.8548e-03, 5.3208e-01, 8.3687e-01, 7.1318e-01, 5.0398e-01,\n",
            "        6.4196e-02, 4.2394e-01, 1.0599e-01, 1.6200e-01, 4.0746e-01, 9.7140e-01,\n",
            "        7.0874e-01, 7.8720e-01, 9.7174e-01, 9.8755e-02, 2.4037e-01, 3.5968e-02,\n",
            "        9.6849e-01, 1.9555e-01, 9.5897e-01, 8.4500e-01, 6.7776e-01, 6.9641e-02,\n",
            "        1.0385e-01, 2.6800e-01, 5.5216e-01, 5.7161e-01, 7.4276e-01, 1.5169e-01,\n",
            "        3.8694e-01, 4.4600e-01, 3.8067e-02, 7.0402e-02, 3.7766e-01, 8.7419e-01,\n",
            "        8.9534e-02, 2.2916e-01, 2.5009e-01, 3.2016e-01, 8.1917e-01, 1.9041e-01,\n",
            "        8.1356e-01, 5.3112e-01, 6.2753e-03, 2.2942e-01, 9.2604e-01, 5.3374e-01,\n",
            "        3.6499e-01, 1.1229e-01, 7.1745e-01, 9.4948e-01, 4.1278e-01, 9.3093e-02,\n",
            "        2.5121e-01, 4.3217e-01, 6.0611e-01, 6.3464e-01, 3.6930e-01, 5.7397e-03,\n",
            "        1.9480e-01, 6.0307e-01, 8.6619e-01, 9.5221e-02, 8.1071e-01, 2.6102e-01,\n",
            "        5.1938e-01, 6.2610e-01, 2.9547e-01, 9.8033e-01, 8.6057e-01, 7.0818e-01,\n",
            "        8.5004e-01, 8.9473e-01, 2.8310e-02, 5.7461e-01, 7.6655e-01, 4.7460e-01,\n",
            "        6.8639e-01, 3.5312e-01, 7.7811e-01, 3.6832e-01, 8.4565e-02, 4.8394e-01,\n",
            "        5.1238e-01, 8.2766e-01, 4.6800e-01, 3.4337e-01, 2.7902e-01, 3.4257e-01,\n",
            "        9.1779e-01, 9.6479e-01, 1.8043e-01, 7.6086e-01, 1.2644e-01, 5.9099e-01,\n",
            "        7.1733e-01, 3.9663e-01, 2.6338e-01, 7.5534e-01, 1.4070e-01, 2.5044e-01,\n",
            "        4.5458e-01, 7.4468e-01, 8.9215e-02, 6.7905e-01, 6.8059e-01, 7.0521e-01,\n",
            "        7.2614e-01, 8.0107e-01, 8.5483e-01, 2.4197e-01, 9.6012e-02, 7.1288e-01,\n",
            "        8.7189e-01, 4.1629e-01, 5.3284e-01, 6.2212e-01, 6.0206e-01, 9.2436e-01,\n",
            "        7.5933e-01, 9.1916e-01, 3.6144e-01, 3.5096e-01, 8.5723e-01, 2.1028e-01,\n",
            "        7.9521e-01, 5.1128e-01, 8.8774e-01, 6.4010e-01, 5.0118e-01, 9.7114e-01,\n",
            "        8.4856e-01, 3.4791e-01, 8.8015e-01, 5.4368e-01, 3.2937e-02, 6.2407e-01,\n",
            "        7.7892e-02, 3.1706e-01, 3.6049e-01, 6.4931e-02, 9.1728e-01, 4.9353e-01,\n",
            "        4.7751e-01, 3.8590e-01, 9.1503e-02, 9.6992e-01, 9.8477e-02, 7.8405e-01,\n",
            "        4.5699e-01, 4.4759e-01, 8.4602e-01, 8.5064e-02, 6.2185e-01, 6.5701e-01,\n",
            "        3.0969e-01, 9.1300e-01, 9.6179e-01, 2.1502e-01, 3.9905e-01, 3.6323e-01,\n",
            "        3.2232e-01, 3.2917e-01, 7.9532e-01, 4.7430e-01, 8.3654e-02, 8.5949e-01,\n",
            "        1.4683e-01, 4.6730e-01, 6.5474e-01, 1.8232e-01, 9.4088e-01, 7.5834e-03,\n",
            "        6.5700e-01, 4.8849e-01, 9.7386e-01, 6.7292e-01, 2.2368e-02, 8.5717e-01,\n",
            "        5.8427e-02, 2.0487e-02, 5.3464e-01, 2.3937e-01, 7.0835e-01, 8.3824e-01,\n",
            "        4.8865e-01, 1.3686e-01, 4.2514e-01, 9.2779e-02, 3.7707e-01, 4.9625e-01,\n",
            "        6.1475e-01, 9.8585e-01, 7.3498e-01, 7.0492e-01, 7.2481e-01, 3.9503e-02,\n",
            "        2.8010e-01, 4.4191e-01, 4.1953e-01, 1.8828e-02, 3.8878e-01, 4.9354e-01,\n",
            "        3.0617e-01, 7.3967e-01, 3.7501e-01, 4.8126e-01, 4.9911e-01, 1.2445e-01,\n",
            "        3.1863e-01, 9.9692e-01, 1.9288e-01, 2.6520e-02, 6.8173e-01, 9.3032e-01,\n",
            "        4.0985e-01, 2.6283e-01, 5.3595e-01, 3.3686e-01, 5.1973e-01, 4.1044e-01,\n",
            "        4.6195e-01, 4.4434e-03, 5.7998e-02, 3.2522e-01, 8.9593e-02, 6.2681e-02,\n",
            "        6.3174e-01, 1.5807e-01, 8.1044e-01, 8.8899e-01, 2.4131e-01, 5.7213e-01,\n",
            "        2.0552e-01, 3.1538e-01, 1.9807e-01, 1.2470e-01, 4.7310e-01, 5.5602e-01,\n",
            "        1.4114e-01, 1.0152e-01, 3.8388e-01, 6.5126e-01, 2.7524e-01, 3.0020e-02,\n",
            "        8.2983e-02, 5.9207e-01, 7.4412e-01, 1.7537e-01, 8.5086e-01, 7.5731e-01,\n",
            "        4.0232e-01, 1.6821e-01, 9.6603e-01, 8.1998e-01, 7.0220e-01, 8.8399e-01,\n",
            "        8.8517e-01, 8.6255e-01, 2.3395e-01, 2.5177e-01, 8.2656e-01, 6.5862e-01,\n",
            "        9.7770e-01, 1.2952e-01, 4.2663e-01, 8.3535e-01, 8.1959e-01, 3.6032e-01,\n",
            "        1.9778e-01, 4.6496e-01, 5.5472e-01, 8.7010e-01, 6.6025e-01, 2.6899e-01,\n",
            "        4.1857e-02, 1.1960e-01, 9.4480e-01, 4.4483e-01, 9.3089e-01, 6.7196e-02,\n",
            "        1.3929e-01, 4.3749e-01, 7.9829e-01, 6.2076e-02, 5.3387e-01, 1.9834e-01,\n",
            "        7.8725e-01, 4.1630e-02, 3.2173e-01, 8.9025e-01, 6.8835e-01, 8.6128e-01,\n",
            "        8.8811e-01, 9.2296e-01, 4.4624e-01, 9.0231e-01, 5.8165e-02, 9.0847e-01,\n",
            "        1.1858e-01, 9.2013e-01, 2.6668e-01, 2.3659e-01, 3.1986e-01, 7.7776e-01,\n",
            "        2.1923e-01, 4.5335e-01, 7.5523e-01, 7.1728e-02, 3.4689e-01, 5.5527e-01,\n",
            "        5.5772e-01, 8.2927e-01, 6.1529e-01, 7.1657e-01, 3.4675e-01, 5.4025e-01,\n",
            "        3.9243e-01, 5.4112e-01, 1.4158e-01, 1.4417e-01, 9.5324e-01, 5.5489e-01,\n",
            "        4.0325e-01, 4.1164e-01, 4.2008e-01, 1.6147e-01, 9.9143e-01, 6.4091e-01,\n",
            "        8.6406e-01, 4.5610e-01, 1.3914e-01, 3.5323e-01, 3.7639e-01, 1.0452e-01,\n",
            "        8.8441e-01, 3.7985e-01, 5.7938e-01, 1.4400e-01, 9.5157e-01, 9.7350e-01,\n",
            "        8.0552e-01, 3.9889e-01, 3.8040e-02, 2.2035e-01, 5.3578e-01, 6.3216e-01,\n",
            "        3.3106e-01, 1.6694e-01, 4.5362e-01, 1.5537e-01, 7.3101e-01, 7.3508e-01,\n",
            "        3.6775e-02, 7.5753e-01, 1.8197e-01, 5.6627e-01, 6.8057e-01, 6.4788e-01,\n",
            "        6.2679e-01, 3.8148e-01, 5.4909e-01, 1.9744e-01, 4.7340e-01, 1.5144e-01,\n",
            "        8.9980e-01, 6.3312e-01, 2.0196e-01, 3.2075e-01, 7.0748e-01, 2.6336e-01,\n",
            "        6.6125e-01, 3.0480e-01, 7.4340e-01, 7.9139e-02, 9.6466e-01, 8.1472e-01,\n",
            "        8.8101e-01, 6.4179e-01, 9.8739e-02, 3.6648e-01, 7.0407e-01, 3.5192e-02,\n",
            "        4.4524e-01, 8.4307e-01, 5.6450e-01, 4.0045e-01, 8.4798e-01, 1.5894e-01,\n",
            "        4.3623e-01, 8.6377e-01, 2.4984e-01, 7.8566e-01, 5.4396e-01, 3.5635e-01,\n",
            "        8.9734e-01, 5.6566e-01, 4.9829e-01, 6.1693e-01, 8.4644e-01, 8.7640e-01,\n",
            "        6.8367e-01, 7.4063e-01, 2.9171e-01, 1.8925e-01, 6.2857e-01, 4.5740e-01,\n",
            "        3.8074e-01, 5.8665e-01, 4.7513e-01, 7.7924e-01, 5.3675e-01, 1.9897e-01,\n",
            "        5.3536e-01, 1.3648e-01, 2.1862e-01, 6.9061e-01, 3.5151e-01, 1.6138e-01,\n",
            "        3.9857e-01, 2.5417e-01, 6.2912e-01, 4.6107e-01, 7.7712e-01, 4.7366e-01,\n",
            "        5.5286e-01, 8.8117e-01, 7.8859e-01, 9.3584e-01, 3.7889e-01, 6.4138e-01,\n",
            "        3.0028e-01, 3.9622e-01, 9.3132e-01, 6.6391e-01, 5.6617e-01, 1.8945e-01,\n",
            "        3.7721e-02, 7.0633e-01, 8.5981e-01, 6.4234e-01, 1.8401e-01, 3.0367e-01,\n",
            "        5.6857e-01, 6.3589e-01, 3.8086e-01, 4.2636e-01, 2.4281e-02, 2.9958e-01,\n",
            "        6.8230e-01, 9.0758e-01, 6.8238e-01, 1.3442e-01, 2.4905e-02, 6.8528e-01,\n",
            "        9.1582e-02, 5.8107e-01, 2.7339e-01, 6.1516e-01, 8.6773e-01, 7.7134e-01,\n",
            "        7.9639e-01, 5.0288e-01, 3.0264e-01, 5.0209e-01, 8.4226e-01, 5.6681e-01,\n",
            "        7.8974e-01, 5.8896e-01, 1.2292e-01, 9.1343e-01, 3.5618e-01, 7.6899e-01,\n",
            "        2.2873e-01, 1.5766e-03, 8.3801e-01, 2.4163e-01, 4.3605e-01, 3.1298e-01,\n",
            "        2.7082e-01, 5.3822e-02, 6.8480e-01, 9.8770e-01, 6.9653e-01, 5.2001e-01,\n",
            "        8.1203e-02, 3.5977e-01, 4.8101e-01, 1.4554e-01, 2.5181e-01, 5.5935e-01,\n",
            "        1.3481e-02, 9.2408e-01, 2.3453e-01, 6.3107e-02, 4.4629e-01, 9.0275e-01,\n",
            "        7.5288e-01, 9.9805e-01, 2.8579e-01, 5.7664e-01, 1.6032e-01, 4.9750e-01,\n",
            "        1.1764e-01, 5.9455e-01, 3.4872e-02, 1.2510e-01, 1.0679e-01, 8.2133e-01,\n",
            "        8.9020e-02, 2.5215e-01, 2.3367e-01, 8.2469e-01, 7.6821e-01, 2.5890e-01,\n",
            "        9.3475e-02, 7.3681e-01, 7.8646e-01, 5.3704e-01, 4.5593e-01, 5.7832e-01,\n",
            "        7.8538e-01, 5.8018e-01, 7.6433e-01, 8.4716e-01, 5.5493e-01, 9.0410e-02,\n",
            "        4.9271e-01, 6.6663e-01, 1.4611e-01, 3.3693e-01, 4.4483e-01, 5.3556e-01,\n",
            "        1.5924e-01, 8.1165e-01, 5.1745e-01, 4.4988e-01, 9.6272e-01, 7.6525e-01,\n",
            "        2.2887e-01, 1.2683e-01, 2.1638e-01, 8.6175e-03, 1.5063e-02, 7.2228e-02,\n",
            "        1.8295e-01, 3.2225e-01, 5.6687e-02, 9.7260e-01, 7.8490e-01, 2.0805e-01,\n",
            "        7.5278e-01, 9.3383e-01, 8.8765e-01, 7.0697e-01, 9.9984e-02, 4.8489e-01,\n",
            "        8.8104e-01, 4.4085e-01, 6.5924e-01, 4.3142e-01, 4.1594e-01, 2.1889e-01,\n",
            "        8.0347e-01, 8.2329e-02, 7.1919e-01, 6.6522e-01, 9.1412e-01, 5.9035e-01,\n",
            "        1.2260e-01, 4.8004e-01, 6.9921e-01, 2.5519e-02, 6.4148e-01, 7.6863e-02,\n",
            "        4.5400e-01, 9.4397e-02, 4.6226e-01, 4.0370e-02, 7.0533e-01, 4.9440e-02,\n",
            "        6.0728e-01, 1.1377e-01, 9.1364e-01, 1.9158e-01, 5.9325e-01, 4.7672e-01,\n",
            "        3.4142e-01, 5.0678e-01, 4.9002e-01, 1.7565e-01, 4.0555e-01, 9.3467e-01,\n",
            "        4.3054e-01, 7.4152e-02, 1.5921e-01, 2.0058e-01, 6.7203e-01, 6.1468e-01,\n",
            "        6.6967e-01, 4.8802e-01, 8.7470e-01, 1.4833e-02, 8.1829e-01, 7.6199e-01,\n",
            "        3.8069e-01, 6.1439e-01, 7.7105e-01, 1.3934e-02, 1.0418e-01, 6.4763e-01,\n",
            "        7.2238e-01, 3.3072e-01, 4.1835e-01, 5.8634e-01, 6.5690e-01, 3.4034e-01,\n",
            "        5.0716e-01, 1.9026e-01, 1.1771e-01, 5.0989e-01, 8.7377e-01, 7.4202e-01,\n",
            "        2.5658e-01, 9.2848e-01, 3.1554e-01, 7.2245e-01, 5.3621e-02, 3.5554e-01,\n",
            "        5.1280e-01, 1.0895e-01, 4.4606e-01, 8.8631e-01, 2.0394e-01, 2.7541e-01,\n",
            "        2.4802e-01, 8.5901e-01, 8.2737e-01, 1.2091e-02, 3.2564e-01, 6.7921e-01,\n",
            "        1.8050e-01, 8.7174e-01, 7.4310e-01, 2.1972e-02, 4.4036e-01, 1.3290e-01,\n",
            "        2.3702e-01, 3.8354e-01, 6.8003e-01, 9.3703e-01, 6.1300e-01, 6.5161e-01,\n",
            "        2.1373e-01, 4.6227e-01, 9.5684e-01, 2.1830e-01, 3.7269e-01, 2.1349e-01,\n",
            "        1.2736e-01, 4.5665e-01, 6.2947e-01, 5.2999e-01, 2.4433e-01, 3.1002e-01,\n",
            "        8.2775e-01, 6.6077e-01, 6.1002e-01, 9.8549e-01, 2.2036e-01, 3.4016e-01,\n",
            "        2.1077e-03, 5.4405e-01, 9.4285e-01, 5.1138e-02, 9.0519e-01, 2.0050e-01,\n",
            "        9.3047e-01, 5.9941e-01, 1.6437e-01, 2.5707e-01, 1.3288e-01, 7.3288e-01,\n",
            "        5.8856e-01, 1.5173e-02, 8.7931e-01, 5.9784e-01, 3.5103e-01, 9.6186e-01,\n",
            "        3.8756e-01, 7.9529e-01, 9.4056e-01, 8.7641e-01, 7.3033e-01, 4.6306e-01,\n",
            "        5.6783e-01, 3.3114e-01, 2.5195e-01, 4.6561e-01, 2.3616e-01, 2.4056e-01,\n",
            "        9.3680e-01, 1.3871e-01, 3.5407e-01, 6.2237e-01, 1.7547e-01, 2.5779e-01,\n",
            "        6.4214e-01, 9.3594e-01, 4.4036e-01, 4.3645e-01, 4.7067e-01, 7.6276e-01,\n",
            "        2.0166e-01, 5.6104e-01, 6.6144e-01, 3.1877e-01, 7.4334e-01, 7.0991e-01,\n",
            "        4.9535e-01, 7.2530e-01, 6.2267e-01, 7.7671e-01, 5.5128e-01, 7.8771e-01,\n",
            "        3.0829e-01, 2.6086e-01, 2.4852e-02, 4.2260e-01, 9.3328e-01, 1.4958e-01,\n",
            "        1.0304e-02, 8.4338e-01, 8.3075e-01, 1.2918e-01, 1.9188e-01, 1.0132e-01,\n",
            "        4.8942e-01, 2.3221e-01, 5.5566e-01, 3.7043e-01, 3.2008e-01, 7.7212e-01,\n",
            "        7.1488e-01, 3.4729e-01, 5.5841e-02, 8.4570e-01, 6.1366e-01, 8.9539e-01,\n",
            "        8.6013e-01, 6.5103e-01, 9.0320e-01, 1.4730e-02, 9.1163e-01, 8.7895e-01,\n",
            "        5.7105e-01, 1.6180e-01, 6.9245e-01, 9.6388e-01, 1.7877e-01, 4.4916e-01,\n",
            "        7.4217e-01, 4.3054e-01, 4.4406e-01, 6.3742e-01, 7.3074e-01, 4.6962e-01,\n",
            "        9.3925e-01, 3.3283e-01, 8.2607e-01, 2.8726e-01, 4.4139e-02, 3.9636e-01,\n",
            "        3.3701e-01, 3.5851e-01, 3.4912e-01, 9.5818e-01, 3.9559e-01, 1.0783e-01,\n",
            "        5.8104e-01, 1.0069e-01, 9.2977e-01, 9.8903e-01, 6.5659e-01, 7.2250e-01,\n",
            "        9.8822e-02, 3.6440e-01, 9.4249e-01, 3.6784e-02, 2.5667e-01, 4.2707e-01,\n",
            "        9.9440e-01, 8.5554e-01, 2.3585e-01, 7.1646e-01, 7.2084e-01, 1.5907e-01,\n",
            "        4.3995e-02, 4.1341e-01, 4.1362e-01, 2.3638e-01, 6.9679e-02, 1.0414e-01,\n",
            "        7.0413e-01, 7.1429e-01, 7.7399e-01, 1.0867e-03, 7.2873e-01, 8.6487e-02,\n",
            "        7.3478e-01, 2.6909e-01, 6.0708e-01, 2.2066e-01, 8.3025e-01, 1.9228e-02,\n",
            "        2.7793e-01, 5.2157e-01, 6.5485e-01, 4.3982e-01, 9.9517e-01, 3.5211e-01,\n",
            "        8.6257e-01, 6.9227e-01, 3.2184e-01, 4.4767e-01, 3.9974e-02, 4.4036e-01,\n",
            "        6.5790e-01, 5.3362e-01, 9.6847e-01, 9.0558e-01, 4.6035e-01, 6.7374e-01,\n",
            "        4.3464e-01, 1.1332e-01, 5.0135e-01, 7.9000e-02, 8.8656e-01, 8.0535e-01,\n",
            "        2.8822e-01, 1.7195e-01, 9.4437e-01, 7.7007e-01, 7.8985e-02, 7.2815e-02,\n",
            "        2.3889e-01, 4.0774e-01, 7.0376e-01, 9.8040e-01, 2.9505e-01, 5.9350e-01,\n",
            "        1.8091e-01, 6.7312e-01, 2.5836e-01, 2.0411e-01, 3.4247e-01, 1.9128e-02,\n",
            "        1.4052e-01, 3.2978e-01, 6.3139e-01, 6.9048e-01, 9.1785e-01, 2.6982e-01,\n",
            "        8.0035e-01, 3.9154e-01, 5.9890e-01, 5.2166e-01, 2.0329e-01, 6.8092e-02,\n",
            "        7.9114e-01, 3.2618e-01, 2.9818e-01, 5.3340e-01, 4.9612e-01, 8.3199e-01,\n",
            "        8.5446e-03, 4.8767e-01, 5.8798e-01, 5.8272e-01, 9.3628e-02, 1.1991e-01,\n",
            "        3.1390e-01, 2.3023e-02, 3.4268e-01, 2.4235e-01, 6.7038e-01, 2.4175e-01,\n",
            "        7.5359e-02, 3.9080e-01, 5.8577e-01, 4.0530e-01, 5.7275e-01, 8.2588e-01,\n",
            "        2.6059e-01, 8.5173e-01, 3.8306e-01, 8.2038e-02, 6.8852e-01, 2.2066e-02,\n",
            "        7.6423e-01, 1.5237e-01, 9.6603e-01, 5.7094e-01, 7.3853e-01, 6.3702e-01,\n",
            "        6.0287e-01, 4.1069e-01, 2.4193e-01, 7.3559e-01, 4.1872e-01, 9.4873e-01,\n",
            "        5.0111e-01, 5.5228e-01, 2.1433e-01, 7.9984e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([0.6496, 0.9589, 0.3861, 0.1461, 0.3792, 0.5986, 0.8642, 0.9185, 0.6079,\n",
            "        0.6343, 0.1575, 0.7372, 0.6204, 0.4897, 0.7192, 0.5002, 0.8579, 0.6054,\n",
            "        0.5317, 0.0172, 0.8095, 0.7306, 0.7816, 0.5778, 0.3689, 0.9142, 0.8498,\n",
            "        0.6859, 0.4428, 0.0950, 0.6039, 0.2131, 0.1676, 0.4339, 0.8810, 0.5123,\n",
            "        0.5604, 0.5223, 0.1523, 0.9158, 0.6581, 0.9351, 0.6949, 0.8286, 0.3892,\n",
            "        0.6417, 0.0679, 0.0869, 0.9586, 0.4956, 0.9678, 0.2796, 0.5241, 0.8966,\n",
            "        0.7106, 0.4531, 0.6310, 0.5576, 0.3876, 0.0224, 0.8473, 0.7876, 0.6649,\n",
            "        0.6337, 0.6985, 0.4503, 0.8946, 0.8412, 0.7578, 0.8790, 0.7980, 0.3376,\n",
            "        0.7271, 0.2426, 0.6122, 0.0802, 0.2922, 0.0446, 0.4259, 0.1459, 0.2227,\n",
            "        0.7445, 0.0430, 0.7992, 0.9832, 0.0387, 0.5959, 0.3031, 0.3972, 0.1947,\n",
            "        0.9502, 0.0052, 0.1868, 0.9183, 0.1020, 0.5522, 0.6134, 0.4246, 0.9607,\n",
            "        0.1041, 0.0044, 0.5063, 0.0918, 0.5134, 0.8590, 0.1779, 0.5531, 0.1648,\n",
            "        0.2959, 0.0460, 0.0750, 0.7007, 0.8076, 0.9590, 0.7089, 0.8432, 0.5151,\n",
            "        0.5428, 0.8674, 0.4309, 0.1937, 0.5459, 0.1427, 0.3202, 0.8294, 0.1649,\n",
            "        0.0579, 0.9335, 0.4586, 0.0469, 0.3827, 0.8666, 0.9104, 0.7672, 0.0975,\n",
            "        0.2204, 0.8970, 0.9555, 0.7374, 0.9184, 0.3571, 0.6043, 0.1839, 0.2297,\n",
            "        0.6080, 0.2407, 0.6170, 0.1873, 0.2366, 0.4630, 0.7764, 0.5117, 0.6555,\n",
            "        0.4404, 0.9615, 0.5689, 0.7538, 0.2396, 0.9837, 0.4002, 0.9837, 0.6763,\n",
            "        0.9195, 0.5722, 0.0517, 0.8731, 0.3020, 0.2690, 0.5599, 0.3844, 0.5435,\n",
            "        0.6279, 0.6455, 0.6480, 0.1341, 0.0707, 0.4437, 0.0848, 0.3048, 0.1145,\n",
            "        0.5054, 0.2075, 0.0331, 0.9817, 0.1080, 0.2312, 0.7636, 0.7938, 0.9188,\n",
            "        0.4745, 0.7328, 0.6325, 0.0078, 0.2341, 0.7938, 0.0265, 0.1762, 0.3517,\n",
            "        0.7269, 0.9971, 0.8700, 0.9368, 0.5300, 0.6829, 0.0508, 0.0336, 0.6386,\n",
            "        0.7354, 0.7417, 0.6845, 0.8314, 0.9637, 0.9378, 0.7908, 0.5446, 0.8338,\n",
            "        0.0335, 0.8351, 0.3605, 0.6373, 0.5939, 0.3270, 0.3390, 0.8966, 0.6015,\n",
            "        0.3508, 0.9639, 0.5645, 0.8216, 0.8624, 0.5776, 0.6071, 0.0293, 0.9201,\n",
            "        0.7620, 0.2926, 0.0974, 0.7698, 0.0897, 0.9084, 0.7972, 0.8173, 0.6949,\n",
            "        0.9308, 0.3083, 0.0708, 0.0889, 0.9941, 0.5313, 0.9649, 0.4087, 0.7664,\n",
            "        0.9877, 0.8831, 0.0207, 0.4926, 0.1366, 0.5924, 0.8382, 0.8144, 0.4344,\n",
            "        0.4933, 0.8073, 0.9666, 0.2159, 0.3398, 0.6121, 0.5998, 0.1542, 0.1927,\n",
            "        0.6069, 0.2836, 0.5197, 0.2405, 0.6216, 0.0680, 0.1533, 0.0911, 0.2303,\n",
            "        0.4860, 0.5653, 0.2259, 0.8255, 0.1394, 0.7714, 0.3133, 0.2151, 0.2057,\n",
            "        0.5873, 0.5954, 0.3324, 0.0236, 0.2209, 0.3845, 0.9114, 0.0118, 0.0832,\n",
            "        0.3056, 0.2015, 0.8451, 0.0027, 0.6039, 0.5285, 0.2662, 0.8958, 0.6184,\n",
            "        0.4677, 0.2127, 0.8638, 0.8732, 0.0440, 0.5709, 0.3436, 0.8119, 0.0460,\n",
            "        0.3817, 0.8380, 0.5657, 0.5874, 0.6829, 0.8130, 0.8060, 0.9257, 0.8956,\n",
            "        0.0719, 0.7226, 0.8075, 0.0345, 0.7285, 0.3099, 0.8232, 0.9792, 0.7843,\n",
            "        0.7747, 0.7595, 0.1017, 0.3534, 0.1358, 0.1252, 0.9883, 0.0921, 0.2630,\n",
            "        0.8028, 0.4461, 0.2180, 0.7177, 0.0273, 0.9093, 0.9454, 0.9242, 0.4993,\n",
            "        0.4847, 0.5070, 0.9775, 0.6441, 0.7029, 0.9064, 0.2949, 0.2622, 0.9559,\n",
            "        0.5596, 0.3909, 0.8427, 0.2683, 0.8912, 0.4464, 0.8840, 0.3177, 0.4157,\n",
            "        0.2948, 0.9529, 0.6143, 0.6608, 0.4674, 0.6217, 0.8179, 0.0405, 0.4505,\n",
            "        0.6480, 0.9904, 0.5359, 0.6408, 0.0304, 0.0982, 0.3151, 0.7309, 0.5027,\n",
            "        0.6447, 0.2188, 0.2960, 0.8668, 0.9800, 0.7035, 0.5548, 0.6906, 0.4911,\n",
            "        0.5527, 0.1397, 0.6201, 0.3735, 0.9924, 0.1525, 0.6631, 0.7472, 0.2483,\n",
            "        0.8129, 0.5890, 0.4315, 0.6402, 0.8550, 0.3189, 0.1967, 0.6104, 0.1730,\n",
            "        0.7712, 0.1176, 0.4691, 0.9670, 0.2838, 0.1713, 0.7757, 0.2735, 0.0150,\n",
            "        0.6989, 0.5645, 0.3225, 0.5076, 0.3005, 0.9172, 0.0090, 0.0068, 0.2982,\n",
            "        0.1083, 0.3312, 0.7484, 0.5912, 0.6116, 0.3518, 0.5710, 0.4349, 0.6925,\n",
            "        0.2850, 0.6590, 0.4961, 0.2983, 0.1709, 0.7557, 0.3923, 0.3961, 0.4719,\n",
            "        0.5959, 0.0671, 0.7669, 0.5064, 0.1217, 0.9937, 0.8080, 0.6586, 0.6908,\n",
            "        0.6077, 0.5481, 0.1243, 0.3043, 0.5453, 0.3999, 0.6451, 0.5036, 0.9773,\n",
            "        0.1604, 0.9318, 0.1398, 0.9127, 0.7257, 0.2244, 0.3012, 0.0562, 0.2880,\n",
            "        0.5307, 0.0623, 0.6260, 0.3920, 0.8576, 0.5625, 0.6039, 0.5319, 0.0525,\n",
            "        0.5371, 0.5419, 0.7225, 0.7793, 0.3257, 0.4990, 0.2086, 0.9870, 0.3211,\n",
            "        0.0794, 0.1235, 0.4822, 0.0112, 0.9296, 0.9869, 0.6497, 0.6140, 0.2429,\n",
            "        0.4990, 0.7035, 0.3491, 0.1972, 0.9498, 0.8163, 0.5609, 0.5130, 0.4962,\n",
            "        0.0255, 0.2383, 0.8604, 0.9467, 0.0251, 0.0914, 0.5114, 0.0642, 0.9556,\n",
            "        0.5731, 0.3459, 0.2839, 0.7699, 0.7037, 0.4488, 0.5160, 0.6261, 0.5522,\n",
            "        0.4782, 0.3898, 0.3045, 0.5522, 0.9788, 0.8969, 0.1905, 0.6811, 0.1702,\n",
            "        0.8694, 0.5394, 0.3978, 0.1762, 0.3877, 0.6428, 0.9007, 0.6357, 0.0777,\n",
            "        0.9025, 0.5667, 0.3585, 0.0715, 0.9292, 0.2709, 0.8828, 0.5854, 0.9383,\n",
            "        0.3222, 0.7828, 0.5843, 0.2090, 0.2187, 0.2202, 0.8854, 0.0501, 0.5187,\n",
            "        0.5764, 0.4586, 0.2808, 0.1689, 0.0420, 0.9973, 0.3559, 0.2742, 0.9011,\n",
            "        0.6276, 0.4105, 0.4854, 0.2211, 0.2151, 0.3167, 0.4615, 0.6361, 0.8167,\n",
            "        0.3077, 0.4108, 0.7413, 0.3427, 0.6640, 0.4911, 0.7851, 0.9771, 0.9340,\n",
            "        0.3386, 0.4024, 0.2773, 0.3895, 0.5167, 0.8213, 0.6820, 0.2884, 0.3621,\n",
            "        0.8291, 0.9779, 0.6037, 0.0297, 0.3893, 0.8988, 0.1533, 0.6444, 0.1893,\n",
            "        0.6569, 0.7107, 0.0036, 0.1842, 0.5233, 0.4184, 0.7503, 0.4438, 0.7126,\n",
            "        0.1250, 0.6880, 0.9672, 0.4040, 0.2151, 0.7194, 0.2567, 0.9463, 0.8301,\n",
            "        0.6142, 0.5356, 0.0186, 0.3335, 0.4419, 0.0748, 0.5325, 0.6852, 0.7506,\n",
            "        0.5501, 0.2337, 0.8686, 0.3802, 0.6166, 0.0132, 0.4318, 0.8762, 0.7728,\n",
            "        0.5231, 0.7821, 0.3313, 0.0584, 0.3458, 0.3496, 0.3234, 0.7421, 0.9430,\n",
            "        0.2279, 0.9695, 0.3730, 0.3688, 0.0455, 0.4644, 0.5815, 0.8601, 0.8549,\n",
            "        0.5635, 0.3078, 0.2616, 0.7684, 0.2626, 0.5678, 0.2379, 0.2515, 0.5791,\n",
            "        0.9149, 0.9165, 0.6361, 0.0680, 0.5610, 0.3499, 0.1915, 0.2615, 0.2545,\n",
            "        0.4700, 0.1699, 0.1574, 0.4903, 0.9350, 0.1524, 0.7964, 0.0958, 0.6282,\n",
            "        0.2805, 0.6849, 0.0434, 0.2003, 0.0979, 0.1131, 0.2275, 0.6315, 0.0858,\n",
            "        0.3482, 0.1650, 0.1963, 0.1233, 0.3469, 0.7991, 0.4522, 0.4648, 0.6960,\n",
            "        0.3894, 0.7458, 0.0957, 0.2552, 0.8727, 0.5269, 0.1124, 0.0114, 0.3287,\n",
            "        0.1787, 0.2848, 0.8003, 0.2603, 0.3287, 0.8837, 0.8132, 0.3041, 0.7428,\n",
            "        0.0633, 0.7034, 0.4580, 0.8987, 0.3858, 0.4536, 0.2840, 0.1858, 0.6704,\n",
            "        0.7446, 0.3903, 0.0435, 0.7584, 0.4248, 0.4556, 0.6795, 0.7827, 0.1479,\n",
            "        0.1773, 0.2287, 0.4338, 0.7127, 0.0710, 0.1841, 0.8987, 0.5287, 0.8879,\n",
            "        0.6591, 0.8456, 0.5052, 0.6678, 0.8189, 0.8776, 0.6177, 0.7770, 0.0046,\n",
            "        0.3631, 0.0813, 0.7699, 0.1455, 0.8616, 0.7368, 0.9379, 0.7613, 0.1457,\n",
            "        0.1309, 0.9960, 0.1655, 0.2557, 0.5975, 0.8887, 0.7025, 0.3946, 0.6107,\n",
            "        0.7611, 0.5746, 0.7461, 0.6846, 0.9176, 0.3385, 0.5950, 0.3310, 0.7179,\n",
            "        0.5687, 0.3630, 0.0830, 0.3731, 0.1351, 0.8640, 0.9234, 0.9354, 0.0428,\n",
            "        0.8125, 0.1475, 0.1985, 0.3135, 0.2660, 0.3019, 0.1892, 0.1540, 0.7358,\n",
            "        0.5309, 0.4444, 0.2420, 0.6738, 0.6583, 0.7685, 0.9930, 0.4161, 0.7040,\n",
            "        0.7734, 0.6574, 0.6575, 0.0455, 0.8499, 0.4391, 0.4708, 0.8340, 0.0371,\n",
            "        0.0774, 0.1743, 0.5048, 0.4977, 0.6564, 0.4088, 0.5135, 0.0038, 0.9479,\n",
            "        0.4487, 0.2716, 0.0302, 0.9964, 0.4927, 0.5780, 0.9153, 0.0280, 0.3814,\n",
            "        0.5199, 0.7772, 0.2248, 0.1431, 0.8094, 0.3600, 0.5528, 0.8426, 0.1275,\n",
            "        0.8599, 0.0550, 0.0094, 0.3566, 0.5621, 0.7662, 0.3291, 0.8116, 0.1904,\n",
            "        0.1878, 0.6386, 0.6138, 0.3014, 0.8762, 0.4270, 0.9590, 0.1847, 0.2127,\n",
            "        0.3369, 0.7172, 0.9805, 0.9592, 0.7997, 0.5562, 0.5998, 0.2882, 0.3989,\n",
            "        0.4453, 0.5937, 0.3925, 0.7750, 0.2625, 0.4650, 0.8065, 0.8219, 0.7401,\n",
            "        0.2994, 0.7914, 0.5385, 0.0891, 0.8400, 0.9106, 0.6584, 0.8940, 0.6774,\n",
            "        0.2678, 0.9850, 0.8094, 0.3681, 0.9635, 0.5705, 0.6117, 0.2061, 0.6181,\n",
            "        0.9143, 0.2468, 0.2363, 0.0464, 0.9526, 0.2893, 0.7647, 0.3086, 0.3506,\n",
            "        0.4642, 0.1455, 0.5863, 0.4347, 0.0407, 0.7369, 0.4894, 0.9739, 0.9212,\n",
            "        0.3011, 0.8020, 0.1800, 0.7052, 0.1991, 0.6449, 0.8152, 0.1347, 0.3054,\n",
            "        0.5774, 0.9450, 0.8382, 0.1327, 0.8785, 0.9208, 0.6848, 0.1926, 0.1107,\n",
            "        0.0806, 0.1254, 0.6671, 0.4538, 0.9514, 0.9121, 0.3797, 0.7144, 0.1667,\n",
            "        0.3034, 0.8892, 0.0866, 0.8605, 0.1798, 0.5732, 0.9780, 0.5108, 0.7136,\n",
            "        0.3654, 0.3432, 0.7767, 0.6392, 0.6387, 0.5854, 0.1804, 0.4034, 0.3338,\n",
            "        0.5938, 0.5007, 0.0739, 0.2566, 0.5552, 0.0551, 0.9648, 0.7521, 0.1010,\n",
            "        0.4425, 0.0691, 0.6937, 0.0817, 0.7995, 0.4446, 0.0933, 0.7419, 0.9474,\n",
            "        0.8743, 0.7487, 0.3494, 0.0899, 0.7877, 0.2065, 0.7167, 0.3673, 0.1422,\n",
            "        0.9034], grad_fn=<SplitBackward0>), tensor([5.1492e-01, 8.4884e-01, 1.1424e-01, 2.1064e-01, 8.0810e-01, 7.7351e-01,\n",
            "        7.6810e-01, 3.8617e-01, 7.8615e-01, 4.8902e-02, 1.6574e-01, 6.7812e-01,\n",
            "        5.7254e-01, 4.7869e-01, 1.9217e-01, 2.3466e-01, 8.0853e-01, 6.3004e-01,\n",
            "        5.9443e-01, 4.4261e-01, 6.0501e-01, 8.0573e-02, 7.6005e-01, 8.2293e-01,\n",
            "        5.1865e-02, 5.0798e-01, 8.3633e-02, 9.1375e-01, 8.5635e-02, 4.4201e-02,\n",
            "        7.3457e-01, 5.8635e-01, 8.3042e-01, 5.8590e-01, 8.8390e-01, 4.9018e-01,\n",
            "        8.3522e-01, 3.3473e-01, 3.9362e-01, 2.5899e-02, 3.6802e-01, 2.6918e-02,\n",
            "        3.4082e-01, 1.2129e-01, 9.7183e-01, 1.2842e-01, 7.9473e-01, 1.6691e-01,\n",
            "        2.3541e-01, 8.3135e-01, 3.5811e-01, 3.0757e-01, 9.6900e-01, 1.3030e-01,\n",
            "        5.5550e-01, 9.6252e-01, 4.5017e-01, 9.4887e-02, 9.6354e-01, 6.0448e-02,\n",
            "        3.8000e-01, 3.9940e-01, 6.7315e-01, 3.4145e-01, 4.8536e-01, 4.4964e-01,\n",
            "        4.6127e-01, 2.2952e-01, 3.7133e-01, 6.3917e-01, 6.2378e-01, 9.0721e-01,\n",
            "        6.1285e-01, 2.9123e-01, 7.8464e-02, 4.7999e-01, 5.6150e-01, 3.6057e-03,\n",
            "        1.5974e-01, 9.0608e-01, 4.4990e-01, 6.1948e-01, 8.4029e-01, 9.5855e-01,\n",
            "        4.6104e-01, 1.1067e-02, 8.6930e-01, 6.1817e-02, 3.2090e-01, 9.8515e-01,\n",
            "        3.3527e-01, 1.0844e-01, 3.6609e-01, 5.3574e-01, 8.0047e-01, 2.3524e-01,\n",
            "        9.9954e-01, 6.4848e-01, 8.6969e-01, 5.5086e-01, 2.6249e-01, 4.1781e-01,\n",
            "        4.4271e-01, 2.5578e-01, 7.4901e-01, 3.3215e-01, 3.4066e-01, 1.2318e-01,\n",
            "        3.7162e-01, 9.0700e-01, 7.4454e-01, 4.5543e-01, 7.0198e-01, 7.8642e-01,\n",
            "        6.2132e-01, 1.2459e-01, 3.5092e-02, 7.4388e-01, 7.6843e-01, 2.6011e-01,\n",
            "        7.9489e-02, 1.1816e-01, 3.9773e-01, 8.0631e-01, 1.4556e-01, 9.8876e-01,\n",
            "        8.0199e-01, 1.0835e-01, 4.9039e-02, 4.6585e-01, 5.6122e-01, 2.4095e-01,\n",
            "        2.9237e-01, 7.2795e-01, 9.5562e-01, 1.7631e-01, 6.3635e-01, 8.6486e-01,\n",
            "        6.6189e-01, 4.1292e-01, 4.5490e-01, 6.6116e-01, 5.3737e-01, 6.8906e-01,\n",
            "        4.1371e-01, 8.8834e-01, 2.4938e-01, 3.9990e-01, 5.6967e-01, 5.7403e-01,\n",
            "        8.0452e-01, 1.0735e-01, 8.3271e-01, 2.7863e-01, 9.1935e-01, 7.2693e-01,\n",
            "        6.3711e-01, 5.7976e-01, 5.3361e-01, 5.4306e-03, 9.4566e-01, 8.3683e-01,\n",
            "        8.9608e-02, 7.0825e-01, 4.7561e-01, 6.3491e-01, 7.6725e-01, 8.3978e-01,\n",
            "        9.4015e-01, 1.4954e-02, 5.7606e-01, 3.1799e-01, 8.3780e-01, 6.3327e-01,\n",
            "        2.1348e-01, 2.1584e-01, 3.6875e-01, 2.6243e-01, 2.0078e-01, 6.9648e-01,\n",
            "        6.1907e-02, 7.8338e-01, 1.5537e-01, 2.0961e-02, 1.0278e-01, 7.7308e-01,\n",
            "        9.3162e-01, 7.2627e-01, 6.2247e-01, 2.9447e-01, 5.2864e-01, 7.2844e-01,\n",
            "        2.6872e-02, 7.7007e-01, 2.7584e-01, 7.2884e-01, 5.0895e-01, 6.3703e-01,\n",
            "        4.6116e-01, 6.4222e-01, 1.2499e-01, 7.8844e-02, 5.7031e-01, 9.9072e-01,\n",
            "        9.8559e-01, 9.6240e-01, 5.2747e-01, 1.5585e-01, 4.4030e-01, 4.7377e-01,\n",
            "        9.2064e-01, 7.4930e-01, 7.4697e-01, 6.7246e-01, 2.4899e-01, 3.7276e-01,\n",
            "        2.7261e-01, 6.4685e-01, 8.1439e-01, 8.7227e-01, 6.1492e-01, 8.7416e-03,\n",
            "        2.3628e-01, 2.9606e-01, 8.9337e-01, 9.7091e-01, 8.2171e-01, 7.7297e-01,\n",
            "        3.7869e-01, 4.8263e-01, 5.2508e-01, 9.3076e-01, 9.2128e-01, 9.6554e-01,\n",
            "        9.0846e-02, 6.6213e-01, 2.3078e-02, 3.6868e-01, 6.0212e-01, 8.0819e-01,\n",
            "        1.9971e-01, 8.2589e-01, 2.3158e-01, 8.9846e-01, 1.9200e-01, 8.9240e-01,\n",
            "        7.9136e-02, 7.8285e-01, 1.6839e-01, 9.8424e-01, 6.1040e-01, 7.3267e-01,\n",
            "        8.2410e-01, 8.0299e-01, 6.9691e-01, 5.6713e-02, 3.8288e-01, 8.3581e-01,\n",
            "        1.3185e-01, 8.8407e-01, 7.7955e-01, 4.7114e-01, 8.1255e-01, 9.8487e-01,\n",
            "        9.3369e-01, 2.4491e-02, 8.0534e-01, 5.2965e-01, 1.0472e-01, 1.4839e-01,\n",
            "        3.6659e-01, 9.6310e-01, 2.3136e-01, 2.2261e-01, 1.7271e-01, 6.4531e-01,\n",
            "        3.5482e-01, 7.3843e-01, 3.5547e-01, 8.2330e-01, 5.8590e-01, 4.7425e-01,\n",
            "        4.2625e-01, 5.9870e-02, 7.2479e-01, 7.6804e-01, 2.4973e-01, 2.1111e-01,\n",
            "        5.6823e-01, 1.3158e-01, 1.6356e-01, 3.0921e-01, 7.0911e-01, 4.9751e-01,\n",
            "        2.3376e-01, 9.9022e-01, 7.4161e-01, 6.5260e-01, 3.1160e-01, 4.1097e-01,\n",
            "        7.6721e-01, 4.6785e-01, 9.1804e-01, 6.9044e-01, 8.9281e-01, 5.8238e-01,\n",
            "        9.9408e-01, 5.0522e-02, 5.3523e-01, 8.5631e-01, 7.2439e-01, 1.3207e-01,\n",
            "        8.8689e-01, 6.6705e-01, 8.4969e-01, 6.7602e-01, 4.1797e-01, 5.7155e-01,\n",
            "        2.9831e-01, 6.1069e-01, 8.9404e-01, 3.2964e-01, 9.4520e-01, 4.5169e-01,\n",
            "        4.1121e-01, 6.8932e-01, 7.6847e-01, 3.5783e-01, 1.7616e-01, 4.6461e-01,\n",
            "        4.5630e-01, 4.2861e-01, 2.1779e-01, 8.2591e-03, 8.1928e-01, 4.7305e-01,\n",
            "        5.5084e-01, 8.6357e-02, 9.3353e-01, 3.4943e-01, 1.5047e-02, 8.6342e-01,\n",
            "        9.0239e-01, 8.8545e-01, 1.4163e-01, 1.7660e-01, 3.9862e-01, 8.8603e-02,\n",
            "        5.6208e-01, 9.4650e-01, 9.2843e-01, 8.0232e-01, 3.4342e-02, 1.7278e-01,\n",
            "        1.4227e-01, 9.7482e-01, 4.5147e-01, 8.8182e-01, 4.0826e-01, 8.2361e-01,\n",
            "        2.9641e-01, 2.2739e-01, 8.4728e-01, 7.4844e-01, 8.2478e-01, 3.2759e-01,\n",
            "        4.0798e-01, 5.0709e-01, 8.7630e-01, 5.9324e-01, 7.0709e-01, 3.3405e-01,\n",
            "        2.5739e-01, 3.3556e-01, 1.8878e-01, 7.3504e-01, 3.4138e-01, 7.1064e-02,\n",
            "        3.3988e-01, 6.9691e-01, 5.1864e-01, 8.3934e-01, 1.4527e-01, 3.5951e-01,\n",
            "        5.9860e-01, 4.0098e-01, 4.0404e-01, 1.7467e-01, 5.6788e-01, 7.5719e-01,\n",
            "        6.2623e-01, 2.3540e-01, 4.1954e-02, 9.0502e-01, 4.7307e-01, 7.1286e-01,\n",
            "        6.1191e-01, 4.3588e-01, 6.5012e-01, 5.2061e-01, 7.2207e-01, 8.1121e-01,\n",
            "        1.1977e-01, 5.1350e-01, 9.8464e-01, 8.6378e-02, 8.6564e-01, 6.5958e-01,\n",
            "        9.7709e-01, 2.9188e-01, 1.8699e-02, 7.0594e-01, 5.0480e-01, 2.6330e-01,\n",
            "        5.1752e-01, 3.8409e-01, 3.9076e-01, 4.3359e-01, 8.9341e-01, 8.5901e-01,\n",
            "        2.1788e-01, 4.6590e-01, 3.3736e-01, 3.3076e-01, 3.9173e-01, 5.1684e-01,\n",
            "        7.0004e-01, 5.9282e-01, 2.1883e-01, 9.7704e-01, 9.8801e-01, 5.5300e-01,\n",
            "        6.2357e-01, 5.4248e-01, 1.6768e-01, 2.9534e-01, 5.1646e-01, 1.3419e-01,\n",
            "        8.5758e-01, 9.2897e-01, 2.1129e-01, 6.0527e-01, 8.1170e-01, 3.6049e-01,\n",
            "        2.1650e-02, 4.3326e-01, 6.8179e-01, 6.0261e-01, 7.8735e-01, 1.2005e-01,\n",
            "        6.3985e-01, 9.9811e-01, 6.6604e-01, 1.4746e-01, 9.3235e-01, 5.9762e-01,\n",
            "        8.0651e-01, 2.9393e-01, 3.5095e-01, 4.9337e-01, 5.5310e-01, 3.7263e-02,\n",
            "        2.2025e-01, 2.4513e-02, 2.8784e-01, 3.5310e-02, 2.1463e-01, 9.5904e-01,\n",
            "        5.3949e-01, 8.1176e-01, 9.8111e-01, 7.7514e-01, 2.9878e-01, 2.4718e-01,\n",
            "        6.5292e-02, 1.1733e-02, 4.0739e-01, 8.0843e-02, 5.2189e-01, 1.9965e-01,\n",
            "        7.0658e-01, 4.5129e-01, 7.0169e-01, 4.7359e-01, 7.4184e-04, 7.4421e-01,\n",
            "        3.0761e-01, 9.2443e-01, 4.5754e-01, 7.4927e-01, 8.6625e-01, 4.4946e-02,\n",
            "        5.6637e-01, 8.6999e-01, 4.0760e-01, 4.1659e-01, 5.0289e-02, 1.2462e-01,\n",
            "        7.1192e-01, 8.5214e-01, 3.3023e-01, 9.3742e-01, 2.8758e-01, 1.5562e-01,\n",
            "        4.2281e-01, 6.8834e-02, 3.1356e-02, 9.7477e-01, 9.9115e-01, 2.9563e-01,\n",
            "        1.3117e-01, 5.7836e-01, 9.8020e-01, 3.7741e-01, 2.8203e-01, 2.2097e-01,\n",
            "        2.0472e-01, 4.6357e-01, 2.9828e-01, 1.6463e-01, 7.0707e-01, 2.9018e-01,\n",
            "        6.9896e-01, 4.0460e-01, 2.7134e-01, 1.4973e-01, 5.9992e-01, 2.3428e-01,\n",
            "        2.2947e-01, 1.5656e-01, 1.4731e-01, 6.0171e-01, 1.9839e-01, 8.4460e-01,\n",
            "        5.1546e-01, 4.2085e-01, 9.0748e-01, 1.3496e-01, 5.7044e-01, 4.0564e-01,\n",
            "        8.0905e-01, 5.8424e-01, 8.0496e-01, 2.8694e-01, 7.9423e-02, 7.7292e-01,\n",
            "        6.0876e-01, 4.4427e-01, 1.1008e-01, 2.8703e-01, 6.7105e-01, 8.1935e-01,\n",
            "        1.1515e-01, 3.5395e-01, 7.9305e-01, 8.6354e-01, 6.3899e-01, 4.1457e-02,\n",
            "        5.8371e-01, 3.8359e-01, 4.7269e-01, 3.3754e-01, 5.7433e-01, 1.0354e-01,\n",
            "        4.0950e-01, 1.9125e-01, 2.6754e-01, 9.4506e-01, 2.2121e-01, 7.7643e-01,\n",
            "        8.9182e-01, 8.1648e-01, 9.3109e-01, 4.4413e-01, 6.0079e-01, 3.0330e-01,\n",
            "        6.3255e-01, 7.4402e-01, 2.0145e-01, 9.1271e-02, 9.4725e-02, 7.4623e-01,\n",
            "        1.6850e-01, 5.0180e-01, 7.3646e-01, 4.3369e-01, 2.9153e-01, 9.4440e-01,\n",
            "        6.6869e-01, 1.1318e-01, 3.6876e-01, 2.8333e-01, 5.9412e-01, 3.4807e-01,\n",
            "        8.8687e-02, 5.6135e-01, 4.0366e-01, 1.8563e-01, 7.7069e-01, 2.0686e-02,\n",
            "        8.4350e-01, 9.3382e-01, 9.8767e-01, 7.9703e-01, 2.2141e-01, 6.6349e-01,\n",
            "        3.6317e-02, 3.2710e-01, 9.6383e-01, 4.1158e-02, 9.2375e-01, 7.4873e-01,\n",
            "        2.0645e-01, 1.7889e-01, 1.4117e-01, 9.7570e-01, 4.4427e-02, 7.1622e-01,\n",
            "        1.1531e-02, 6.1558e-01, 7.5988e-01, 9.5590e-01, 1.8515e-01, 6.9020e-01,\n",
            "        3.4815e-01, 3.0649e-01, 1.6068e-03, 3.6523e-01, 8.5696e-01, 6.8070e-02,\n",
            "        4.4600e-01, 1.7084e-01, 6.0250e-02, 4.5537e-01, 7.6571e-01, 3.3148e-01,\n",
            "        6.9589e-01, 1.6948e-01, 4.7577e-01, 4.8502e-01, 6.0671e-01, 2.9563e-01,\n",
            "        9.1476e-01, 5.6711e-01, 4.8364e-01, 7.0020e-01, 1.4547e-01, 1.0038e-01,\n",
            "        7.4390e-02, 9.0893e-01, 1.4130e-02, 9.7434e-01, 9.2038e-01, 2.7048e-01,\n",
            "        2.9644e-01, 3.7497e-01, 2.8376e-01, 3.8824e-01, 8.1193e-01, 4.2542e-01,\n",
            "        6.6587e-01, 7.3682e-01, 1.0522e-01, 7.1142e-01, 6.9276e-01, 4.4517e-01,\n",
            "        9.3033e-01, 5.5708e-02, 9.8819e-02, 4.5321e-01, 4.5992e-01, 5.4711e-01,\n",
            "        5.2301e-01, 7.0622e-01, 7.0087e-01, 8.7475e-01, 7.1564e-01, 4.5159e-01,\n",
            "        7.0286e-01, 2.8946e-01, 8.4131e-01, 6.6831e-01, 8.2384e-01, 6.0958e-01,\n",
            "        3.4677e-01, 8.1766e-01, 3.2069e-01, 5.7624e-01, 3.2428e-01, 8.4588e-01,\n",
            "        4.8359e-01, 4.8758e-02, 9.6076e-01, 9.1269e-01, 3.2188e-01, 3.3928e-01,\n",
            "        4.8490e-01, 7.3031e-01, 8.8038e-01, 8.7877e-02, 3.0634e-01, 2.7938e-01,\n",
            "        2.5533e-01, 8.8333e-01, 8.0374e-01, 3.4582e-01, 2.9022e-01, 4.1342e-01,\n",
            "        4.8186e-01, 3.4399e-01, 8.3713e-01, 5.7759e-01, 3.6716e-02, 8.9637e-01,\n",
            "        5.9878e-01, 5.3838e-03, 2.5143e-01, 4.8829e-01, 9.0578e-01, 4.6399e-01,\n",
            "        7.3871e-01, 1.3925e-01, 4.4622e-01, 7.9921e-01, 3.7694e-01, 1.3791e-02,\n",
            "        1.0749e-01, 8.2965e-01, 1.9333e-01, 9.6103e-01, 7.6619e-01, 5.9503e-01,\n",
            "        7.8125e-01, 9.7813e-01, 9.8533e-01, 8.3686e-01, 6.8005e-01, 3.7428e-01,\n",
            "        6.0620e-01, 3.6299e-01, 7.1208e-01, 8.8655e-01, 7.7410e-01, 8.4267e-01,\n",
            "        2.2247e-01, 5.6270e-02, 2.9241e-01, 9.6691e-01, 7.7291e-01, 6.5268e-01,\n",
            "        3.8763e-01, 2.1848e-02, 8.1633e-01, 4.7787e-01, 3.3674e-01, 2.9929e-01,\n",
            "        1.3030e-01, 8.1594e-01, 8.1667e-01, 6.0004e-02, 9.8426e-01, 7.1705e-01,\n",
            "        2.6946e-01, 7.8663e-01, 7.0779e-01, 7.9023e-01, 9.6550e-01, 7.4019e-01,\n",
            "        3.5125e-01, 8.8157e-01, 6.4203e-01, 8.2644e-01, 3.9086e-01, 1.2421e-01,\n",
            "        7.6997e-01, 7.8297e-01, 1.3868e-01, 2.9589e-01, 6.4311e-02, 4.1304e-01,\n",
            "        9.1538e-01, 3.8767e-01, 1.3686e-01, 5.8063e-01, 9.4934e-01, 5.8815e-01,\n",
            "        6.4870e-01, 1.8481e-01, 6.2096e-01, 3.0304e-01, 1.9980e-01, 7.5271e-01,\n",
            "        1.5449e-01, 3.0817e-01, 8.1370e-01, 4.0957e-01, 4.6822e-01, 5.4879e-01,\n",
            "        4.9676e-02, 1.3958e-01, 5.7106e-01, 8.0965e-01, 6.8549e-01, 6.6727e-01,\n",
            "        1.6944e-01, 6.2486e-01, 3.3243e-02, 6.8244e-01, 8.2157e-01, 6.3199e-01,\n",
            "        6.5295e-01, 3.4455e-01, 1.5858e-02, 3.0383e-01, 4.6097e-01, 4.3386e-01,\n",
            "        7.1913e-01, 1.7139e-01, 7.0073e-01, 1.0184e-01, 7.7361e-01, 6.7240e-01,\n",
            "        2.8937e-01, 1.1835e-01, 8.4479e-01, 5.3063e-01, 4.1782e-01, 2.6929e-01,\n",
            "        1.8826e-01, 9.3236e-01, 6.1590e-01, 8.5328e-01, 8.2800e-01, 3.5857e-01,\n",
            "        6.0444e-01, 7.1593e-01, 1.1297e-01, 4.7311e-01, 9.6691e-01, 2.7584e-01,\n",
            "        7.4211e-01, 2.1648e-01, 6.4145e-01, 7.3684e-01, 7.9676e-01, 7.0553e-03,\n",
            "        8.4413e-01, 7.1990e-01, 4.7142e-01, 7.8432e-01, 9.4419e-01, 4.9103e-01,\n",
            "        4.2217e-01, 8.7195e-01, 6.9380e-03, 3.6719e-01, 5.6285e-01, 7.6533e-02,\n",
            "        7.3677e-01, 1.0127e-02, 4.6477e-01, 8.7064e-02, 7.4467e-01, 9.2720e-01,\n",
            "        7.6715e-01, 5.6042e-01, 5.9056e-01, 7.2731e-01, 7.1477e-02, 9.4688e-02,\n",
            "        5.1334e-01, 7.6917e-01, 2.4777e-01, 4.7660e-01, 4.3658e-01, 5.4801e-01,\n",
            "        2.4281e-01, 9.0765e-02, 1.2507e-01, 8.0506e-02, 9.6273e-01, 2.7524e-01,\n",
            "        9.0683e-01, 8.5254e-01, 8.9994e-01, 2.1361e-01, 7.1499e-01, 4.3547e-01,\n",
            "        6.1186e-01, 1.7987e-01, 9.0115e-01, 7.9529e-01, 7.4291e-02, 8.9941e-01,\n",
            "        9.5537e-01, 6.6432e-02, 7.0139e-01, 8.7982e-01, 9.4847e-02, 2.9944e-01,\n",
            "        2.5767e-01, 4.3746e-01, 9.5259e-01, 3.2951e-01, 1.2759e-01, 9.1289e-01,\n",
            "        8.3417e-01, 7.6666e-01, 5.8394e-01, 8.0508e-01, 6.7849e-02, 1.6677e-01,\n",
            "        1.1475e-01, 3.3280e-01, 5.1725e-01, 6.0444e-01, 8.9891e-01, 5.4249e-01,\n",
            "        3.8719e-02, 7.2593e-01, 3.2462e-01, 4.6037e-01, 2.4320e-01, 2.2605e-01,\n",
            "        7.0101e-01, 2.8245e-02, 4.9637e-01, 1.5730e-01, 5.4291e-01, 4.9720e-01,\n",
            "        2.7823e-01, 7.0027e-01, 5.5726e-01, 4.0679e-01, 2.6244e-02, 1.6836e-01,\n",
            "        1.3289e-01, 2.0558e-01, 1.1892e-01, 6.8000e-01, 1.2410e-01, 8.4745e-01,\n",
            "        2.3750e-01, 4.0645e-01, 4.8207e-01, 1.7723e-01, 2.9700e-02, 6.9853e-01,\n",
            "        4.4933e-01, 1.9357e-01, 9.7665e-02, 1.4997e-01, 3.0186e-02, 3.2399e-01,\n",
            "        7.1603e-03, 4.1279e-01, 9.7547e-01, 9.7081e-01, 2.2097e-01, 4.6605e-01,\n",
            "        9.6950e-01, 4.4551e-02, 2.3496e-02, 8.8711e-01, 3.6605e-01, 1.4635e-01,\n",
            "        7.9847e-01, 1.2434e-01, 5.9872e-01, 1.9979e-02, 8.0815e-01, 3.9386e-01,\n",
            "        7.1645e-01, 3.2211e-01, 4.0604e-02, 2.2518e-01, 6.3783e-01, 7.4411e-01,\n",
            "        6.7305e-01, 7.3970e-01, 5.9434e-01, 8.5069e-01, 3.7758e-01, 3.4814e-01,\n",
            "        9.8357e-01, 9.8384e-01, 8.8933e-01, 2.2180e-02, 1.7145e-01, 4.9610e-01,\n",
            "        3.6822e-01, 7.5496e-01, 9.2734e-01, 4.6367e-01, 9.5877e-02, 6.6386e-01,\n",
            "        7.0708e-01, 9.6059e-02, 1.1329e-01, 9.2770e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([8.7857e-01, 7.9474e-01, 9.7612e-01, 1.8320e-01, 9.4263e-01, 5.3522e-01,\n",
            "        4.1973e-01, 7.2247e-01, 4.7819e-01, 4.1602e-02, 3.2468e-01, 8.5204e-01,\n",
            "        6.9680e-01, 8.7889e-01, 5.3623e-01, 5.6412e-01, 2.4877e-01, 3.1046e-01,\n",
            "        3.1839e-01, 9.3795e-01, 6.4554e-01, 1.7100e-01, 6.9942e-01, 7.5618e-01,\n",
            "        3.9756e-01, 3.0223e-01, 7.7119e-01, 9.2850e-02, 3.1533e-01, 2.2248e-01,\n",
            "        2.4733e-01, 9.9486e-01, 8.4692e-01, 3.2845e-01, 2.6557e-01, 8.7835e-01,\n",
            "        5.9054e-01, 7.3625e-01, 3.6828e-01, 1.8890e-01, 4.7379e-01, 5.0751e-01,\n",
            "        8.2023e-01, 4.9000e-01, 3.1667e-01, 4.3427e-01, 8.4750e-01, 7.7762e-01,\n",
            "        2.3678e-01, 5.3722e-01, 1.9747e-01, 6.7583e-01, 5.0414e-01, 9.9631e-01,\n",
            "        3.5451e-01, 8.8474e-01, 4.6347e-01, 8.2017e-01, 6.8185e-01, 6.8610e-01,\n",
            "        3.8335e-01, 4.2987e-01, 4.8411e-01, 7.2242e-01, 3.9526e-01, 8.7336e-01,\n",
            "        6.3866e-02, 7.8273e-01, 6.9059e-01, 6.6643e-01, 3.1441e-01, 8.5302e-01,\n",
            "        6.7330e-01, 7.3241e-01, 1.1482e-01, 3.2924e-01, 1.1273e-01, 4.7866e-01,\n",
            "        7.4557e-01, 3.3819e-03, 2.3047e-01, 6.3034e-01, 8.5735e-01, 1.2726e-01,\n",
            "        2.8164e-01, 2.9551e-01, 5.7515e-01, 5.3737e-01, 4.3079e-01, 4.4557e-01,\n",
            "        2.6097e-02, 5.0066e-01, 1.4334e-01, 4.7027e-01, 3.8321e-01, 1.7514e-01,\n",
            "        6.8522e-01, 2.3901e-01, 5.4896e-03, 8.4766e-01, 6.7535e-01, 1.5146e-01,\n",
            "        7.1115e-01, 3.1825e-01, 7.5872e-01, 8.4662e-01, 9.0489e-03, 1.9998e-01,\n",
            "        3.2918e-01, 7.9864e-01, 5.5044e-01, 9.2887e-01, 6.7403e-01, 6.8638e-01,\n",
            "        3.0268e-02, 8.5796e-01, 7.0464e-01, 9.8611e-01, 3.0095e-01, 4.5657e-01,\n",
            "        7.8108e-01, 1.6339e-01, 9.1155e-01, 7.8885e-01, 4.5853e-01, 4.6464e-01,\n",
            "        2.5175e-02, 9.4942e-01, 7.2010e-01, 3.1398e-01, 5.3293e-01, 7.1321e-01,\n",
            "        3.0350e-01, 3.0638e-02, 6.8942e-01, 4.0219e-01, 9.8876e-01, 6.5465e-01,\n",
            "        4.2939e-01, 1.0715e-01, 3.2515e-01, 9.5770e-01, 3.3128e-01, 6.3978e-01,\n",
            "        3.8990e-01, 7.8474e-01, 6.5554e-01, 2.0959e-01, 3.3381e-01, 5.8318e-01,\n",
            "        9.8039e-01, 1.3267e-01, 3.9531e-01, 5.0575e-02, 9.1642e-01, 3.4071e-01,\n",
            "        3.1343e-01, 3.9792e-01, 8.0448e-01, 5.0719e-01, 4.3787e-01, 7.0920e-01,\n",
            "        2.2199e-01, 2.1037e-01, 1.5751e-01, 2.3766e-01, 3.2107e-01, 6.0522e-02,\n",
            "        4.9995e-03, 4.7860e-03, 5.2231e-01, 3.3573e-02, 3.2814e-01, 3.8810e-01,\n",
            "        9.4385e-01, 9.8624e-01, 1.6500e-01, 2.0145e-01, 4.4998e-01, 7.2665e-01,\n",
            "        9.3831e-01, 2.0684e-01, 2.3934e-01, 3.0613e-01, 9.3504e-01, 5.9484e-01,\n",
            "        5.3058e-01, 1.6678e-01, 6.3120e-01, 8.8080e-01, 3.9773e-01, 6.6304e-01,\n",
            "        1.6317e-01, 4.8173e-01, 5.0215e-01, 4.1007e-01, 7.1442e-01, 6.0239e-01,\n",
            "        6.2200e-01, 9.0336e-01, 1.8927e-01, 4.8261e-01, 5.0905e-01, 6.1074e-01,\n",
            "        5.3831e-01, 7.5425e-01, 8.3575e-01, 8.7794e-01, 5.5701e-01, 9.5638e-01,\n",
            "        9.7955e-01, 1.0867e-01, 6.6518e-01, 9.3084e-01, 2.8151e-01, 5.4276e-01,\n",
            "        5.1063e-01, 7.8684e-02, 1.8174e-01, 3.9710e-01, 5.4881e-01, 3.1464e-01,\n",
            "        9.0760e-01, 9.8952e-01, 3.5099e-01, 6.8760e-01, 5.4703e-01, 1.8180e-01,\n",
            "        1.8973e-01, 7.0103e-01, 6.6731e-01, 9.9703e-01, 3.0120e-01, 1.1461e-01,\n",
            "        3.0984e-01, 1.1422e-01, 7.6006e-01, 9.8146e-01, 2.5080e-01, 6.3077e-01,\n",
            "        9.1612e-03, 3.7338e-01, 6.1418e-01, 2.7623e-01, 9.7205e-01, 3.3440e-01,\n",
            "        4.6192e-02, 2.5504e-01, 9.4907e-01, 1.7818e-01, 9.1346e-01, 6.8620e-02,\n",
            "        1.9537e-01, 4.5560e-01, 5.0927e-01, 6.0828e-01, 4.6434e-01, 1.5572e-01,\n",
            "        3.4986e-01, 9.8431e-01, 9.2047e-01, 8.0791e-02, 4.6768e-03, 1.8606e-01,\n",
            "        4.2905e-01, 7.9110e-01, 2.9192e-01, 7.2312e-01, 1.0406e-02, 8.8368e-02,\n",
            "        4.6058e-01, 8.9199e-01, 9.6471e-01, 6.9290e-01, 5.3978e-01, 1.5030e-01,\n",
            "        7.1609e-01, 2.4677e-01, 4.2342e-01, 9.9021e-01, 7.5070e-01, 2.5251e-03,\n",
            "        4.7938e-01, 3.8213e-01, 8.3737e-01, 3.5947e-01, 5.1398e-01, 2.2466e-01,\n",
            "        8.6332e-01, 6.6116e-01, 1.9377e-01, 8.7065e-02, 5.5253e-01, 6.0771e-01,\n",
            "        9.2967e-01, 6.6432e-01, 2.3191e-02, 5.9089e-01, 3.3774e-01, 7.3553e-01,\n",
            "        8.4899e-01, 9.6168e-01, 8.3658e-01, 9.7232e-01, 2.1298e-01, 3.2381e-01,\n",
            "        7.2621e-01, 7.2610e-01, 1.3162e-01, 8.2693e-01, 5.7225e-01, 4.0811e-01,\n",
            "        7.5346e-01, 3.9202e-01, 9.7065e-01, 6.1833e-01, 1.6818e-01, 2.4042e-01,\n",
            "        4.6905e-01, 8.6637e-02, 7.2992e-01, 1.0452e-01, 7.0303e-01, 3.9071e-01,\n",
            "        4.1020e-01, 8.7418e-01, 9.7045e-01, 6.4439e-03, 9.1528e-01, 6.5636e-01,\n",
            "        9.3129e-01, 5.8863e-01, 7.1922e-01, 7.4731e-01, 7.8196e-01, 5.0841e-01,\n",
            "        8.3004e-01, 4.9811e-01, 5.0861e-01, 6.8411e-01, 1.4257e-02, 5.4886e-01,\n",
            "        9.6342e-01, 8.5097e-01, 1.1007e-01, 2.6085e-01, 2.8069e-01, 6.7840e-02,\n",
            "        3.9226e-01, 2.1837e-01, 7.4713e-01, 2.0290e-02, 4.8232e-01, 7.3278e-01,\n",
            "        7.7233e-02, 7.1929e-01, 2.8544e-01, 9.7402e-01, 2.8830e-02, 4.2814e-01,\n",
            "        4.0127e-01, 1.0031e-01, 4.8467e-01, 4.2571e-03, 2.2302e-01, 3.3230e-01,\n",
            "        8.9704e-01, 1.6704e-01, 8.4925e-01, 5.8999e-01, 7.6709e-01, 5.1677e-03,\n",
            "        4.8633e-01, 4.2571e-01, 5.8402e-01, 2.5713e-01, 4.9162e-01, 7.7202e-01,\n",
            "        7.7199e-01, 9.4186e-02, 7.5785e-01, 5.8984e-01, 8.6958e-01, 5.2187e-01,\n",
            "        1.6787e-01, 1.7383e-02, 3.1198e-01, 4.4589e-01, 7.5557e-01, 7.2629e-01,\n",
            "        7.5178e-01, 8.2699e-01, 3.9646e-02, 3.9882e-01, 5.4007e-01, 9.8846e-01,\n",
            "        5.0236e-01, 8.3706e-01, 8.4518e-01, 8.8219e-01, 9.1534e-01, 7.9194e-02,\n",
            "        8.5867e-02, 6.3934e-01, 8.2448e-01, 5.9046e-01, 6.4284e-01, 8.7934e-01,\n",
            "        3.0312e-01, 9.1012e-01, 5.3886e-02, 7.2420e-01, 4.4462e-03, 2.2182e-01,\n",
            "        6.1101e-01, 9.2714e-01, 7.9407e-01, 7.7060e-02, 5.5091e-01, 3.6535e-01,\n",
            "        6.4601e-01, 4.5103e-01, 1.8151e-01, 4.1454e-01, 6.3992e-01, 1.5702e-01,\n",
            "        4.7414e-01, 2.9242e-01, 3.9562e-01, 3.9734e-01, 8.8098e-01, 7.1759e-01,\n",
            "        4.3489e-01, 2.3431e-01, 2.2727e-04, 6.6449e-01, 6.1145e-01, 3.0125e-01,\n",
            "        8.2825e-01, 5.7566e-02, 8.1571e-02, 6.3464e-01, 5.7045e-01, 8.7897e-01,\n",
            "        1.2326e-01, 2.0775e-01, 6.4661e-01, 6.1235e-01, 1.9156e-01, 7.5783e-01,\n",
            "        5.9740e-01, 8.7873e-01, 2.4656e-01, 4.7081e-01, 4.9693e-01, 7.1940e-01,\n",
            "        5.5062e-01, 5.2626e-01, 7.5145e-01, 5.2117e-01, 2.5094e-01, 7.7290e-02,\n",
            "        5.6336e-01, 2.2510e-01, 5.3191e-01, 9.2612e-01, 9.0501e-02, 8.9465e-01,\n",
            "        7.8509e-01, 1.0799e-02, 7.6705e-01, 6.1239e-02, 9.5688e-01, 8.0029e-01,\n",
            "        8.1485e-01, 1.2263e-01, 9.1111e-01, 4.0962e-01, 4.0499e-01, 9.3226e-01,\n",
            "        4.8032e-01, 4.9434e-01, 2.4861e-01, 3.5844e-01, 7.4798e-01, 3.6312e-01,\n",
            "        1.4757e-02, 3.6421e-01, 8.8767e-01, 6.2277e-01, 3.3789e-02, 1.7666e-01,\n",
            "        6.5649e-02, 2.1550e-01, 2.6518e-01, 4.4464e-01, 1.7830e-01, 4.5469e-01,\n",
            "        1.6661e-01, 6.1093e-01, 9.4820e-01, 1.1217e-01, 3.8057e-01, 2.8210e-01,\n",
            "        4.6726e-01, 1.9085e-01, 3.5085e-02, 3.2509e-01, 7.9483e-01, 6.7128e-01,\n",
            "        4.0476e-01, 5.3462e-01, 1.8397e-01, 8.2371e-01, 4.4007e-01, 2.8238e-01,\n",
            "        2.1934e-01, 7.9098e-01, 9.0252e-01, 7.3728e-01, 5.1407e-01, 4.2253e-01,\n",
            "        4.7939e-01, 4.6135e-01, 9.0941e-01, 1.0707e-01, 4.8103e-01, 4.8232e-02,\n",
            "        8.6490e-01, 2.4446e-01, 6.3747e-01, 8.3539e-01, 2.6808e-01, 9.1079e-01,\n",
            "        7.9350e-01, 6.9720e-01, 8.0706e-01, 1.5811e-01, 3.7949e-01, 1.4541e-01,\n",
            "        6.2376e-01, 3.5854e-01, 1.8180e-02, 4.0398e-01, 5.7625e-01, 1.7013e-01,\n",
            "        6.4954e-01, 9.9249e-01, 4.8919e-02, 8.5409e-01, 1.4121e-01, 2.9405e-01,\n",
            "        4.0437e-01, 9.7575e-01, 6.7691e-01, 4.4963e-01, 7.5178e-01, 9.6072e-01,\n",
            "        9.5297e-01, 1.1613e-01, 3.1580e-01, 6.7732e-01, 8.6335e-01, 1.5098e-01,\n",
            "        5.7377e-01, 8.2431e-01, 5.4937e-01, 6.0709e-01, 4.1855e-01, 1.2843e-01,\n",
            "        4.4127e-01, 7.5517e-02, 2.5581e-01, 1.6962e-01, 2.2761e-01, 7.8267e-01,\n",
            "        6.3312e-01, 7.8000e-01, 1.2401e-01, 6.9280e-01, 7.4811e-01, 7.3640e-01,\n",
            "        4.1049e-01, 1.5188e-01, 3.7760e-01, 8.1399e-02, 5.1055e-01, 4.1007e-01,\n",
            "        2.5575e-01, 6.4191e-01, 6.7457e-01, 2.5612e-01, 6.5026e-01, 1.2137e-01,\n",
            "        3.7633e-01, 5.6684e-01, 4.9271e-01, 5.1807e-01, 1.3499e-01, 4.4162e-01,\n",
            "        5.1575e-02, 6.5852e-01, 3.0471e-01, 1.0683e-01, 7.2516e-01, 9.7358e-01,\n",
            "        2.5916e-01, 5.0122e-02, 5.4584e-01, 2.6473e-01, 7.1294e-01, 6.2030e-01,\n",
            "        4.1634e-01, 9.2958e-01, 9.6423e-01, 8.1718e-01, 7.3855e-01, 8.7481e-02,\n",
            "        7.4166e-01, 6.9968e-01, 4.1585e-01, 7.4997e-01, 1.1788e-01, 3.5089e-01,\n",
            "        5.7870e-01, 4.7351e-01, 6.7127e-01, 5.7414e-01, 6.3539e-01, 5.1065e-01,\n",
            "        1.4228e-01, 6.8368e-01, 3.7152e-01, 7.7306e-01, 4.7017e-01, 6.8102e-01,\n",
            "        4.0596e-01, 4.9680e-02, 7.9123e-01, 1.7492e-01, 8.3534e-01, 3.0972e-01,\n",
            "        3.8525e-01, 1.8882e-01, 3.6830e-01, 1.6709e-03, 2.6153e-01, 5.3975e-01,\n",
            "        2.6520e-01, 8.2421e-01, 4.2112e-01, 7.0231e-01, 2.1170e-01, 6.6878e-01,\n",
            "        1.4847e-01, 2.6954e-01, 4.4237e-01, 2.2590e-01, 1.0119e-01, 5.8220e-01,\n",
            "        3.0734e-02, 6.1353e-01, 6.5329e-01, 8.0123e-01, 1.0731e-01, 5.1637e-01,\n",
            "        5.1262e-01, 2.4227e-01, 4.9870e-01, 5.5239e-01, 3.2290e-01, 1.9163e-01,\n",
            "        9.8688e-01, 7.1132e-01, 4.0007e-01, 3.6668e-01, 5.4479e-01, 5.0036e-01,\n",
            "        7.2216e-02, 2.4963e-01, 7.5717e-01, 3.0775e-01, 6.5184e-01, 2.8324e-01,\n",
            "        5.6787e-01, 1.7203e-02, 8.4129e-01, 2.8143e-01, 9.7539e-01, 9.0487e-01,\n",
            "        3.3008e-02, 7.8059e-01, 7.5634e-01, 4.1770e-01, 4.9072e-01, 9.3293e-01,\n",
            "        2.8991e-01, 9.0404e-01, 8.7094e-01, 3.2159e-01, 2.8368e-01, 8.9904e-01,\n",
            "        4.5787e-01, 8.1527e-01, 6.5623e-01, 4.7447e-01, 2.6758e-01, 6.3015e-01,\n",
            "        4.4899e-02, 4.5529e-02, 6.2226e-01, 3.0166e-01, 7.2820e-01, 4.8625e-01,\n",
            "        7.2366e-01, 8.1369e-01, 1.6487e-01, 5.6794e-01, 6.8502e-01, 2.9647e-01,\n",
            "        3.4357e-03, 7.8197e-01, 4.2285e-01, 5.8754e-01, 2.8549e-01, 9.4612e-01,\n",
            "        2.5633e-02, 6.8232e-01, 7.2976e-01, 2.4503e-01, 5.8124e-01, 8.7503e-01,\n",
            "        6.6821e-01, 1.9787e-01, 7.0482e-01, 9.9573e-01, 7.7339e-01, 5.7519e-01,\n",
            "        5.6782e-01, 4.8648e-01, 4.6907e-01, 8.4838e-01, 3.1953e-01, 2.2336e-02,\n",
            "        9.7840e-01, 9.3438e-02, 1.0435e-01, 8.3338e-01, 8.8589e-01, 7.5685e-01,\n",
            "        1.8486e-01, 9.3898e-02, 5.9170e-01, 1.0804e-01, 9.7536e-01, 1.8745e-01,\n",
            "        4.4015e-01, 8.1677e-01, 7.4363e-01, 1.9304e-01, 5.5419e-01, 9.0777e-01,\n",
            "        6.8701e-01, 2.4342e-01, 8.8998e-01, 7.6364e-01, 2.2892e-01, 1.9801e-01,\n",
            "        6.3271e-01, 8.2213e-03, 7.4766e-02, 1.8905e-01, 7.3817e-02, 5.2599e-01,\n",
            "        7.8748e-01, 1.1510e-01, 8.5951e-01, 6.9119e-01, 9.1033e-01, 9.3001e-02,\n",
            "        7.3906e-01, 3.0677e-01, 6.3480e-01, 3.3605e-01, 6.0383e-01, 6.8544e-01,\n",
            "        1.1524e-01, 7.2641e-01, 9.1297e-01, 2.3189e-01, 1.1092e-01, 7.6314e-01,\n",
            "        5.8231e-01, 6.4692e-01, 6.3597e-01, 8.3615e-02, 7.1792e-01, 8.5809e-01,\n",
            "        2.5501e-01, 6.6405e-01, 5.6343e-02, 1.5427e-01, 2.2958e-02, 3.9870e-01,\n",
            "        6.4699e-01, 7.1382e-01, 4.8141e-01, 3.5716e-01, 7.1968e-01, 3.0454e-01,\n",
            "        9.2822e-01, 4.7421e-01, 6.7274e-01, 2.0278e-01, 9.4054e-02, 1.1106e-01,\n",
            "        8.7231e-01, 8.9284e-01, 4.6172e-02, 9.5692e-01, 3.7693e-01, 5.3316e-01,\n",
            "        1.3050e-01, 9.5961e-01, 4.3725e-01, 7.6209e-01, 1.8501e-01, 4.6166e-01,\n",
            "        3.6979e-01, 1.6819e-01, 5.5609e-01, 7.7543e-02, 8.0514e-01, 5.0439e-01,\n",
            "        6.1024e-01, 1.8279e-01, 2.1517e-01, 6.8322e-01, 1.4063e-02, 1.4216e-01,\n",
            "        1.1570e-01, 6.5372e-01, 7.8998e-01, 7.7766e-02, 5.2832e-01, 6.5927e-01,\n",
            "        5.3321e-01, 5.7150e-01, 1.0674e-01, 7.3955e-01, 8.6179e-01, 7.8185e-01,\n",
            "        4.9384e-01, 1.4233e-01, 5.3381e-01, 1.5391e-01, 7.3989e-01, 3.4605e-01,\n",
            "        1.2051e-01, 1.0456e-01, 7.7414e-01, 8.6812e-01, 6.4527e-01, 4.9995e-01,\n",
            "        7.9709e-01, 7.1236e-01, 5.9383e-01, 4.3772e-01, 6.8342e-01, 1.2212e-01,\n",
            "        3.8925e-01, 8.0331e-01, 1.2036e-01, 3.1017e-02, 1.0254e-01, 4.6921e-01,\n",
            "        1.1594e-01, 7.4442e-01, 1.9598e-02, 5.5719e-01, 9.4973e-02, 9.4796e-01,\n",
            "        7.8594e-01, 6.0333e-01, 8.0061e-01, 7.7490e-01, 9.0525e-01, 9.1896e-01,\n",
            "        3.0154e-01, 5.2602e-01, 4.0593e-01, 9.2071e-01, 9.2619e-02, 5.8157e-01,\n",
            "        5.6935e-01, 3.2777e-01, 5.4072e-01, 4.1246e-01, 8.5291e-01, 2.1006e-01,\n",
            "        6.3108e-01, 2.4902e-01, 1.6002e-01, 6.6160e-01, 5.9020e-01, 4.6066e-01,\n",
            "        2.4480e-01, 4.5922e-01, 5.1431e-01, 7.9433e-01, 2.7379e-01, 8.6151e-01,\n",
            "        1.9919e-01, 4.9743e-01, 2.5509e-01, 8.8822e-01, 6.4430e-01, 6.0807e-01,\n",
            "        5.6265e-01, 2.6012e-01, 1.0820e-01, 5.5464e-01, 3.0029e-01, 5.7098e-01,\n",
            "        2.6409e-01, 1.9176e-01, 1.8882e-01, 8.6353e-01, 2.9429e-01, 6.7035e-01,\n",
            "        4.2414e-01, 7.1574e-02, 3.1680e-01, 4.6564e-01, 4.7337e-01, 7.5890e-01,\n",
            "        9.1658e-02, 4.0242e-01, 4.1214e-02, 9.9096e-01, 6.1750e-01, 9.8503e-01,\n",
            "        7.4382e-01, 8.7052e-01, 7.4093e-01, 4.9352e-01, 3.8197e-01, 6.9479e-01,\n",
            "        2.3115e-01, 9.9420e-01, 9.9932e-01, 3.7017e-01, 4.5811e-01, 2.7132e-01,\n",
            "        2.4298e-02, 3.0764e-01, 1.2431e-01, 9.0919e-01, 9.7180e-01, 3.7511e-01,\n",
            "        4.3843e-01, 4.9991e-02, 8.6834e-01, 6.1570e-01, 7.6902e-01, 1.6026e-01,\n",
            "        3.9996e-01, 7.3691e-01, 5.2233e-01, 7.4271e-02, 1.4265e-01, 2.5424e-01,\n",
            "        3.9800e-01, 3.0468e-01, 7.7463e-01, 5.6142e-01, 2.1752e-01, 4.1520e-01,\n",
            "        8.9204e-01, 1.0101e-01, 6.8302e-02, 5.4471e-01, 2.2198e-01, 8.6052e-01,\n",
            "        3.6555e-01, 5.7856e-01, 8.7948e-01, 1.2803e-01, 8.2934e-01, 7.4326e-01,\n",
            "        8.3068e-01, 8.1819e-01, 4.6718e-01, 5.8637e-01, 7.4606e-01, 7.1672e-01,\n",
            "        7.1726e-01, 9.7169e-01, 8.7147e-01, 9.2270e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([1.1925e-01, 6.8681e-01, 1.8896e-01, 9.0223e-01, 9.7583e-01, 1.7095e-01,\n",
            "        1.0958e-01, 5.7784e-01, 8.1851e-01, 9.3215e-01, 5.8085e-01, 9.2157e-01,\n",
            "        8.8487e-01, 5.4741e-01, 9.6615e-01, 1.5400e-01, 9.4807e-01, 5.2553e-01,\n",
            "        9.4284e-01, 6.3678e-02, 4.8728e-01, 5.7162e-02, 6.1477e-01, 6.9534e-01,\n",
            "        6.0523e-01, 5.9204e-01, 9.0885e-01, 1.1365e-01, 8.2741e-01, 1.8909e-02,\n",
            "        9.4685e-01, 4.1118e-01, 1.3994e-01, 5.2834e-01, 4.8465e-01, 3.4684e-01,\n",
            "        8.0877e-01, 5.5339e-01, 6.4903e-02, 4.6297e-01, 6.5932e-02, 6.7318e-03,\n",
            "        3.9606e-01, 2.9881e-01, 9.8323e-01, 1.3269e-02, 4.4359e-01, 1.9255e-01,\n",
            "        4.3422e-01, 6.5250e-02, 6.6293e-02, 8.2652e-01, 4.6797e-01, 5.8541e-01,\n",
            "        7.5677e-01, 6.5066e-01, 6.1785e-01, 6.9859e-01, 9.6593e-01, 5.7103e-01,\n",
            "        4.0399e-01, 3.0661e-01, 2.4165e-02, 2.7610e-01, 3.0720e-01, 6.2963e-01,\n",
            "        5.0256e-01, 8.4573e-01, 1.8236e-01, 8.7137e-01, 8.4953e-01, 2.4934e-01,\n",
            "        9.4065e-01, 6.9702e-04, 9.1921e-01, 9.6792e-01, 4.0980e-01, 7.2710e-01,\n",
            "        2.2859e-01, 9.2764e-01, 8.0078e-01, 9.1417e-01, 9.0694e-01, 3.2586e-02,\n",
            "        3.2118e-01, 8.0555e-01, 6.5254e-01, 5.0263e-01, 3.7303e-01, 8.1916e-01,\n",
            "        4.4904e-01, 9.0172e-01, 5.8979e-01, 6.8597e-01, 8.3288e-01, 9.9284e-01,\n",
            "        6.1468e-01, 1.9394e-02, 5.2643e-01, 4.8780e-02, 6.9343e-03, 7.6545e-01,\n",
            "        9.0118e-01, 4.8951e-01, 2.9597e-01, 1.2349e-01, 1.2387e-01, 9.3450e-01,\n",
            "        5.8412e-01, 3.5313e-01, 8.5505e-01, 8.9514e-01, 9.0664e-01, 2.1161e-01,\n",
            "        6.5511e-01, 8.7811e-01, 1.5420e-01, 9.3402e-01, 8.6853e-01, 5.8425e-01,\n",
            "        1.0038e-01, 2.1880e-01, 4.8545e-01, 8.3632e-02, 2.8560e-01, 3.7272e-01,\n",
            "        1.3674e-01, 1.3128e-01, 5.2752e-01, 4.3991e-01, 9.2058e-01, 5.5954e-01,\n",
            "        6.3782e-01, 7.3025e-01, 5.6488e-02, 7.9994e-01, 6.1390e-01, 7.2040e-01,\n",
            "        6.0051e-01, 7.8028e-01, 6.5705e-01, 5.2184e-03, 4.0862e-01, 6.6776e-01,\n",
            "        4.0223e-01, 4.9933e-01, 1.7980e-01, 9.6062e-01, 2.7479e-02, 7.5410e-01,\n",
            "        3.9133e-01, 9.2098e-01, 6.5237e-01, 2.5947e-01, 1.3129e-01, 1.8384e-01,\n",
            "        8.8158e-01, 7.9395e-01, 7.2625e-01, 7.9490e-01, 1.7602e-01, 1.0437e-03,\n",
            "        8.1863e-02, 6.9451e-01, 2.0380e-02, 5.6403e-01, 7.3486e-01, 4.0871e-01,\n",
            "        9.9485e-01, 4.4218e-01, 8.0118e-01, 1.7986e-02, 9.9513e-01, 3.5993e-01,\n",
            "        9.3833e-01, 5.1437e-01, 8.3389e-01, 2.4505e-01, 1.6120e-01, 3.7444e-01,\n",
            "        9.5761e-01, 9.1008e-01, 1.6121e-01, 8.4443e-01, 7.4591e-01, 2.8884e-01,\n",
            "        9.7472e-02, 5.7145e-01, 7.8377e-01, 4.5124e-01, 8.4173e-01, 7.2731e-01,\n",
            "        3.9459e-01, 2.4259e-01, 1.2594e-01, 5.4648e-01, 8.9316e-01, 6.0983e-01,\n",
            "        7.4709e-01, 6.1249e-01, 2.0651e-01, 7.8935e-01, 7.8999e-01, 7.5532e-01,\n",
            "        1.4652e-01, 6.9762e-01, 5.8302e-01, 7.7194e-01, 2.2310e-01, 6.8277e-01,\n",
            "        4.1198e-02, 3.2958e-01, 9.1608e-01, 8.7570e-01, 1.0796e-01, 6.7635e-01,\n",
            "        9.0567e-01, 2.8910e-01, 8.4095e-01, 6.1633e-01, 1.0781e-01, 7.6010e-01,\n",
            "        2.4841e-01, 7.9460e-01, 3.9353e-01, 8.6688e-01, 8.9061e-01, 9.2871e-01,\n",
            "        7.2349e-01, 7.6844e-01, 2.8565e-01, 7.8326e-01, 3.9903e-01, 1.1011e-01,\n",
            "        9.6896e-01, 6.1180e-01, 2.0208e-01, 2.0050e-01, 4.4523e-01, 6.5868e-01,\n",
            "        8.8600e-01, 8.8021e-01, 2.2282e-01, 6.9442e-01, 4.2137e-01, 3.6272e-02,\n",
            "        6.5045e-01, 5.7895e-01, 3.8392e-01, 4.5968e-01, 2.3621e-01, 7.4225e-01,\n",
            "        7.8902e-01, 8.6813e-01, 3.5311e-01, 5.9129e-01, 7.5315e-01, 1.6292e-01,\n",
            "        7.3864e-01, 1.3792e-01, 2.9702e-01, 3.3644e-01, 1.2444e-01, 9.8839e-01,\n",
            "        6.8796e-01, 2.3665e-01, 9.9980e-01, 5.2426e-01, 6.9468e-01, 5.7075e-01,\n",
            "        9.8641e-01, 8.0220e-01, 3.5424e-01, 2.6696e-01, 3.3770e-01, 7.0329e-01,\n",
            "        9.8652e-01, 8.3519e-01, 2.7952e-01, 8.1193e-02, 7.5737e-01, 8.3554e-02,\n",
            "        6.7027e-01, 5.3303e-02, 3.0402e-01, 8.4846e-01, 1.4826e-01, 7.7239e-01,\n",
            "        9.5616e-01, 5.2597e-02, 1.2683e-01, 9.5941e-01, 9.3833e-01, 8.7532e-01,\n",
            "        3.9219e-01, 5.5598e-01, 6.4591e-01, 3.3385e-01, 2.0443e-01, 7.5161e-01,\n",
            "        5.2045e-01, 9.6034e-01, 1.0772e-01, 3.7916e-01, 1.7396e-01, 5.4150e-01,\n",
            "        8.1412e-01, 2.9951e-01, 3.5831e-01, 2.1106e-01, 3.1736e-01, 5.8967e-01,\n",
            "        7.2637e-02, 5.1965e-01, 8.8772e-01, 8.6895e-01, 6.0386e-01, 5.7680e-01,\n",
            "        9.1770e-01, 3.1120e-01, 7.3565e-01, 8.5145e-01, 6.0676e-01, 6.0006e-01,\n",
            "        9.3657e-01, 2.7558e-01, 2.0199e-01, 3.5472e-01, 4.0569e-01, 7.6096e-02,\n",
            "        7.9513e-01, 5.8787e-01, 5.2614e-01, 3.9108e-01, 1.9344e-01, 1.7072e-01,\n",
            "        6.5502e-01, 7.5401e-01, 5.8017e-01, 4.1415e-01, 7.8311e-01, 6.7046e-01,\n",
            "        5.2993e-01, 8.4102e-01, 8.7522e-01, 5.5521e-01, 9.1750e-01, 7.1774e-02,\n",
            "        9.1835e-01, 9.6361e-02, 4.1818e-01, 6.0234e-01, 2.4034e-01, 6.3767e-01,\n",
            "        4.0408e-01, 9.4223e-01, 5.0668e-01, 9.5378e-01, 7.7339e-02, 2.7346e-01,\n",
            "        2.0934e-01, 8.2412e-01, 2.3808e-01, 3.0424e-01, 9.3554e-01, 9.0024e-01,\n",
            "        1.1792e-01, 2.0795e-01, 1.8864e-01, 8.2734e-01, 2.1517e-01, 7.5353e-01,\n",
            "        4.9343e-01, 1.8396e-01, 6.1784e-01, 7.6061e-01, 7.3029e-01, 6.4934e-01,\n",
            "        6.1876e-01, 6.3316e-01, 7.7103e-02, 6.9007e-01, 2.4162e-01, 9.5115e-01,\n",
            "        8.1860e-01, 7.2462e-01, 2.6978e-01, 9.1137e-01, 2.3512e-01, 4.8291e-01,\n",
            "        6.1982e-01, 7.3305e-01, 3.8818e-01, 5.2087e-01, 5.2306e-01, 7.9672e-01,\n",
            "        6.0922e-02, 5.5735e-01, 9.6518e-01, 5.3939e-01, 5.8937e-01, 4.3690e-01,\n",
            "        5.2778e-01, 2.7672e-01, 9.7346e-01, 9.9430e-01, 3.7304e-02, 3.6368e-01,\n",
            "        9.5755e-01, 1.0146e-01, 5.3538e-01, 6.9004e-01, 7.8063e-01, 2.1273e-01,\n",
            "        1.3520e-02, 4.1354e-01, 3.7984e-01, 2.0041e-01, 4.1566e-01, 1.4509e-02,\n",
            "        6.3955e-01, 5.3914e-01, 2.8916e-01, 8.2693e-01, 3.4369e-01, 9.5369e-01,\n",
            "        5.8245e-01, 8.8103e-01, 6.8011e-01, 1.3502e-01, 5.3851e-01, 5.5144e-01,\n",
            "        7.4597e-01, 8.3900e-01, 3.0033e-01, 8.9376e-01, 2.7206e-01, 3.7206e-01,\n",
            "        8.2229e-01, 9.6141e-01, 4.6912e-01, 7.2982e-01, 8.4972e-01, 6.9303e-01,\n",
            "        9.1728e-02, 1.4909e-01, 1.9997e-01, 2.0527e-01, 5.8534e-01, 1.7515e-01,\n",
            "        5.7200e-01, 8.7716e-01, 4.4384e-01, 9.7623e-01, 1.8470e-01, 9.4769e-01,\n",
            "        8.7301e-01, 7.5933e-01, 6.4121e-01, 7.0910e-01, 6.9678e-01, 3.7956e-01,\n",
            "        4.5401e-01, 2.8274e-02, 9.2724e-01, 4.8953e-01, 9.8034e-01, 1.9594e-02,\n",
            "        5.3303e-01, 9.3369e-01, 9.0367e-01, 6.9485e-01, 4.9400e-01, 8.5197e-01,\n",
            "        3.4073e-01, 8.9980e-01, 5.3735e-01, 5.2248e-01, 5.9063e-01, 7.6298e-01,\n",
            "        1.8535e-01, 4.1192e-02, 1.3466e-01, 1.9834e-01, 1.2295e-02, 3.2451e-01,\n",
            "        9.9254e-01, 5.7148e-01, 7.9429e-01, 6.2446e-01, 7.3980e-01, 4.4744e-03,\n",
            "        6.3134e-01, 1.5101e-01, 6.6043e-01, 5.4734e-03, 2.9374e-01, 7.6549e-01,\n",
            "        5.5139e-01, 5.0931e-01, 6.3489e-01, 1.5208e-01, 5.8022e-01, 6.3172e-01,\n",
            "        8.1212e-01, 1.8729e-01, 8.8174e-01, 1.8601e-01, 9.5568e-01, 9.9634e-01,\n",
            "        3.2908e-01, 6.5604e-01, 5.7763e-04, 9.3986e-01, 7.0636e-01, 4.5918e-01,\n",
            "        9.4509e-01, 1.0202e-01, 1.7938e-01, 6.6745e-01, 6.7450e-01, 1.4565e-01,\n",
            "        8.8685e-02, 4.9099e-01, 4.4045e-01, 2.3271e-01, 3.4804e-01, 4.5541e-01,\n",
            "        5.8291e-01, 7.7402e-01, 8.7617e-01, 5.2690e-02, 5.3365e-01, 8.8113e-01,\n",
            "        1.6955e-01, 9.5838e-01, 9.6479e-01, 1.4016e-01, 4.5336e-01, 6.0147e-01,\n",
            "        7.1805e-01, 3.0573e-01, 5.6336e-01, 2.3818e-01, 8.7288e-01, 6.0793e-01,\n",
            "        6.5728e-02, 9.1579e-01, 3.5562e-01, 9.3269e-01, 1.4453e-01, 8.7357e-01,\n",
            "        9.2709e-01, 1.9138e-01, 5.6999e-01, 7.7546e-01, 5.6955e-02, 6.1367e-01,\n",
            "        7.9544e-01, 3.9984e-01, 8.5231e-01, 6.0307e-01, 7.4930e-02, 4.5347e-01,\n",
            "        6.5893e-01, 6.4427e-01, 5.0140e-01, 7.3920e-01, 3.0192e-02, 8.9090e-01,\n",
            "        3.1052e-01, 8.3767e-01, 4.3607e-01, 8.0450e-01, 8.2978e-01, 1.6165e-01,\n",
            "        1.2686e-01, 3.5176e-01, 2.3835e-01, 2.0728e-01, 1.0236e-01, 7.1600e-01,\n",
            "        2.3479e-01, 2.3920e-01, 6.0921e-01, 2.9117e-01, 7.9017e-02, 5.0950e-01,\n",
            "        2.2861e-01, 8.4653e-01, 5.3512e-01, 7.4152e-02, 3.8896e-01, 4.0569e-01,\n",
            "        4.4608e-02, 6.1224e-01, 1.2496e-01, 5.7716e-01, 1.4439e-01, 2.4895e-01,\n",
            "        3.8040e-01, 6.5456e-01, 4.4506e-01, 4.3447e-01, 1.8933e-01, 9.6259e-01,\n",
            "        4.4446e-01, 5.6186e-01, 1.2434e-01, 6.6425e-01, 9.0665e-02, 1.3238e-01,\n",
            "        3.1015e-01, 6.4914e-02, 6.3138e-01, 2.0862e-01, 1.9568e-01, 2.2151e-01,\n",
            "        8.9434e-01, 3.9071e-01, 6.5265e-01, 1.0141e-01, 6.2372e-01, 8.4331e-01,\n",
            "        5.8495e-01, 8.5679e-01, 9.2918e-01, 5.8881e-01, 5.4489e-01, 3.5259e-01,\n",
            "        8.0748e-01, 1.3095e-01, 1.7002e-01, 3.6584e-02, 8.0287e-01, 7.2331e-01,\n",
            "        8.4856e-01, 6.4926e-01, 2.8626e-01, 9.9030e-01, 7.5842e-01, 2.5782e-01,\n",
            "        4.0100e-01, 3.4254e-03, 7.6246e-02, 5.2099e-01, 2.0325e-01, 2.5837e-01,\n",
            "        3.1358e-01, 8.1316e-01, 5.6005e-01, 8.0580e-02, 9.4227e-01, 2.1077e-01,\n",
            "        3.0431e-01, 8.9631e-01, 4.9593e-01, 9.3986e-01, 4.2221e-01, 6.8734e-01,\n",
            "        2.4580e-01, 6.5894e-01, 6.8328e-01, 4.9136e-01, 2.6341e-01, 8.1862e-01,\n",
            "        6.6855e-01, 1.5248e-02, 5.5949e-02, 8.4529e-01, 1.2275e-01, 9.2191e-01,\n",
            "        5.5051e-01, 2.9884e-01, 5.7500e-01, 1.9196e-01, 2.5172e-01, 2.1282e-01,\n",
            "        6.0387e-02, 3.6086e-01, 6.7363e-01, 1.1663e-01, 1.3727e-01, 8.4187e-01,\n",
            "        1.9953e-02, 8.5983e-01, 8.1059e-01, 4.7845e-02, 3.1588e-02, 9.8605e-01,\n",
            "        1.2583e-01, 6.4187e-01, 7.8543e-01, 6.9506e-01, 7.0566e-01, 5.0966e-01,\n",
            "        7.3308e-01, 9.4173e-01, 2.6306e-01, 9.4751e-01, 7.5877e-01, 2.3017e-01,\n",
            "        9.2657e-01, 6.9036e-01, 3.3959e-01, 3.1983e-01, 2.8115e-01, 5.3133e-01,\n",
            "        3.1195e-01, 1.2624e-01, 6.3132e-01, 7.8918e-01, 8.1506e-01, 6.6230e-01,\n",
            "        2.0697e-01, 9.7308e-01, 1.4108e-01, 9.5680e-01, 8.3861e-01, 9.6566e-01,\n",
            "        5.7700e-02, 2.8034e-01, 1.3058e-01, 2.4857e-01, 5.2371e-01, 5.6305e-01,\n",
            "        3.4706e-01, 6.8155e-02, 6.1158e-01, 1.3950e-01, 7.9073e-01, 1.6727e-01,\n",
            "        9.6579e-01, 7.3286e-01, 4.9363e-01, 6.2610e-01, 3.3610e-01, 1.1899e-01,\n",
            "        5.6027e-01, 5.8377e-02, 5.6852e-01, 6.8055e-01, 4.1776e-01, 6.7434e-01,\n",
            "        6.7805e-01, 8.0593e-01, 1.4778e-01, 7.3822e-02, 3.0322e-02, 4.0261e-01,\n",
            "        8.9051e-01, 4.4650e-01, 7.4494e-01, 5.0114e-01, 9.5862e-04, 3.2596e-01,\n",
            "        3.7849e-01, 8.1080e-01, 9.1454e-01, 3.8750e-01, 5.9483e-01, 5.9981e-01,\n",
            "        3.2280e-01, 4.5521e-01, 2.9650e-01, 8.8438e-01, 7.2750e-01, 2.1835e-01,\n",
            "        9.3284e-01, 8.5492e-01, 9.3103e-01, 1.7432e-02, 1.3859e-01, 6.0925e-01,\n",
            "        6.2047e-01, 4.0891e-01, 5.0114e-01, 9.4869e-01, 9.3720e-01, 6.1461e-01,\n",
            "        8.0502e-01, 9.0312e-01, 8.6923e-01, 5.3865e-01, 8.3714e-01, 9.0139e-01,\n",
            "        2.0099e-01, 9.6025e-01, 1.5879e-01, 5.3623e-01, 5.8255e-01, 4.6622e-02,\n",
            "        3.5604e-01, 7.0724e-01, 9.6657e-01, 4.9053e-01, 6.5507e-01, 2.9030e-01,\n",
            "        9.9824e-01, 8.2423e-01, 9.2646e-02, 1.5066e-01, 6.1755e-01, 3.5430e-02,\n",
            "        4.1960e-01, 3.0088e-01, 9.2227e-01, 3.8830e-01, 6.6802e-01, 3.8622e-01,\n",
            "        7.4251e-01, 5.9029e-01, 9.7047e-01, 1.8373e-01, 4.2117e-01, 4.0968e-01,\n",
            "        6.3929e-01, 4.9791e-01, 6.6825e-01, 5.1183e-01, 2.7886e-01, 4.9882e-01,\n",
            "        3.5625e-01, 6.5450e-01, 3.6527e-02, 5.1744e-01, 8.9006e-01, 1.2157e-02,\n",
            "        5.9904e-01, 2.4540e-01, 1.6794e-01, 9.2998e-01, 4.0279e-02, 2.4260e-01,\n",
            "        3.4836e-01, 3.4447e-01, 4.9939e-01, 2.7418e-01, 2.5026e-01, 4.6842e-02,\n",
            "        3.1047e-01, 7.1694e-01, 1.0032e-01, 6.6946e-01, 7.9484e-01, 9.6020e-01,\n",
            "        9.5785e-01, 4.7742e-01, 6.4045e-01, 2.5790e-01, 9.0411e-01, 1.8239e-01,\n",
            "        5.6629e-01, 3.1424e-01, 7.9681e-01, 8.2040e-01, 3.6405e-01, 6.2518e-01,\n",
            "        8.5671e-01, 1.6450e-02, 7.2958e-01, 7.5289e-01, 6.9599e-01, 2.1148e-01,\n",
            "        9.2471e-01, 8.1655e-02, 4.4848e-01, 1.6865e-01, 3.4887e-02, 4.5478e-02,\n",
            "        7.8096e-01, 1.2938e-01, 3.9106e-01, 6.3004e-01, 3.3767e-02, 8.8991e-01,\n",
            "        5.2249e-01, 1.4721e-01, 2.4869e-01, 2.7902e-01, 4.4089e-01, 9.6541e-01,\n",
            "        8.7808e-01, 6.2858e-01, 3.7202e-01, 4.4740e-01, 7.9769e-01, 4.0701e-01,\n",
            "        6.4757e-01, 3.6987e-01, 7.0429e-01, 6.9080e-01, 5.7584e-01, 1.4612e-01,\n",
            "        8.7374e-02, 8.0099e-01, 9.9734e-01, 3.1033e-01, 2.5533e-01, 4.7699e-01,\n",
            "        8.9931e-01, 5.2820e-01, 2.1355e-01, 9.8249e-01, 4.3130e-01, 8.9913e-01,\n",
            "        7.3011e-01, 1.5631e-01, 5.5716e-01, 8.5785e-01, 3.0137e-01, 3.9062e-01,\n",
            "        3.5729e-01, 5.3095e-01, 7.0903e-01, 4.8060e-01, 8.4849e-01, 8.3610e-01,\n",
            "        2.5110e-01, 6.7341e-01, 4.4396e-01, 3.2635e-01, 7.8772e-01, 8.6130e-01,\n",
            "        8.7632e-01, 7.7043e-01, 1.3978e-01, 7.4934e-01, 2.6930e-01, 7.5239e-02,\n",
            "        6.8217e-01, 2.9890e-01, 8.0782e-01, 6.8344e-01, 9.6626e-01, 1.9505e-01,\n",
            "        2.1102e-01, 6.4651e-01, 3.7906e-01, 3.8882e-01, 2.7281e-04, 8.4345e-01,\n",
            "        2.2716e-01, 1.2875e-01, 6.2419e-01, 2.4556e-01, 9.6293e-01, 2.7204e-01,\n",
            "        1.4464e-01, 2.2702e-01, 1.8500e-01, 3.9073e-01, 7.5716e-01, 9.6164e-01,\n",
            "        8.2638e-01, 6.4418e-01, 1.0885e-01, 3.1821e-01, 8.8076e-01, 8.3374e-01,\n",
            "        8.4679e-03, 1.0961e-01, 8.7625e-01, 8.4715e-01, 7.6044e-01, 3.5389e-01,\n",
            "        4.7129e-01, 1.1337e-02, 1.0149e-01, 8.9915e-01, 6.9596e-01, 8.9623e-01,\n",
            "        7.3623e-01, 3.0725e-01, 4.3853e-01, 6.8112e-01, 8.0361e-01, 7.1457e-01,\n",
            "        4.3817e-01, 3.5636e-01, 3.8396e-01, 1.4785e-02, 7.8263e-01, 9.2124e-01,\n",
            "        1.7243e-01, 9.4371e-01, 6.9253e-01, 9.0690e-01, 2.6055e-01, 8.4046e-01,\n",
            "        9.5194e-02, 3.7927e-01, 1.6089e-01, 2.3517e-01, 1.6561e-01, 1.7640e-01,\n",
            "        4.7562e-01, 5.6887e-01, 9.2977e-02, 6.7819e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([0.2355, 0.4099, 0.1521, 0.1077, 0.2865, 0.3309, 0.4287, 0.9945, 0.0639,\n",
            "        0.9464, 0.7783, 0.6266, 0.2945, 0.9292, 0.1519, 0.2842, 0.8181, 0.7754,\n",
            "        0.1803, 0.6362, 0.9763, 0.6074, 0.2673, 0.3366, 0.9178, 0.0933, 0.4529,\n",
            "        0.4163, 0.4573, 0.2792, 0.2131, 0.9512, 0.1701, 0.5499, 0.3667, 0.8301,\n",
            "        0.8840, 0.7060, 0.9395, 0.4110, 0.1065, 0.0919, 0.3630, 0.7365, 0.2265,\n",
            "        0.5686, 0.4378, 0.1645, 0.1614, 0.4817, 0.8058, 0.7558, 0.5555, 0.3463,\n",
            "        0.3641, 0.3010, 0.0802, 0.6314, 0.3833, 0.7554, 0.9091, 0.1995, 0.8315,\n",
            "        0.3775, 0.6494, 0.4668, 0.0438, 0.8121, 0.9887, 0.8857, 0.4801, 0.2599,\n",
            "        0.1537, 0.9621, 0.0760, 0.3292, 0.8831, 0.4584, 0.4060, 0.1263, 0.9186,\n",
            "        0.6605, 0.9697, 0.9383, 0.2993, 0.7180, 0.1394, 0.7669, 0.9659, 0.5253,\n",
            "        0.1406, 0.4778, 0.0622, 0.7868, 0.2264, 0.3225, 0.6780, 0.6838, 0.5975,\n",
            "        0.8482, 0.3985, 0.1344, 0.8127, 0.7753, 0.0374, 0.4729, 0.8265, 0.5215,\n",
            "        0.3663, 0.7708, 0.1300, 0.7348, 0.0976, 0.5863, 0.9377, 0.1197, 0.5350,\n",
            "        0.2524, 0.1775, 0.8622, 0.5089, 0.7306, 0.4124, 0.1468, 0.6299, 0.4159,\n",
            "        0.7665, 0.0764, 0.3868, 0.4931, 0.6910, 0.1398, 0.9861, 0.7585, 0.0460,\n",
            "        0.3148, 0.5403, 0.0077, 0.2365, 0.2639, 0.0761, 0.9495, 0.0656, 0.4300,\n",
            "        0.0142, 0.8482, 0.0258, 0.6746, 0.7343, 0.8930, 0.8766, 0.3561, 0.9712,\n",
            "        0.5771, 0.3675, 0.1681, 0.0237, 0.0388, 0.0857, 0.7539, 0.8428, 0.1468,\n",
            "        0.4612, 0.4408, 0.0869, 0.5450, 0.6990, 0.6294, 0.3856, 0.9986, 0.7154,\n",
            "        0.1330, 0.3734, 0.0466, 0.0819, 0.0923, 0.9227, 0.4900, 0.8164, 0.5660,\n",
            "        0.4438, 0.3435, 0.0834, 0.8018, 0.2807, 0.5489, 0.8348, 0.1119, 0.7251,\n",
            "        0.5735, 0.7893, 0.1132, 0.8214, 0.1463, 0.5791, 0.6675, 0.0448, 0.5244,\n",
            "        0.6484, 0.7683, 0.1876, 0.9610, 0.8148, 0.9100, 0.2340, 0.9800, 0.6316,\n",
            "        0.1139, 0.0349, 0.2029, 0.6207, 0.3577, 0.3602, 0.9753, 0.2489, 0.2849,\n",
            "        0.2838, 0.6632, 0.6778, 0.4587, 0.9474, 0.3628, 0.8038, 0.1252, 0.3273,\n",
            "        0.4290, 0.6744, 0.9340, 0.2445, 0.1775, 0.1706, 0.9188, 0.8421, 0.7687,\n",
            "        0.5456, 0.1782, 0.2703, 0.1276, 0.7321, 0.6156, 0.6362, 0.0450, 0.9360,\n",
            "        0.7369, 0.8362, 0.0941, 0.2819, 0.7962, 0.7630, 0.5073, 0.9039, 0.8465,\n",
            "        0.3480, 0.5274, 0.9356, 0.9094, 0.9948, 0.7432, 0.7047, 0.8660, 0.2329,\n",
            "        0.6371, 0.7584, 0.8411, 0.0123, 0.4981, 0.8989, 0.7179, 0.2114, 0.0462,\n",
            "        0.2341, 0.1914, 0.4854, 0.0816, 0.5844, 0.0890, 0.5809, 0.6527, 0.4155,\n",
            "        0.2292, 0.0872, 0.7118, 0.3661, 0.0125, 0.4469, 0.0906, 0.0434, 0.3473,\n",
            "        0.4753, 0.0853, 0.9377, 0.3776, 0.1736, 0.7098, 0.3537, 0.7481, 0.7149,\n",
            "        0.3244, 0.8912, 0.8266, 0.7535, 0.2930, 0.9962, 0.5044, 0.7119, 0.6155,\n",
            "        0.3431, 0.7143, 0.5526, 0.1544, 0.3084, 0.1803, 0.7026, 0.2716, 0.5248,\n",
            "        0.1900, 0.7375, 0.4598, 0.9488, 0.8196, 0.7739, 0.0430, 0.5774, 0.9329,\n",
            "        0.5402, 0.0916, 0.1661, 0.3530, 0.5706, 0.4855, 0.9888, 0.1503, 0.1502,\n",
            "        0.4786, 0.2809, 0.3487, 0.1214, 0.8822, 0.6404, 0.7319, 0.1056, 0.2723,\n",
            "        0.6740, 0.3548, 0.2676, 0.1631, 0.0037, 0.6905, 0.6184, 0.6371, 0.5566,\n",
            "        0.7452, 0.8033, 0.8529, 0.5828, 0.4186, 0.4820, 0.0107, 0.6463, 0.7136,\n",
            "        0.1430, 0.9311, 0.8688, 0.0658, 0.2753, 0.4133, 0.3854, 0.2683, 0.9630,\n",
            "        0.0157, 0.3303, 0.1681, 0.4315, 0.0750, 0.1609, 0.9961, 0.8400, 0.0690,\n",
            "        0.0438, 0.6329, 0.3565, 0.0914, 0.7600, 0.1219, 0.9500, 0.6203, 0.5664,\n",
            "        0.8665, 0.6413, 0.2845, 0.5156, 0.7480, 0.3657, 0.0242, 0.6967, 0.2386,\n",
            "        0.2137, 0.3106, 0.5239, 0.6874, 0.8931, 0.8224, 0.7501, 0.0692, 0.1384,\n",
            "        0.3946, 0.5206, 0.6235, 0.5631, 0.5462, 0.7783, 0.3269, 0.7170, 0.9237,\n",
            "        0.4168, 0.0259, 0.2673, 0.3210, 0.1034, 0.7998, 0.6055, 0.0898, 0.3716,\n",
            "        0.4138, 0.4994, 0.2640, 0.6184, 0.6644, 0.1202, 0.0178, 0.4520, 0.2573,\n",
            "        0.2372, 0.7169, 0.0234, 0.2002, 0.5513, 0.3643, 0.9918, 0.9529, 0.4850,\n",
            "        0.6287, 0.8566, 0.7287, 0.4994, 0.1985, 0.9073, 0.8032, 0.7647, 0.0120,\n",
            "        0.6207, 0.7168, 0.5411, 0.8930, 0.1355, 0.9937, 0.3855, 0.6306, 0.6573,\n",
            "        0.0743, 0.1919, 0.0403, 0.4787, 0.5685, 0.3653, 0.7457, 0.4948, 0.8275,\n",
            "        0.3972, 0.2549, 0.8862, 0.3975, 0.7855, 0.1136, 0.1453, 0.0874, 0.4953,\n",
            "        0.6355, 0.3268, 0.8904, 0.6172, 0.8294, 0.8915, 0.7450, 0.1690, 0.6939,\n",
            "        0.4595, 0.5140, 0.6271, 0.6056, 0.9665, 0.6766, 0.9980, 0.8609, 0.0290,\n",
            "        0.4541, 0.9861, 0.5242, 0.5714, 0.8376, 0.6201, 0.4643, 0.7621, 0.8943,\n",
            "        0.7200, 0.9319, 0.8553, 0.8102, 0.5175, 0.1908, 0.3055, 0.4004, 0.2262,\n",
            "        0.9172, 0.7813, 0.6406, 0.1553, 0.6378, 0.3953, 0.7035, 0.3615, 0.8245,\n",
            "        0.2536, 0.5536, 0.9202, 0.4367, 0.6661, 0.5640, 0.0842, 0.5418, 0.3296,\n",
            "        0.1075, 0.1546, 0.3406, 0.0894, 0.9383, 0.8480, 0.8545, 0.2164, 0.9147,\n",
            "        0.9152, 0.3779, 0.8451, 0.5392, 0.1543, 0.9143, 0.4533, 0.3305, 0.8211,\n",
            "        0.0756, 0.7264, 0.8963, 0.3683, 0.9758, 0.3222, 0.9571, 0.1953, 0.9166,\n",
            "        0.9818, 0.6100, 0.7516, 0.4695, 0.1073, 0.7154, 0.4742, 0.7235, 0.3614,\n",
            "        0.2490, 0.0965, 0.8917, 0.8415, 0.8689, 0.8619, 0.7526, 0.2597, 0.7017,\n",
            "        0.4823, 0.4549, 0.3024, 0.2732, 0.5566, 0.3424, 0.9159, 0.4667, 0.5763,\n",
            "        0.4138, 0.9698, 0.2768, 0.8567, 0.4462, 0.6098, 0.5823, 0.8108, 0.8495,\n",
            "        0.9050, 0.8206, 0.7813, 0.8179, 0.0169, 0.3147, 0.9952, 0.8519, 0.5828,\n",
            "        0.5433, 0.8186, 0.6078, 0.6499, 0.5619, 0.9295, 0.0028, 0.1513, 0.4311,\n",
            "        0.5267, 0.8378, 0.2203, 0.7479, 0.0033, 0.4109, 0.7134, 0.6825, 0.4103,\n",
            "        0.4154, 0.5163, 0.7978, 0.1192, 0.4918, 0.2481, 0.7657, 0.3983, 0.5070,\n",
            "        0.3593, 0.6446, 0.8453, 0.6655, 0.9280, 0.5454, 0.3228, 0.4270, 0.8762,\n",
            "        0.2616, 0.7366, 0.6021, 0.3713, 0.6359, 0.7921, 0.8268, 0.8541, 0.6503,\n",
            "        0.9438, 0.4495, 0.6046, 0.2737, 0.3565, 0.1670, 0.8687, 0.3939, 0.8826,\n",
            "        0.6432, 0.3850, 0.7452, 0.3038, 0.7779, 0.3268, 0.1405, 0.4743, 0.3737,\n",
            "        0.7351, 0.3342, 0.6962, 0.8790, 0.6935, 0.4317, 0.2736, 0.2028, 0.8468,\n",
            "        0.5182, 0.5961, 0.9918, 0.4011, 0.0499, 0.7350, 0.7165, 0.9149, 0.0846,\n",
            "        0.9972, 0.6501, 0.1351, 0.4218, 0.5962, 0.5620, 0.8962, 0.1865, 0.5340,\n",
            "        0.4063, 0.2467, 0.6334, 0.8952, 0.3408, 0.7829, 0.7011, 0.1777, 0.9247,\n",
            "        0.7795, 0.7604, 0.2274, 0.1654, 0.7437, 0.9767, 0.1336, 0.7638, 0.9505,\n",
            "        0.4658, 0.8119, 0.1099, 0.7495, 0.8849, 0.5249, 0.2937, 0.4341, 0.3439,\n",
            "        0.6551, 0.1737, 0.3551, 0.3408, 0.2537, 0.5237, 0.3418, 0.8913, 0.5315,\n",
            "        0.7563, 0.0562, 0.7204, 0.5701, 0.5152, 0.0350, 0.2346, 0.6198, 0.7490,\n",
            "        0.7031, 0.5920, 0.8984, 0.9781, 0.9207, 0.1481, 0.5984, 0.5448, 0.8516,\n",
            "        0.1293, 0.5540, 0.0247, 0.7976, 0.6143, 0.7685, 0.7415, 0.8221, 0.9264,\n",
            "        0.0085, 0.0062, 0.4046, 0.2029, 0.6999, 0.0923, 0.5995, 0.4588, 0.3665,\n",
            "        0.3177, 0.7159, 0.9713, 0.2741, 0.7828, 0.5250, 0.1232, 0.8012, 0.3767,\n",
            "        0.7060, 0.4554, 0.3179, 0.4178, 0.7266, 0.5222, 0.7307, 0.4946, 0.3969,\n",
            "        0.6574, 0.3649, 0.4997, 0.4937, 0.2987, 0.4234, 0.7915, 0.5426, 0.9142,\n",
            "        0.8002, 0.2343, 0.8374, 0.4197, 0.7101, 0.0525, 0.7705, 0.3418, 0.9599,\n",
            "        0.3746, 0.6969, 0.6930, 0.8595, 0.2481, 0.1885, 0.3815, 0.0419, 0.5692,\n",
            "        0.1853, 0.3237, 0.8556, 0.5808, 0.3303, 0.9228, 0.3148, 0.3833, 0.2725,\n",
            "        0.6946, 0.4346, 0.4362, 0.6065, 0.0386, 0.2659, 0.1790, 0.8545, 0.0479,\n",
            "        0.1054, 0.0992, 0.5935, 0.2453, 0.8017, 0.5596, 0.2091, 0.3099, 0.6727,\n",
            "        0.6282, 0.8388, 0.7938, 0.9376, 0.8679, 0.5340, 0.9637, 0.9313, 0.0954,\n",
            "        0.0224, 0.3651, 0.8864, 0.2852, 0.5109, 0.0823, 0.3666, 0.6108, 0.6151,\n",
            "        0.1520, 0.9607, 0.0912, 0.7327, 0.9656, 0.2473, 0.6519, 0.9459, 0.4730,\n",
            "        0.2391, 0.7852, 0.1004, 0.4977, 0.3516, 0.0340, 0.7019, 0.7879, 0.7198,\n",
            "        0.7207, 0.6607, 0.6564, 0.6504, 0.1638, 0.0961, 0.3334, 0.9868, 0.8166,\n",
            "        0.0579, 0.3167, 0.6310, 0.0400, 0.5888, 0.7057, 0.3784, 0.0146, 0.1169,\n",
            "        0.5938, 0.6430, 0.0900, 0.1729, 0.4619, 0.7062, 0.8556, 0.7215, 0.6217,\n",
            "        0.3729, 0.1096, 0.1966, 0.3889, 0.0969, 0.8441, 0.9456, 0.0260, 0.1945,\n",
            "        0.2671, 0.7246, 0.5390, 0.1388, 0.5362, 0.7654, 0.9415, 0.8007, 0.0546,\n",
            "        0.0993, 0.5214, 0.3185, 0.2396, 0.7748, 0.7800, 0.9325, 0.8123, 0.1532,\n",
            "        0.4966, 0.3828, 0.2209, 0.6741, 0.1976, 0.1943, 0.3380, 0.2506, 0.4627,\n",
            "        0.4851, 0.5158, 0.0381, 0.6087, 0.2847, 0.3855, 0.3396, 0.7449, 0.1769,\n",
            "        0.2624, 0.4791, 0.3396, 0.0491, 0.2382, 0.6390, 0.2171, 0.4243, 0.1134,\n",
            "        0.1779, 0.5535, 0.1747, 0.9248, 0.9229, 0.9227, 0.9211, 0.6860, 0.7986,\n",
            "        0.6262, 0.3119, 0.2170, 0.0666, 0.9339, 0.7862, 0.3546, 0.1081, 0.9792,\n",
            "        0.3334, 0.2276, 0.4982, 0.6410, 0.5067, 0.3915, 0.4814, 0.7397, 0.1154,\n",
            "        0.7472, 0.6568, 0.8380, 0.5541, 0.8000, 0.8013, 0.2290, 0.5931, 0.2457,\n",
            "        0.5798, 0.2231, 0.5661, 0.2822, 0.7052, 0.9127, 0.4698, 0.5828, 0.2037,\n",
            "        0.8051], grad_fn=<SplitBackward0>), tensor([5.5065e-01, 4.9486e-01, 7.0141e-01, 1.0115e-01, 7.0366e-01, 5.4407e-01,\n",
            "        6.6422e-01, 2.3302e-01, 1.4425e-01, 7.8225e-01, 6.2604e-01, 8.5510e-01,\n",
            "        9.3551e-01, 7.2379e-01, 5.5668e-01, 4.8011e-01, 7.7423e-01, 3.5044e-01,\n",
            "        7.5058e-01, 5.4304e-02, 5.1597e-01, 3.0526e-01, 2.8768e-01, 6.1430e-01,\n",
            "        5.5522e-01, 3.2230e-01, 4.7122e-01, 3.2837e-01, 4.7430e-01, 8.0581e-01,\n",
            "        8.5758e-01, 8.6328e-01, 4.9438e-01, 5.9695e-01, 9.8801e-02, 7.7373e-01,\n",
            "        5.1985e-01, 3.9834e-01, 6.7225e-01, 1.6397e-01, 6.4311e-01, 3.2635e-01,\n",
            "        5.6527e-01, 3.2729e-01, 1.5327e-01, 4.6486e-01, 6.2153e-01, 8.8032e-01,\n",
            "        4.9165e-01, 3.2588e-01, 7.0675e-01, 3.2389e-02, 4.8725e-01, 8.1186e-01,\n",
            "        9.1359e-01, 8.7428e-01, 4.1627e-01, 4.2373e-01, 3.0893e-01, 1.0654e-01,\n",
            "        4.1872e-01, 2.7303e-01, 7.1687e-01, 4.3160e-01, 6.2386e-01, 6.3615e-01,\n",
            "        8.8029e-01, 5.2125e-01, 1.9523e-01, 6.2269e-01, 7.4747e-01, 2.6125e-01,\n",
            "        1.7083e-01, 6.6926e-01, 3.6888e-01, 7.3104e-01, 4.8485e-01, 8.7792e-01,\n",
            "        8.8856e-01, 6.0328e-01, 4.6037e-01, 2.0829e-01, 4.3372e-01, 7.0031e-01,\n",
            "        1.3142e-01, 9.9964e-01, 4.5376e-01, 8.1440e-01, 7.0503e-01, 7.8847e-02,\n",
            "        5.9593e-01, 5.3909e-01, 6.5084e-01, 2.1951e-01, 9.4194e-01, 7.9827e-01,\n",
            "        5.0958e-01, 8.8882e-01, 2.7176e-01, 8.2297e-01, 8.0972e-01, 8.2577e-01,\n",
            "        6.9961e-01, 6.2455e-01, 6.7682e-01, 7.1395e-01, 4.4101e-02, 9.4377e-02,\n",
            "        7.2946e-02, 8.9453e-01, 2.4413e-01, 5.1442e-01, 8.3675e-01, 5.9171e-01,\n",
            "        6.9255e-01, 4.4987e-01, 3.2235e-01, 8.3901e-01, 3.9034e-01, 8.2287e-01,\n",
            "        9.9582e-01, 9.6310e-01, 8.3783e-02, 9.4924e-01, 5.3071e-01, 6.9515e-01,\n",
            "        7.9726e-01, 8.0811e-01, 9.7737e-01, 3.8627e-01, 6.8191e-01, 1.7717e-01,\n",
            "        2.0741e-01, 2.6644e-01, 7.0161e-01, 1.2833e-01, 6.8103e-01, 1.2878e-01,\n",
            "        8.7864e-02, 7.8122e-01, 6.0457e-01, 2.2523e-01, 4.6400e-01, 6.4829e-01,\n",
            "        3.6070e-01, 7.5655e-01, 1.2898e-01, 8.7725e-01, 6.9647e-01, 2.7663e-01,\n",
            "        9.4903e-01, 3.8659e-01, 6.2224e-01, 1.8075e-03, 6.5866e-01, 8.6821e-02,\n",
            "        2.8511e-01, 5.4937e-01, 8.7729e-01, 5.0472e-01, 9.2051e-02, 1.5755e-01,\n",
            "        3.6766e-01, 8.2189e-01, 9.9966e-01, 4.0238e-01, 4.1282e-01, 6.1711e-01,\n",
            "        1.9931e-01, 2.1233e-01, 7.1401e-01, 3.1797e-03, 9.2804e-01, 1.4008e-01,\n",
            "        7.9789e-01, 7.9549e-01, 2.2564e-01, 4.4527e-01, 7.7523e-01, 4.3709e-01,\n",
            "        8.4489e-01, 2.2940e-01, 6.8285e-01, 5.9948e-01, 7.7434e-01, 1.2674e-01,\n",
            "        3.5949e-01, 6.9529e-01, 6.5487e-01, 9.0691e-01, 9.6738e-01, 5.8144e-01,\n",
            "        5.3482e-01, 1.2473e-01, 1.8304e-01, 6.3893e-01, 6.9676e-01, 7.0133e-01,\n",
            "        8.2343e-01, 7.7537e-02, 1.6790e-01, 4.5442e-01, 9.5526e-01, 4.9820e-01,\n",
            "        1.0525e-01, 9.9499e-01, 6.0606e-01, 7.2428e-01, 9.1784e-01, 5.1845e-01,\n",
            "        9.0721e-01, 1.8949e-01, 1.5993e-01, 9.3923e-01, 3.4925e-01, 5.2665e-01,\n",
            "        5.7068e-01, 1.2885e-01, 1.8606e-01, 4.1517e-01, 8.7398e-01, 4.8669e-01,\n",
            "        8.1950e-01, 3.7737e-01, 5.1616e-01, 1.0869e-01, 5.6456e-01, 1.3137e-01,\n",
            "        5.3519e-01, 4.4598e-01, 3.2874e-01, 6.2173e-02, 1.4362e-01, 7.0738e-01,\n",
            "        3.9316e-01, 9.9828e-01, 5.3907e-01, 5.8473e-01, 2.5882e-01, 9.2225e-01,\n",
            "        1.3595e-01, 5.8610e-01, 6.4437e-01, 4.4456e-01, 3.4303e-01, 1.1081e-01,\n",
            "        7.9407e-01, 3.5847e-01, 3.4603e-01, 3.0264e-02, 5.3435e-01, 6.0161e-01,\n",
            "        7.2960e-01, 6.6222e-01, 6.4626e-02, 7.6799e-02, 3.9326e-01, 5.2464e-01,\n",
            "        9.1179e-01, 9.6037e-01, 1.4815e-01, 1.2998e-01, 8.8173e-01, 1.8695e-01,\n",
            "        9.0823e-01, 2.8102e-02, 6.5888e-01, 6.9748e-01, 6.9335e-01, 7.1700e-01,\n",
            "        4.9362e-01, 3.3748e-01, 8.3229e-01, 8.1312e-01, 6.4185e-01, 7.2047e-01,\n",
            "        8.3332e-01, 1.1006e-01, 2.5047e-01, 6.4593e-02, 3.5454e-02, 8.2307e-01,\n",
            "        5.8081e-01, 9.0209e-01, 6.6370e-01, 3.8020e-02, 3.6399e-01, 5.5484e-01,\n",
            "        9.2634e-01, 3.8475e-01, 4.1614e-01, 7.1091e-01, 4.7402e-01, 2.7598e-01,\n",
            "        9.8326e-01, 1.1592e-01, 6.6410e-01, 3.0837e-01, 8.8098e-01, 4.3170e-01,\n",
            "        7.3104e-01, 1.9777e-01, 1.6109e-01, 8.8197e-01, 1.6904e-01, 4.2492e-01,\n",
            "        1.1752e-01, 1.4712e-01, 5.6916e-01, 5.9669e-01, 4.5977e-01, 5.0369e-01,\n",
            "        6.5771e-01, 7.3122e-01, 4.4016e-01, 8.3331e-01, 8.6926e-01, 2.0504e-01,\n",
            "        7.9742e-01, 9.0071e-01, 8.0117e-01, 1.7528e-01, 5.4327e-01, 7.5806e-01,\n",
            "        9.0411e-01, 6.4179e-01, 7.8577e-01, 9.9050e-01, 2.0661e-01, 1.1074e-01,\n",
            "        9.5655e-02, 4.3872e-01, 5.4916e-01, 8.4182e-01, 8.5534e-01, 6.2425e-01,\n",
            "        5.8870e-01, 1.6040e-01, 1.9052e-01, 2.1620e-01, 7.0514e-01, 4.9132e-02,\n",
            "        1.6934e-01, 6.5611e-01, 8.5716e-01, 6.6167e-01, 3.5662e-01, 9.6980e-01,\n",
            "        1.9363e-01, 8.9121e-01, 6.0542e-02, 5.0049e-03, 2.9146e-01, 9.8062e-01,\n",
            "        1.0050e-01, 6.3549e-01, 5.0422e-01, 5.7770e-01, 2.6721e-01, 3.6263e-01,\n",
            "        5.5019e-01, 8.5277e-01, 2.7922e-01, 7.0651e-01, 8.8536e-01, 6.3343e-01,\n",
            "        8.2469e-01, 1.9579e-01, 9.0172e-01, 6.5548e-01, 6.1083e-01, 2.1509e-01,\n",
            "        3.9263e-01, 1.1262e-01, 1.0312e-01, 2.6481e-01, 7.3815e-01, 8.5747e-01,\n",
            "        8.0090e-01, 6.9675e-01, 5.3915e-01, 8.4870e-01, 1.3100e-01, 2.4474e-01,\n",
            "        4.9017e-01, 7.5670e-01, 9.6879e-01, 2.5288e-01, 4.3422e-01, 2.8141e-01,\n",
            "        6.1564e-01, 6.2749e-01, 4.7930e-01, 6.8935e-01, 4.6501e-01, 6.3420e-01,\n",
            "        3.5790e-01, 8.4776e-01, 2.2625e-01, 9.1713e-01, 7.0476e-01, 1.1396e-01,\n",
            "        2.0670e-01, 5.7564e-01, 5.8275e-01, 8.9975e-01, 7.3806e-01, 2.4471e-01,\n",
            "        4.0616e-01, 7.9673e-01, 2.4288e-01, 4.1801e-01, 3.0928e-01, 6.1608e-01,\n",
            "        1.9608e-01, 2.0304e-01, 6.3379e-01, 5.0144e-01, 8.7964e-01, 1.3732e-01,\n",
            "        4.5979e-01, 6.5704e-01, 9.8880e-01, 4.6073e-01, 1.1852e-02, 3.4293e-01,\n",
            "        2.7037e-01, 8.2107e-01, 8.8981e-01, 4.5947e-01, 4.6093e-01, 7.1699e-01,\n",
            "        7.2524e-01, 9.6926e-02, 3.2735e-01, 2.6065e-01, 6.5445e-01, 8.6729e-01,\n",
            "        6.1068e-01, 9.3987e-01, 1.7561e-01, 2.5659e-01, 4.8952e-02, 5.2217e-01,\n",
            "        7.4576e-01, 3.2168e-01, 4.8852e-01, 3.3372e-02, 6.5158e-01, 1.1627e-01,\n",
            "        4.3784e-01, 4.9880e-01, 2.1394e-01, 2.7493e-01, 5.3248e-01, 6.3265e-01,\n",
            "        5.9054e-01, 9.7598e-01, 2.2182e-01, 9.3163e-01, 3.1344e-01, 1.9815e-01,\n",
            "        2.2055e-01, 4.0981e-02, 1.9353e-01, 6.2442e-01, 2.6313e-01, 1.8720e-01,\n",
            "        8.4404e-01, 4.6854e-01, 3.9493e-01, 4.2280e-02, 7.9335e-01, 9.6209e-01,\n",
            "        4.3004e-01, 2.3239e-01, 2.2528e-01, 9.4483e-01, 4.6759e-01, 7.8280e-01,\n",
            "        4.2689e-02, 3.6282e-01, 9.2502e-01, 9.9566e-01, 1.5474e-01, 2.1000e-01,\n",
            "        1.8874e-01, 2.0106e-01, 2.8213e-01, 6.3076e-01, 2.3269e-02, 7.4326e-01,\n",
            "        6.5857e-01, 9.5343e-01, 8.7452e-01, 5.0071e-02, 6.2191e-01, 6.7687e-01,\n",
            "        3.1250e-01, 6.3800e-01, 6.2580e-01, 2.1261e-01, 7.3584e-01, 9.3183e-01,\n",
            "        4.2500e-01, 4.0804e-01, 3.5016e-01, 5.6618e-01, 3.3904e-01, 4.3812e-01,\n",
            "        9.6207e-01, 6.1816e-01, 7.0638e-01, 1.3876e-01, 6.2002e-01, 7.5487e-01,\n",
            "        2.0425e-01, 9.0851e-01, 7.9381e-01, 6.5054e-01, 3.6249e-01, 1.5483e-01,\n",
            "        6.2649e-01, 7.6282e-01, 7.5730e-01, 6.4102e-02, 4.5333e-01, 3.2638e-01,\n",
            "        5.5778e-01, 6.6132e-01, 1.9132e-01, 8.0824e-01, 6.3585e-01, 4.4619e-01,\n",
            "        4.3008e-01, 5.1161e-01, 7.0968e-01, 2.5633e-01, 1.6738e-01, 3.0955e-01,\n",
            "        6.1105e-01, 7.4772e-02, 1.0032e-01, 1.4390e-01, 5.9288e-01, 1.0881e-01,\n",
            "        4.0829e-01, 7.5662e-01, 4.1943e-01, 9.8642e-01, 7.7753e-01, 2.0818e-01,\n",
            "        2.8934e-01, 5.6991e-01, 3.7381e-01, 9.7683e-01, 8.9841e-01, 6.0635e-01,\n",
            "        6.3153e-01, 6.8627e-01, 3.6319e-02, 4.5005e-01, 3.0579e-03, 8.4828e-01,\n",
            "        7.7732e-01, 2.2879e-02, 8.4716e-01, 1.0088e-01, 5.5398e-01, 8.8182e-01,\n",
            "        5.4003e-01, 6.7732e-03, 9.8250e-02, 5.1487e-01, 4.4043e-01, 4.2400e-01,\n",
            "        7.1846e-02, 4.7100e-01, 8.3088e-01, 3.2221e-01, 4.1955e-01, 1.6768e-01,\n",
            "        8.9439e-01, 1.2264e-01, 1.6205e-01, 3.0327e-01, 4.2602e-01, 4.6344e-01,\n",
            "        2.5642e-01, 3.1570e-01, 3.8599e-01, 2.1784e-01, 1.0411e-01, 4.8849e-01,\n",
            "        9.7603e-01, 4.9031e-01, 4.8213e-01, 1.6942e-01, 3.4130e-04, 2.5584e-01,\n",
            "        6.6371e-01, 6.5420e-01, 2.1186e-01, 3.2606e-01, 9.3089e-01, 3.9789e-01,\n",
            "        7.0599e-01, 5.0123e-01, 9.9742e-01, 9.0281e-02, 3.3580e-01, 2.4208e-01,\n",
            "        3.3547e-01, 6.7798e-01, 2.3253e-01, 9.4596e-01, 9.3983e-01, 4.9793e-01,\n",
            "        9.8735e-01, 5.5680e-01, 9.3483e-01, 3.5239e-01, 4.9983e-01, 9.2543e-01,\n",
            "        3.3574e-01, 8.9586e-01, 5.4514e-01, 1.2761e-01, 4.9016e-01, 4.2518e-01,\n",
            "        6.9341e-01, 5.3087e-01, 9.0245e-01, 1.1683e-01, 7.4834e-01, 5.0355e-01,\n",
            "        5.1173e-01, 1.4761e-01, 3.2614e-01, 2.1847e-01, 1.1318e-01, 3.4444e-01,\n",
            "        8.2524e-01, 4.1478e-02, 9.4092e-01, 8.4591e-01, 3.3636e-01, 9.5272e-01,\n",
            "        9.9888e-01, 6.8131e-01, 3.3700e-01, 5.9829e-01, 7.4948e-02, 3.4628e-01,\n",
            "        8.5408e-01, 8.2062e-02, 4.2900e-01, 6.1527e-01, 1.9234e-01, 5.1735e-01,\n",
            "        6.8088e-01, 3.9423e-01, 2.5933e-01, 7.8521e-01, 4.3646e-01, 4.5198e-01,\n",
            "        2.7465e-01, 2.9172e-01, 1.0899e-01, 1.6328e-01, 7.9326e-01, 1.1220e-01,\n",
            "        1.9231e-02, 5.8135e-01, 5.5668e-01, 1.8478e-01, 6.1670e-01, 9.8246e-01,\n",
            "        1.8493e-01, 6.6938e-02, 5.6752e-01, 2.4000e-01, 6.6600e-01, 8.0904e-01,\n",
            "        7.6026e-02, 3.4404e-02, 7.9883e-01, 7.9045e-01, 2.7357e-01, 1.2047e-01,\n",
            "        8.8959e-01, 5.1848e-01, 3.3751e-01, 7.1863e-01, 2.7269e-02, 6.6703e-02,\n",
            "        2.8114e-01, 5.8874e-01, 7.2533e-01, 8.5105e-01, 6.6117e-02, 2.5781e-01,\n",
            "        9.6727e-01, 5.2162e-01, 8.7631e-01, 1.1994e-01, 5.7623e-01, 3.5328e-02,\n",
            "        7.6902e-01, 9.5913e-01, 2.4944e-01, 5.0375e-01, 8.6956e-01, 5.1786e-01,\n",
            "        9.1136e-01, 7.8226e-01, 1.8128e-01, 3.2731e-02, 7.7300e-01, 7.8276e-01,\n",
            "        3.2115e-01, 1.6169e-01, 7.2741e-01, 2.6022e-01, 3.1286e-01, 1.2811e-01,\n",
            "        3.0787e-01, 8.8973e-01, 3.8192e-01, 4.0369e-01, 9.8653e-01, 6.2823e-01,\n",
            "        4.8061e-01, 3.3221e-01, 5.3966e-01, 5.2509e-01, 1.5121e-01, 5.5497e-01,\n",
            "        5.8791e-01, 9.8045e-01, 4.3536e-01, 4.1831e-01, 8.6008e-01, 3.8708e-01,\n",
            "        1.5476e-01, 6.7597e-01, 1.0946e-01, 8.3925e-01, 5.9766e-03, 8.4600e-01,\n",
            "        8.2175e-01, 3.6087e-01, 9.4439e-01, 4.6352e-01, 2.2622e-02, 5.7847e-01,\n",
            "        1.6786e-01, 5.4874e-01, 7.0583e-01, 4.7386e-01, 5.3632e-01, 3.0030e-01,\n",
            "        9.7264e-01, 5.4196e-02, 9.6945e-01, 7.6721e-01, 2.6843e-01, 1.7255e-01,\n",
            "        4.5089e-01, 7.9520e-01, 4.2949e-01, 6.4735e-01, 1.0714e-01, 2.4386e-01,\n",
            "        4.3166e-01, 1.4541e-01, 4.5128e-01, 6.9863e-02, 8.6771e-01, 6.2782e-01,\n",
            "        1.9793e-01, 2.5192e-02, 9.4202e-01, 5.3365e-01, 6.4893e-02, 6.5008e-01,\n",
            "        5.8418e-01, 3.4852e-01, 1.5186e-01, 3.1221e-02, 6.0100e-01, 7.9137e-01,\n",
            "        4.6486e-01, 7.4956e-01, 7.7455e-01, 1.5378e-01, 1.2565e-01, 8.8964e-01,\n",
            "        1.0803e-01, 6.5361e-02, 3.9017e-01, 8.1723e-01, 5.7230e-02, 1.0103e-01,\n",
            "        7.5297e-01, 8.1023e-01, 5.5765e-01, 1.6907e-01, 4.4448e-01, 9.0355e-01,\n",
            "        9.5179e-01, 8.2387e-01, 8.7476e-01, 8.9071e-01, 1.7844e-01, 2.6440e-01,\n",
            "        1.4909e-01, 1.4757e-01, 5.9770e-01, 1.8523e-01, 3.7051e-01, 2.2763e-01,\n",
            "        5.7593e-01, 4.7681e-01, 2.7505e-01, 1.5257e-01, 7.6685e-01, 9.1154e-01,\n",
            "        1.4089e-01, 3.3882e-01, 2.1700e-01, 8.5575e-02, 1.3488e-01, 5.5835e-01,\n",
            "        6.9738e-01, 2.8656e-01, 8.8219e-01, 8.0593e-01, 1.0345e-01, 2.3758e-01,\n",
            "        2.5653e-01, 9.0548e-01, 2.6872e-01, 4.3844e-01, 8.0929e-01, 1.3193e-01,\n",
            "        8.3776e-01, 1.1599e-01, 7.1738e-02, 8.8812e-01, 5.3750e-01, 6.3303e-01,\n",
            "        1.0605e-02, 4.6271e-01, 2.2468e-01, 9.8786e-01, 2.1187e-02, 5.5065e-02,\n",
            "        9.0430e-01, 1.5386e-01, 5.0542e-01, 1.6527e-01, 1.5115e-01, 9.6844e-01,\n",
            "        6.2927e-01, 4.7402e-01, 7.5904e-01, 8.9131e-01, 8.8111e-01, 7.9250e-03,\n",
            "        7.4909e-01, 7.8932e-02, 9.2145e-01, 9.4583e-02, 1.4411e-01, 7.0188e-01,\n",
            "        9.4154e-01, 9.5907e-01, 3.2540e-01, 5.0610e-01, 9.0760e-04, 8.1366e-01,\n",
            "        6.6758e-01, 1.2426e-01, 3.1679e-01, 2.7671e-01, 5.4759e-01, 8.3228e-01,\n",
            "        9.6450e-01, 6.9583e-01, 3.8329e-01, 7.8392e-01, 1.1635e-01, 6.1924e-01,\n",
            "        2.2957e-01, 6.2588e-01, 1.4636e-02, 5.6619e-01, 8.0679e-01, 4.7900e-01,\n",
            "        2.3389e-01, 5.9701e-02, 7.8762e-01, 1.7832e-01, 3.7577e-01, 3.1915e-01,\n",
            "        6.4033e-02, 5.4259e-01, 3.1080e-01, 9.7437e-02, 8.8980e-01, 3.3408e-01,\n",
            "        4.0060e-01, 3.2912e-01, 5.4091e-01, 1.6233e-01, 1.3839e-01, 5.2900e-01,\n",
            "        3.6411e-01, 8.9809e-01, 4.5860e-02, 5.7773e-01, 5.5681e-01, 1.8199e-01,\n",
            "        4.4131e-01, 9.4508e-01, 1.1174e-02, 5.7752e-01, 3.2936e-01, 5.0079e-01,\n",
            "        6.1621e-01, 6.8941e-01, 9.6148e-01, 9.0991e-01, 2.4898e-01, 4.7856e-01,\n",
            "        6.4487e-01, 8.0885e-01, 3.8293e-01, 2.4945e-01, 6.2910e-01, 3.1234e-01,\n",
            "        5.2445e-01, 6.0016e-01, 3.6951e-01, 8.4983e-01, 4.0837e-01, 5.2772e-01,\n",
            "        5.1869e-01, 2.9143e-01, 7.4061e-01, 7.2131e-01, 5.7999e-01, 5.9568e-01,\n",
            "        8.8116e-01, 1.5211e-01, 5.4120e-01, 9.6552e-01, 9.2041e-01, 4.4873e-01,\n",
            "        5.5256e-02, 9.3827e-01, 5.1292e-01, 4.6566e-01, 7.3220e-01, 7.4640e-01,\n",
            "        2.2459e-01, 2.0910e-01, 3.8599e-01, 4.5369e-02, 2.5162e-01, 8.5252e-02,\n",
            "        4.0869e-01, 3.9590e-01, 4.8436e-01, 4.8822e-01, 7.2882e-01, 8.6329e-01,\n",
            "        8.9477e-01, 8.2668e-01, 2.1686e-01, 6.1664e-01, 9.2650e-01, 7.4358e-01,\n",
            "        1.8309e-01, 9.9168e-01, 6.1558e-01, 9.9332e-01, 6.2604e-01, 6.9470e-01,\n",
            "        2.8077e-01, 8.8740e-01, 7.5182e-01, 1.9891e-01, 6.2485e-01, 7.8396e-01,\n",
            "        4.8408e-02, 3.6179e-01, 7.6517e-01, 6.6916e-01],\n",
            "       grad_fn=<SplitBackward0>), tensor([-0.0755,  0.0646, -0.0559, -0.0955, -0.0038,  0.0665, -0.0206,  0.0099,\n",
            "        -0.0304,  0.0584,  0.0978,  0.0929, -0.0299, -0.0066, -0.0303, -0.0820,\n",
            "         0.0484,  0.0114,  0.0295,  0.0199, -0.0850, -0.0231,  0.0868, -0.0639,\n",
            "         0.0755,  0.0662, -0.0732, -0.0383,  0.0406, -0.0164,  0.0403, -0.0851,\n",
            "        -0.0363, -0.0880,  0.0057,  0.0896,  0.0789,  0.0581,  0.0980, -0.0327,\n",
            "        -0.0609, -0.0810, -0.0108, -0.0351,  0.0627, -0.0824, -0.0489,  0.0687,\n",
            "         0.0858, -0.0683,  0.0555,  0.0921, -0.0180, -0.0168,  0.0344,  0.0370,\n",
            "         0.0230, -0.0900, -0.0743,  0.0401,  0.0105, -0.0118,  0.0284,  0.0415,\n",
            "         0.0225,  0.0070,  0.0955, -0.0897,  0.0394, -0.0789, -0.0439, -0.0116,\n",
            "         0.0143, -0.0444,  0.0804, -0.0133, -0.0645, -0.0808, -0.0884,  0.0958,\n",
            "        -0.0226, -0.0489,  0.0111,  0.0628, -0.0661,  0.0019, -0.0767,  0.0192,\n",
            "         0.0548, -0.0483,  0.0733, -0.0155, -0.0792, -0.0525,  0.0043, -0.0689,\n",
            "        -0.0610, -0.0145,  0.0591, -0.0813], grad_fn=<SplitBackward0>), tensor([0.9617, 0.4742, 0.4079, 0.8489, 0.9416, 0.6564, 0.8938, 0.4214, 0.3741,\n",
            "        0.3596, 0.9329, 0.5819, 0.6351, 0.1175, 0.6958, 0.3303, 0.0357, 0.9661,\n",
            "        0.7094, 0.1399, 0.4097, 0.9818, 0.0513, 0.3167, 0.2957, 0.8314, 0.6155,\n",
            "        0.8569, 0.6973, 0.3060, 0.2547, 0.0202, 0.3351, 0.9466, 0.8948, 0.5600,\n",
            "        0.3933, 0.6116, 0.8207, 0.3190, 0.1328, 0.9602, 0.3625, 0.0801, 0.1180,\n",
            "        0.8942, 0.1416, 0.9779, 0.0755, 0.8422, 0.4349, 0.4804, 0.0347, 0.2395,\n",
            "        0.4523, 0.5703, 0.8957, 0.2118, 0.5680, 0.1806, 0.0815, 0.6149, 0.7053,\n",
            "        0.7912, 0.9286, 0.1435, 0.4623, 0.9562, 0.0658, 0.3323, 0.9858, 0.8304,\n",
            "        0.4665, 0.7916, 0.7605, 0.8930, 0.9666, 0.3797, 0.9934, 0.9399, 0.1724,\n",
            "        0.3362, 0.3535, 0.4991, 0.9504, 0.4569, 0.7214, 0.7478, 0.4711, 0.4630,\n",
            "        0.1780, 0.1889, 0.5140, 0.2481, 0.5239, 0.6233, 0.8705, 0.8561, 0.0270,\n",
            "        0.4979], grad_fn=<SplitBackward0>), tensor([0.0849], grad_fn=<SplitBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert param_chunks to a numpy array\n",
        "param_chunks_np = np.concatenate([chunk.detach().numpy() for chunk in param_chunks])\n",
        "\n",
        "print(type(param_chunks_np))\n",
        "print(\"size:\", param_chunks_np.shape)"
      ],
      "metadata": {
        "id": "GS1CbLGJR9gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16a2a23-451e-4884-f0e8-212e77fcb16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "size: (11301,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: learning a mapping from each chunk to an integer via VQ-VAE"
      ],
      "metadata": {
        "id": "Uo4o1jPJO08f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X6JeFrWrCi0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "\n",
        "from six.moves import xrange\n",
        "\n",
        "# import umap\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mce1mksSrCi0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0BgGzArCi0"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "using the weights of the above neural network as input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_variance = np.var(param_chunks_np / 255.0)\n",
        "\n",
        "print(data_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elD5efknVUrr",
        "outputId": "0c8edd1c-a795-43f1-9d5b-54fe58234ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3355306e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsAR4cQHrCi2"
      },
      "source": [
        "## Vector Quantizer Layer\n",
        "\n",
        "This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.\n",
        "\n",
        "The output tensor will have the same shape as the input.\n",
        "\n",
        "As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw84w9yPrCi3"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "        '''\n",
        "        # convert inputs from HW -> HW\n",
        "        inputs = inputs.permute(0, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 1).contiguous(), perplexity, encodings # (0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXrFrH_UrCi3"
      },
      "source": [
        "We will also implement a slightly modified version  which will use exponential moving averages to update the embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRx68eOsrCi3"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 1).contiguous() # (0, 2, 3, 1)\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 1).contiguous(), perplexity, encodings # (0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBBSF1p1rCi4"
      },
      "source": [
        "## Encoder & Decoder Architecture\n",
        "\n",
        "The encoder and decoder architecture is based on a ResNet and is implemented below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlplEblqrCi4"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUz-8SmDrCi4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._linear_1 = nn.Linear(in_channels*64, num_hiddens//2)\n",
        "        self._linear_2 = nn.Linear(num_hiddens//2, num_hiddens)\n",
        "\n",
        "        '''\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "        '''\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._linear_1(inputs.view(inputs.size(0), -1)) # x = self._linear_1(inputs)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._linear_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x # self._residual_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP6Yhd3MrCi5"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._linear_1 = nn.Linear(in_channels, num_hiddens)\n",
        "        '''\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "        '''\n",
        "        self._linear_2 = nn.Linear(num_hiddens, num_hiddens//2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._linear_1(inputs.view(inputs.size(0), -1)) # x = self._linear_1(inputs)\n",
        "\n",
        "        # x = self._residual_stack(x)\n",
        "\n",
        "        x = self._linear_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faiz3uawrCi5"
      },
      "source": [
        "## Train\n",
        "\n",
        "We use the hyperparameters from the author's code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IHhIF9crCi5"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_training_updates = 15000\n",
        "\n",
        "num_hiddens = 128\n",
        "num_residual_hiddens = 32\n",
        "num_residual_layers = 2\n",
        "\n",
        "embedding_dim = 64\n",
        "num_embeddings = 512\n",
        "\n",
        "commitment_cost = 0.25\n",
        "\n",
        "decay = 0.99\n",
        "\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW6vZgTorCi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "3ece3b46-bd3f-4d8a-dde2-d6d47804c64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7bec4152e1a0>\n",
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7bec4152df00>\n",
            "ori_data= [tensor([0.9987, 0.0837, 0.8892, 0.9612, 0.5779, 0.5428, 0.8213, 0.5816, 0.4451,\n",
            "        0.6820, 0.3003, 0.6115, 0.4024, 0.0519, 0.8102, 0.4817, 0.1380, 0.8465,\n",
            "        0.4678, 0.8126, 0.8453, 0.7748, 0.4584, 0.0297, 0.3307, 0.8973, 0.5007,\n",
            "        0.7179, 0.9101, 0.5906, 0.9124, 0.2726, 0.3281, 0.6734, 0.8778, 0.9616,\n",
            "        0.2192, 0.8790, 0.5764, 0.3547, 0.8626, 0.1425, 0.1769, 0.0489, 0.7946,\n",
            "        0.2609, 0.7134, 0.1685, 0.2359, 0.0733, 0.8441, 0.2988, 0.2073, 0.6296,\n",
            "        0.0879, 0.7292, 0.9173, 0.2195, 0.7781, 0.5295, 0.2066, 0.8683, 0.1681,\n",
            "        0.8702])]\n",
            "<class 'list'>\n",
            "data= tensor([[ 0.3653,  0.8310,  0.6386,  0.7896,  0.6743,  0.0742,  0.3951,  0.3101,\n",
            "          0.7066,  0.8433,  0.5777,  0.1289,  0.5197,  0.9888,  0.1075,  0.0138,\n",
            "          0.7749,  0.3798,  0.7840,  0.8762,  0.2247,  0.4057,  0.6874,  0.0893,\n",
            "          0.1996,  0.1330,  0.0044,  0.3746,  0.8993,  0.0046,  0.6925,  0.4251,\n",
            "          0.3979,  0.0614,  0.2374,  0.4627,  0.2866,  0.3820,  0.5714,  0.4110,\n",
            "          0.7983,  0.8640,  0.2982,  0.6485,  0.5137,  0.0486,  0.8338, -0.2452,\n",
            "          0.0426,  0.4993,  0.6006,  0.7800,  0.8981,  0.7792,  0.9890,  0.8634,\n",
            "         -0.0038,  0.6784,  0.6406,  0.0381,  0.2823,  0.3076,  0.8574,  0.0256]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor batch_idx, data in enumerate(training_loader):\\n    print(\"Batch Index:\", batch_idx)\\n    print(\"Data:\", data)\\n    print()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a TensorDataset from param_chunks_np\n",
        "dataset = TensorDataset(torch.from_numpy(param_chunks_np))\n",
        "\n",
        "# Set the batch size and other DataLoader parameters\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "pin_memory = True\n",
        "\n",
        "# Create the DataLoader using the custom dataset\n",
        "training_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
        "\n",
        "print(training_loader)\n",
        "\n",
        "print(iter(training_loader))\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "\n",
        "print(\"ori_data=\", data)\n",
        "\n",
        "print(type(next(iter(training_loader))))\n",
        "# print(data[0])\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "# for i in range(len(data)):\n",
        "#    data[i] = data[i].to(device)\n",
        "data = torch.stack(data).to(device)\n",
        "print(\"data=\", data)\n",
        "\n",
        "# There's no label in the NN weight dataset\n",
        "'''\n",
        "for batch_idx, data in enumerate(training_loader):\n",
        "    print(\"Batch Index:\", batch_idx)\n",
        "    print(\"Data:\", data)\n",
        "    print()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))\n",
        "\n",
        "cifar_training_loader = DataLoader(cifar_training_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True,\n",
        "                             pin_memory=True)\n",
        "\n",
        "print(cifar_training_loader)\n",
        "\n",
        "print(iter(cifar_training_loader))\n",
        "\n",
        "(data, _) = next(iter(cifar_training_loader))\n",
        "print(type(next(iter(cifar_training_loader))))\n",
        "print(type(data))\n",
        "print(type(_))\n",
        "print(\"ori_data=\", data)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "print(\"data=\", data)\n",
        "\n",
        "'''\n",
        "for batch_idx, (data, _) in enumerate(cifar_training_loader):\n",
        "    print(\"Batch Index:\", batch_idx)\n",
        "    # print(\"Data:\", data)\n",
        "    print(\"Label:\", _)\n",
        "    print()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yemq6mH1XYaz",
        "outputId": "04a53c4a-2284-4b8f-e2e3-e376b035b02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7bec4152d090>\n",
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7bec4152d0f0>\n",
            "<class 'list'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "ori_data= tensor([[[[-0.1196, -0.1118, -0.1157,  ..., -0.1039, -0.0843, -0.0843],\n",
            "          [ 0.0294,  0.0176,  0.0176,  ..., -0.0804, -0.0647, -0.0882],\n",
            "          [ 0.0647,  0.0490,  0.0451,  ..., -0.0843, -0.0882, -0.1039],\n",
            "          ...,\n",
            "          [-0.1627, -0.1706, -0.1706,  ..., -0.2137, -0.2373, -0.2804],\n",
            "          [-0.1980, -0.2020, -0.2059,  ..., -0.3039, -0.3235, -0.3510],\n",
            "          [-0.2373, -0.2451, -0.2451,  ..., -0.3863, -0.3824, -0.3863]],\n",
            "\n",
            "         [[-0.0765, -0.0765, -0.0804,  ..., -0.1039, -0.0843, -0.0804],\n",
            "          [ 0.1392,  0.1314,  0.1392,  ..., -0.0647, -0.0451, -0.0647],\n",
            "          [ 0.2333,  0.2216,  0.2255,  ..., -0.0843, -0.0882, -0.1000],\n",
            "          ...,\n",
            "          [-0.2333, -0.2412, -0.2412,  ..., -0.2608, -0.2843, -0.3275],\n",
            "          [-0.2686, -0.2725, -0.2765,  ..., -0.3392, -0.3549, -0.3824],\n",
            "          [-0.3039, -0.3078, -0.3118,  ..., -0.4098, -0.4059, -0.4098]],\n",
            "\n",
            "         [[-0.0412, -0.0412, -0.0451,  ..., -0.1392, -0.1275, -0.1353],\n",
            "          [ 0.3314,  0.3235,  0.3353,  ..., -0.0569, -0.0412, -0.0725],\n",
            "          [ 0.4843,  0.4765,  0.4843,  ..., -0.1235, -0.1353, -0.1588],\n",
            "          ...,\n",
            "          [-0.2490, -0.2569, -0.2569,  ..., -0.2608, -0.2843, -0.3314],\n",
            "          [-0.2843, -0.2882, -0.2922,  ..., -0.3235, -0.3431, -0.3706],\n",
            "          [-0.3078, -0.3157, -0.3196,  ..., -0.3902, -0.3902, -0.3902]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3118,  0.3275,  0.3157,  ...,  0.1039,  0.1353,  0.2804],\n",
            "          [ 0.2569,  0.2686,  0.2922,  ...,  0.2333,  0.1353,  0.2922],\n",
            "          [ 0.2686,  0.2451,  0.2569,  ...,  0.2961,  0.1824,  0.2529],\n",
            "          ...,\n",
            "          [ 0.1039,  0.0725,  0.0412,  ..., -0.0176,  0.0804,  0.1235],\n",
            "          [ 0.1039,  0.0804,  0.0176,  ...,  0.0882,  0.0529,  0.0098],\n",
            "          [ 0.0608,  0.0765,  0.0765,  ...,  0.1275,  0.1353,  0.0804]],\n",
            "\n",
            "         [[ 0.3745,  0.3902,  0.3824,  ...,  0.1392,  0.1588,  0.3118],\n",
            "          [ 0.3235,  0.3314,  0.3549,  ...,  0.2725,  0.1627,  0.3314],\n",
            "          [ 0.3314,  0.3078,  0.3196,  ...,  0.3431,  0.2216,  0.3039],\n",
            "          ...,\n",
            "          [ 0.2176,  0.1667,  0.1039,  ...,  0.0373,  0.1549,  0.1784],\n",
            "          [ 0.2373,  0.2137,  0.1157,  ...,  0.1510,  0.1353,  0.0961],\n",
            "          [ 0.1941,  0.2216,  0.1980,  ...,  0.2137,  0.2333,  0.2059]],\n",
            "\n",
            "         [[ 0.4255,  0.4451,  0.4294,  ...,  0.1706,  0.1941,  0.3784],\n",
            "          [ 0.3745,  0.3784,  0.4020,  ...,  0.3118,  0.2059,  0.4020],\n",
            "          [ 0.3863,  0.3588,  0.3706,  ...,  0.3863,  0.2725,  0.3863],\n",
            "          ...,\n",
            "          [ 0.3471,  0.2608,  0.2020,  ...,  0.1000,  0.2843,  0.3510],\n",
            "          [ 0.4020,  0.3471,  0.2333,  ...,  0.3235,  0.3078,  0.2725],\n",
            "          [ 0.3980,  0.4059,  0.3706,  ...,  0.3863,  0.3706,  0.3196]]],\n",
            "\n",
            "\n",
            "        [[[-0.3314, -0.3157, -0.3275,  ..., -0.2922, -0.3078, -0.2882],\n",
            "          [-0.3157, -0.2804, -0.2961,  ..., -0.2765, -0.3039, -0.2882],\n",
            "          [-0.3039, -0.2647, -0.2569,  ..., -0.2804, -0.2843, -0.2804],\n",
            "          ...,\n",
            "          [-0.1588, -0.2373, -0.2529,  ..., -0.2451, -0.1000, -0.2294],\n",
            "          [-0.2137, -0.2176, -0.2373,  ..., -0.2882, -0.1196, -0.2333],\n",
            "          [-0.2961, -0.2686, -0.2176,  ..., -0.3000, -0.1392, -0.2098]],\n",
            "\n",
            "         [[-0.2255, -0.1902, -0.2412,  ..., -0.2020, -0.1392, -0.0765],\n",
            "          [-0.2137, -0.1706, -0.2294,  ..., -0.1863, -0.1471, -0.0882],\n",
            "          [-0.2098, -0.1706, -0.1824,  ..., -0.1902, -0.1510, -0.0961],\n",
            "          ...,\n",
            "          [-0.0608, -0.1431, -0.1588,  ..., -0.2294, -0.0882, -0.1196],\n",
            "          [-0.1196, -0.1196, -0.1353,  ..., -0.2647, -0.1000, -0.1235],\n",
            "          [-0.2020, -0.1706, -0.1118,  ..., -0.2843, -0.1157, -0.1196]],\n",
            "\n",
            "         [[-0.2922, -0.2725, -0.3196,  ..., -0.2451, -0.2647, -0.2294],\n",
            "          [-0.2804, -0.2490, -0.3039,  ..., -0.2373, -0.2608, -0.2294],\n",
            "          [-0.2725, -0.2451, -0.2608,  ..., -0.2451, -0.2490, -0.2216],\n",
            "          ...,\n",
            "          [-0.1549, -0.2451, -0.2647,  ..., -0.3235, -0.1824, -0.2255],\n",
            "          [-0.2137, -0.2294, -0.2569,  ..., -0.3549, -0.1941, -0.2255],\n",
            "          [-0.2961, -0.2686, -0.2176,  ..., -0.3706, -0.2176, -0.2294]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1824, -0.1784, -0.2608,  ..., -0.2804, -0.3000, -0.2961],\n",
            "          [ 0.1745, -0.2569, -0.3314,  ..., -0.1902, -0.2647, -0.2765],\n",
            "          [ 0.2843, -0.0373, -0.1353,  ..., -0.1588, -0.2725, -0.3078],\n",
            "          ...,\n",
            "          [-0.0765, -0.0843, -0.1471,  ...,  0.0020,  0.0255, -0.0137],\n",
            "          [-0.1000,  0.0412, -0.0451,  ...,  0.0882,  0.0216, -0.0294],\n",
            "          [-0.1627, -0.0294,  0.0333,  ...,  0.0765,  0.0373,  0.0020]],\n",
            "\n",
            "         [[ 0.2137, -0.1353, -0.2020,  ..., -0.3784, -0.3667, -0.3431],\n",
            "          [ 0.1784, -0.2412, -0.3000,  ..., -0.2922, -0.3314, -0.3157],\n",
            "          [ 0.2804, -0.0255, -0.1078,  ..., -0.2765, -0.3471, -0.3431],\n",
            "          ...,\n",
            "          [-0.1549, -0.2333, -0.3078,  ..., -0.1000, -0.1118, -0.1627],\n",
            "          [-0.1902, -0.0961, -0.2059,  ..., -0.0569, -0.1157, -0.1510],\n",
            "          [-0.2608, -0.1549, -0.1196,  ..., -0.0686, -0.0608, -0.0569]],\n",
            "\n",
            "         [[-0.0451, -0.3588, -0.3863,  ..., -0.4451, -0.4176, -0.3941],\n",
            "          [-0.0686, -0.4216, -0.4451,  ..., -0.3863, -0.4098, -0.3902],\n",
            "          [ 0.0333, -0.2216, -0.3078,  ..., -0.3745, -0.4294, -0.4255],\n",
            "          ...,\n",
            "          [-0.2490, -0.2961, -0.3588,  ..., -0.2137, -0.2098, -0.2569],\n",
            "          [-0.2882, -0.1784, -0.2843,  ..., -0.1706, -0.2176, -0.2490],\n",
            "          [-0.3667, -0.2608, -0.2255,  ..., -0.1902, -0.1824, -0.1745]]],\n",
            "\n",
            "\n",
            "        [[[-0.4765, -0.4333, -0.4373,  ..., -0.0020,  0.0098,  0.0216],\n",
            "          [-0.4451, -0.4216, -0.4373,  ...,  0.0373,  0.0412,  0.0412],\n",
            "          [-0.4255, -0.4098, -0.4294,  ...,  0.0451,  0.0412,  0.0373],\n",
            "          ...,\n",
            "          [-0.4882, -0.4882, -0.4882,  ...,  0.1078,  0.1039,  0.1275],\n",
            "          [-0.4882, -0.4882, -0.4843,  ...,  0.0922,  0.0922,  0.1118],\n",
            "          [-0.4843, -0.4843, -0.4804,  ...,  0.0725,  0.0765,  0.0843]],\n",
            "\n",
            "         [[-0.4647, -0.4176, -0.4255,  ...,  0.0333,  0.0451,  0.0569],\n",
            "          [-0.4294, -0.4059, -0.4216,  ...,  0.0725,  0.0765,  0.0765],\n",
            "          [-0.4098, -0.3941, -0.4137,  ...,  0.0804,  0.0765,  0.0725],\n",
            "          ...,\n",
            "          [-0.4843, -0.4843, -0.4843,  ...,  0.1196,  0.1235,  0.1471],\n",
            "          [-0.4843, -0.4843, -0.4804,  ...,  0.1000,  0.1078,  0.1314],\n",
            "          [-0.4804, -0.4804, -0.4765,  ...,  0.0843,  0.0922,  0.1039]],\n",
            "\n",
            "         [[-0.4686, -0.4216, -0.4294,  ...,  0.0608,  0.0725,  0.0843],\n",
            "          [-0.4333, -0.4098, -0.4255,  ...,  0.1000,  0.1039,  0.1039],\n",
            "          [-0.4137, -0.3980, -0.4176,  ...,  0.1078,  0.1039,  0.1039],\n",
            "          ...,\n",
            "          [-0.5000, -0.5000, -0.5000,  ...,  0.1431,  0.1471,  0.1706],\n",
            "          [-0.5000, -0.5000, -0.5000,  ...,  0.1392,  0.1314,  0.1549],\n",
            "          [-0.5000, -0.5000, -0.4961,  ...,  0.1196,  0.1196,  0.1275]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4412,  0.3275,  0.2059,  ..., -0.0255, -0.0176, -0.0020],\n",
            "          [ 0.4725,  0.4059,  0.3078,  ..., -0.0333, -0.0137, -0.0020],\n",
            "          [ 0.4569,  0.4294,  0.3549,  ..., -0.0216, -0.0294, -0.0137],\n",
            "          ...,\n",
            "          [-0.0451, -0.0294, -0.0373,  ..., -0.0647, -0.0725, -0.0804],\n",
            "          [-0.0804, -0.0451, -0.0686,  ..., -0.1078, -0.1000, -0.0961],\n",
            "          [-0.1275, -0.1157, -0.0882,  ..., -0.1353, -0.1314, -0.1235]],\n",
            "\n",
            "         [[ 0.4412,  0.3196,  0.1980,  ..., -0.0373, -0.0294, -0.0137],\n",
            "          [ 0.4765,  0.4020,  0.2961,  ..., -0.0412, -0.0255, -0.0176],\n",
            "          [ 0.4569,  0.4255,  0.3431,  ..., -0.0216, -0.0373, -0.0294],\n",
            "          ...,\n",
            "          [-0.0686, -0.0569, -0.0647,  ..., -0.0843, -0.0922, -0.1000],\n",
            "          [-0.1118, -0.0686, -0.0961,  ..., -0.1275, -0.1196, -0.1196],\n",
            "          [-0.1588, -0.1431, -0.1196,  ..., -0.1588, -0.1510, -0.1510]],\n",
            "\n",
            "         [[ 0.4137,  0.2569,  0.1039,  ..., -0.1235, -0.1157, -0.1118],\n",
            "          [ 0.4529,  0.3667,  0.2255,  ..., -0.1235, -0.1157, -0.1196],\n",
            "          [ 0.4176,  0.3941,  0.2882,  ..., -0.1039, -0.1196, -0.1235],\n",
            "          ...,\n",
            "          [-0.1392, -0.1196, -0.1314,  ..., -0.1392, -0.1471, -0.1510],\n",
            "          [-0.1706, -0.1118, -0.1588,  ..., -0.1980, -0.1863, -0.1784],\n",
            "          [-0.2137, -0.1863, -0.1745,  ..., -0.2294, -0.2255, -0.2176]]]])\n",
            "data= tensor([[[[-0.1196, -0.1118, -0.1157,  ..., -0.1039, -0.0843, -0.0843],\n",
            "          [ 0.0294,  0.0176,  0.0176,  ..., -0.0804, -0.0647, -0.0882],\n",
            "          [ 0.0647,  0.0490,  0.0451,  ..., -0.0843, -0.0882, -0.1039],\n",
            "          ...,\n",
            "          [-0.1627, -0.1706, -0.1706,  ..., -0.2137, -0.2373, -0.2804],\n",
            "          [-0.1980, -0.2020, -0.2059,  ..., -0.3039, -0.3235, -0.3510],\n",
            "          [-0.2373, -0.2451, -0.2451,  ..., -0.3863, -0.3824, -0.3863]],\n",
            "\n",
            "         [[-0.0765, -0.0765, -0.0804,  ..., -0.1039, -0.0843, -0.0804],\n",
            "          [ 0.1392,  0.1314,  0.1392,  ..., -0.0647, -0.0451, -0.0647],\n",
            "          [ 0.2333,  0.2216,  0.2255,  ..., -0.0843, -0.0882, -0.1000],\n",
            "          ...,\n",
            "          [-0.2333, -0.2412, -0.2412,  ..., -0.2608, -0.2843, -0.3275],\n",
            "          [-0.2686, -0.2725, -0.2765,  ..., -0.3392, -0.3549, -0.3824],\n",
            "          [-0.3039, -0.3078, -0.3118,  ..., -0.4098, -0.4059, -0.4098]],\n",
            "\n",
            "         [[-0.0412, -0.0412, -0.0451,  ..., -0.1392, -0.1275, -0.1353],\n",
            "          [ 0.3314,  0.3235,  0.3353,  ..., -0.0569, -0.0412, -0.0725],\n",
            "          [ 0.4843,  0.4765,  0.4843,  ..., -0.1235, -0.1353, -0.1588],\n",
            "          ...,\n",
            "          [-0.2490, -0.2569, -0.2569,  ..., -0.2608, -0.2843, -0.3314],\n",
            "          [-0.2843, -0.2882, -0.2922,  ..., -0.3235, -0.3431, -0.3706],\n",
            "          [-0.3078, -0.3157, -0.3196,  ..., -0.3902, -0.3902, -0.3902]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3118,  0.3275,  0.3157,  ...,  0.1039,  0.1353,  0.2804],\n",
            "          [ 0.2569,  0.2686,  0.2922,  ...,  0.2333,  0.1353,  0.2922],\n",
            "          [ 0.2686,  0.2451,  0.2569,  ...,  0.2961,  0.1824,  0.2529],\n",
            "          ...,\n",
            "          [ 0.1039,  0.0725,  0.0412,  ..., -0.0176,  0.0804,  0.1235],\n",
            "          [ 0.1039,  0.0804,  0.0176,  ...,  0.0882,  0.0529,  0.0098],\n",
            "          [ 0.0608,  0.0765,  0.0765,  ...,  0.1275,  0.1353,  0.0804]],\n",
            "\n",
            "         [[ 0.3745,  0.3902,  0.3824,  ...,  0.1392,  0.1588,  0.3118],\n",
            "          [ 0.3235,  0.3314,  0.3549,  ...,  0.2725,  0.1627,  0.3314],\n",
            "          [ 0.3314,  0.3078,  0.3196,  ...,  0.3431,  0.2216,  0.3039],\n",
            "          ...,\n",
            "          [ 0.2176,  0.1667,  0.1039,  ...,  0.0373,  0.1549,  0.1784],\n",
            "          [ 0.2373,  0.2137,  0.1157,  ...,  0.1510,  0.1353,  0.0961],\n",
            "          [ 0.1941,  0.2216,  0.1980,  ...,  0.2137,  0.2333,  0.2059]],\n",
            "\n",
            "         [[ 0.4255,  0.4451,  0.4294,  ...,  0.1706,  0.1941,  0.3784],\n",
            "          [ 0.3745,  0.3784,  0.4020,  ...,  0.3118,  0.2059,  0.4020],\n",
            "          [ 0.3863,  0.3588,  0.3706,  ...,  0.3863,  0.2725,  0.3863],\n",
            "          ...,\n",
            "          [ 0.3471,  0.2608,  0.2020,  ...,  0.1000,  0.2843,  0.3510],\n",
            "          [ 0.4020,  0.3471,  0.2333,  ...,  0.3235,  0.3078,  0.2725],\n",
            "          [ 0.3980,  0.4059,  0.3706,  ...,  0.3863,  0.3706,  0.3196]]],\n",
            "\n",
            "\n",
            "        [[[-0.3314, -0.3157, -0.3275,  ..., -0.2922, -0.3078, -0.2882],\n",
            "          [-0.3157, -0.2804, -0.2961,  ..., -0.2765, -0.3039, -0.2882],\n",
            "          [-0.3039, -0.2647, -0.2569,  ..., -0.2804, -0.2843, -0.2804],\n",
            "          ...,\n",
            "          [-0.1588, -0.2373, -0.2529,  ..., -0.2451, -0.1000, -0.2294],\n",
            "          [-0.2137, -0.2176, -0.2373,  ..., -0.2882, -0.1196, -0.2333],\n",
            "          [-0.2961, -0.2686, -0.2176,  ..., -0.3000, -0.1392, -0.2098]],\n",
            "\n",
            "         [[-0.2255, -0.1902, -0.2412,  ..., -0.2020, -0.1392, -0.0765],\n",
            "          [-0.2137, -0.1706, -0.2294,  ..., -0.1863, -0.1471, -0.0882],\n",
            "          [-0.2098, -0.1706, -0.1824,  ..., -0.1902, -0.1510, -0.0961],\n",
            "          ...,\n",
            "          [-0.0608, -0.1431, -0.1588,  ..., -0.2294, -0.0882, -0.1196],\n",
            "          [-0.1196, -0.1196, -0.1353,  ..., -0.2647, -0.1000, -0.1235],\n",
            "          [-0.2020, -0.1706, -0.1118,  ..., -0.2843, -0.1157, -0.1196]],\n",
            "\n",
            "         [[-0.2922, -0.2725, -0.3196,  ..., -0.2451, -0.2647, -0.2294],\n",
            "          [-0.2804, -0.2490, -0.3039,  ..., -0.2373, -0.2608, -0.2294],\n",
            "          [-0.2725, -0.2451, -0.2608,  ..., -0.2451, -0.2490, -0.2216],\n",
            "          ...,\n",
            "          [-0.1549, -0.2451, -0.2647,  ..., -0.3235, -0.1824, -0.2255],\n",
            "          [-0.2137, -0.2294, -0.2569,  ..., -0.3549, -0.1941, -0.2255],\n",
            "          [-0.2961, -0.2686, -0.2176,  ..., -0.3706, -0.2176, -0.2294]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1824, -0.1784, -0.2608,  ..., -0.2804, -0.3000, -0.2961],\n",
            "          [ 0.1745, -0.2569, -0.3314,  ..., -0.1902, -0.2647, -0.2765],\n",
            "          [ 0.2843, -0.0373, -0.1353,  ..., -0.1588, -0.2725, -0.3078],\n",
            "          ...,\n",
            "          [-0.0765, -0.0843, -0.1471,  ...,  0.0020,  0.0255, -0.0137],\n",
            "          [-0.1000,  0.0412, -0.0451,  ...,  0.0882,  0.0216, -0.0294],\n",
            "          [-0.1627, -0.0294,  0.0333,  ...,  0.0765,  0.0373,  0.0020]],\n",
            "\n",
            "         [[ 0.2137, -0.1353, -0.2020,  ..., -0.3784, -0.3667, -0.3431],\n",
            "          [ 0.1784, -0.2412, -0.3000,  ..., -0.2922, -0.3314, -0.3157],\n",
            "          [ 0.2804, -0.0255, -0.1078,  ..., -0.2765, -0.3471, -0.3431],\n",
            "          ...,\n",
            "          [-0.1549, -0.2333, -0.3078,  ..., -0.1000, -0.1118, -0.1627],\n",
            "          [-0.1902, -0.0961, -0.2059,  ..., -0.0569, -0.1157, -0.1510],\n",
            "          [-0.2608, -0.1549, -0.1196,  ..., -0.0686, -0.0608, -0.0569]],\n",
            "\n",
            "         [[-0.0451, -0.3588, -0.3863,  ..., -0.4451, -0.4176, -0.3941],\n",
            "          [-0.0686, -0.4216, -0.4451,  ..., -0.3863, -0.4098, -0.3902],\n",
            "          [ 0.0333, -0.2216, -0.3078,  ..., -0.3745, -0.4294, -0.4255],\n",
            "          ...,\n",
            "          [-0.2490, -0.2961, -0.3588,  ..., -0.2137, -0.2098, -0.2569],\n",
            "          [-0.2882, -0.1784, -0.2843,  ..., -0.1706, -0.2176, -0.2490],\n",
            "          [-0.3667, -0.2608, -0.2255,  ..., -0.1902, -0.1824, -0.1745]]],\n",
            "\n",
            "\n",
            "        [[[-0.4765, -0.4333, -0.4373,  ..., -0.0020,  0.0098,  0.0216],\n",
            "          [-0.4451, -0.4216, -0.4373,  ...,  0.0373,  0.0412,  0.0412],\n",
            "          [-0.4255, -0.4098, -0.4294,  ...,  0.0451,  0.0412,  0.0373],\n",
            "          ...,\n",
            "          [-0.4882, -0.4882, -0.4882,  ...,  0.1078,  0.1039,  0.1275],\n",
            "          [-0.4882, -0.4882, -0.4843,  ...,  0.0922,  0.0922,  0.1118],\n",
            "          [-0.4843, -0.4843, -0.4804,  ...,  0.0725,  0.0765,  0.0843]],\n",
            "\n",
            "         [[-0.4647, -0.4176, -0.4255,  ...,  0.0333,  0.0451,  0.0569],\n",
            "          [-0.4294, -0.4059, -0.4216,  ...,  0.0725,  0.0765,  0.0765],\n",
            "          [-0.4098, -0.3941, -0.4137,  ...,  0.0804,  0.0765,  0.0725],\n",
            "          ...,\n",
            "          [-0.4843, -0.4843, -0.4843,  ...,  0.1196,  0.1235,  0.1471],\n",
            "          [-0.4843, -0.4843, -0.4804,  ...,  0.1000,  0.1078,  0.1314],\n",
            "          [-0.4804, -0.4804, -0.4765,  ...,  0.0843,  0.0922,  0.1039]],\n",
            "\n",
            "         [[-0.4686, -0.4216, -0.4294,  ...,  0.0608,  0.0725,  0.0843],\n",
            "          [-0.4333, -0.4098, -0.4255,  ...,  0.1000,  0.1039,  0.1039],\n",
            "          [-0.4137, -0.3980, -0.4176,  ...,  0.1078,  0.1039,  0.1039],\n",
            "          ...,\n",
            "          [-0.5000, -0.5000, -0.5000,  ...,  0.1431,  0.1471,  0.1706],\n",
            "          [-0.5000, -0.5000, -0.5000,  ...,  0.1392,  0.1314,  0.1549],\n",
            "          [-0.5000, -0.5000, -0.4961,  ...,  0.1196,  0.1196,  0.1275]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4412,  0.3275,  0.2059,  ..., -0.0255, -0.0176, -0.0020],\n",
            "          [ 0.4725,  0.4059,  0.3078,  ..., -0.0333, -0.0137, -0.0020],\n",
            "          [ 0.4569,  0.4294,  0.3549,  ..., -0.0216, -0.0294, -0.0137],\n",
            "          ...,\n",
            "          [-0.0451, -0.0294, -0.0373,  ..., -0.0647, -0.0725, -0.0804],\n",
            "          [-0.0804, -0.0451, -0.0686,  ..., -0.1078, -0.1000, -0.0961],\n",
            "          [-0.1275, -0.1157, -0.0882,  ..., -0.1353, -0.1314, -0.1235]],\n",
            "\n",
            "         [[ 0.4412,  0.3196,  0.1980,  ..., -0.0373, -0.0294, -0.0137],\n",
            "          [ 0.4765,  0.4020,  0.2961,  ..., -0.0412, -0.0255, -0.0176],\n",
            "          [ 0.4569,  0.4255,  0.3431,  ..., -0.0216, -0.0373, -0.0294],\n",
            "          ...,\n",
            "          [-0.0686, -0.0569, -0.0647,  ..., -0.0843, -0.0922, -0.1000],\n",
            "          [-0.1118, -0.0686, -0.0961,  ..., -0.1275, -0.1196, -0.1196],\n",
            "          [-0.1588, -0.1431, -0.1196,  ..., -0.1588, -0.1510, -0.1510]],\n",
            "\n",
            "         [[ 0.4137,  0.2569,  0.1039,  ..., -0.1235, -0.1157, -0.1118],\n",
            "          [ 0.4529,  0.3667,  0.2255,  ..., -0.1235, -0.1157, -0.1196],\n",
            "          [ 0.4176,  0.3941,  0.2882,  ..., -0.1039, -0.1196, -0.1235],\n",
            "          ...,\n",
            "          [-0.1392, -0.1196, -0.1314,  ..., -0.1392, -0.1471, -0.1510],\n",
            "          [-0.1706, -0.1118, -0.1588,  ..., -0.1980, -0.1863, -0.1784],\n",
            "          [-0.2137, -0.1863, -0.1745,  ..., -0.2294, -0.2255, -0.2176]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor batch_idx, (data, _) in enumerate(cifar_training_loader):\\n    print(\"Batch Index:\", batch_idx)\\n    # print(\"Data:\", data)\\n    print(\"Label:\", _)\\n    print()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTLi8nUrCi6"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Updated input size to [1, 64]\n",
        "        self._encoder = Encoder(1, num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "        # Replaced with nn.Linear\n",
        "        self._pre_vq_linear = nn.Linear(num_hiddens, embedding_dim)\n",
        "\n",
        "        if decay > 0.0:\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                              commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
        "\n",
        "        self._decoder = Decoder(embedding_dim,\n",
        "                                num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.unsqueeze(0)  # Add an extra dimension for batch size\n",
        "        ## print(\"x=\", x)\n",
        "        ## print(x.shape)\n",
        "        z = self._encoder(x)\n",
        "        z = z.view(z.size(0), -1)  # Flatten the tensor\n",
        "        ## print(\"z0=\", z)\n",
        "        # Replaced self._pre_vq_conv with self._pre_vq_linear\n",
        "        z = self._pre_vq_linear(z)\n",
        "        ## print(\"z1=\", z)\n",
        "\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        ## print(\"quantized:\", quantized)\n",
        "\n",
        "        # # Reshape quantized before passing it to the decoder\n",
        "        # quantized = quantized.unsqueeze(2).unsqueeze(3)\n",
        "        # print(\"quantized_after:\", quantized)\n",
        "        x_recon = self._decoder(quantized)\n",
        "\n",
        "        # x_recon = 0\n",
        "        return loss, x_recon, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGDClXkCrCi6"
      },
      "outputs": [],
      "source": [
        "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "              num_embeddings, embedding_dim,\n",
        "              commitment_cost, decay).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hRFXzlrCi6"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeKJtXg-rCi7",
        "outputId": "a01aef2f-3902-40c9-8351-5ebd75e6bb04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 iterations\n",
            "recon_error: 6457055.848\n",
            "perplexity: 1.000\n",
            "\n",
            "200 iterations\n",
            "recon_error: 192350.233\n",
            "perplexity: 1.000\n",
            "\n",
            "300 iterations\n",
            "recon_error: 146587.510\n",
            "perplexity: 1.000\n",
            "\n",
            "400 iterations\n",
            "recon_error: 121408.033\n",
            "perplexity: 1.000\n",
            "\n",
            "500 iterations\n",
            "recon_error: 101813.981\n",
            "perplexity: 1.000\n",
            "\n",
            "600 iterations\n",
            "recon_error: 93564.669\n",
            "perplexity: 1.000\n",
            "\n",
            "700 iterations\n",
            "recon_error: 92244.182\n",
            "perplexity: 1.000\n",
            "\n",
            "800 iterations\n",
            "recon_error: 89081.391\n",
            "perplexity: 1.000\n",
            "\n",
            "900 iterations\n",
            "recon_error: 94212.106\n",
            "perplexity: 1.000\n",
            "\n",
            "1000 iterations\n",
            "recon_error: 95659.677\n",
            "perplexity: 1.000\n",
            "\n",
            "1100 iterations\n",
            "recon_error: 94446.051\n",
            "perplexity: 1.000\n",
            "\n",
            "1200 iterations\n",
            "recon_error: 95878.895\n",
            "perplexity: 1.000\n",
            "\n",
            "1300 iterations\n",
            "recon_error: 95851.265\n",
            "perplexity: 1.000\n",
            "\n",
            "1400 iterations\n",
            "recon_error: 90210.002\n",
            "perplexity: 1.000\n",
            "\n",
            "1500 iterations\n",
            "recon_error: 89792.214\n",
            "perplexity: 1.000\n",
            "\n",
            "1600 iterations\n",
            "recon_error: 97843.034\n",
            "perplexity: 1.000\n",
            "\n",
            "1700 iterations\n",
            "recon_error: 85901.337\n",
            "perplexity: 1.000\n",
            "\n",
            "1800 iterations\n",
            "recon_error: 91055.397\n",
            "perplexity: 1.000\n",
            "\n",
            "1900 iterations\n",
            "recon_error: 88042.020\n",
            "perplexity: 1.000\n",
            "\n",
            "2000 iterations\n",
            "recon_error: 88119.926\n",
            "perplexity: 1.000\n",
            "\n",
            "2100 iterations\n",
            "recon_error: 89536.042\n",
            "perplexity: 1.000\n",
            "\n",
            "2200 iterations\n",
            "recon_error: 85437.225\n",
            "perplexity: 1.000\n",
            "\n",
            "2300 iterations\n",
            "recon_error: 85440.661\n",
            "perplexity: 1.000\n",
            "\n",
            "2400 iterations\n",
            "recon_error: 87694.808\n",
            "perplexity: 1.000\n",
            "\n",
            "2500 iterations\n",
            "recon_error: 83945.229\n",
            "perplexity: 1.000\n",
            "\n",
            "2600 iterations\n",
            "recon_error: 81220.437\n",
            "perplexity: 1.000\n",
            "\n",
            "2700 iterations\n",
            "recon_error: 84744.967\n",
            "perplexity: 1.000\n",
            "\n",
            "2800 iterations\n",
            "recon_error: 82442.799\n",
            "perplexity: 1.000\n",
            "\n",
            "2900 iterations\n",
            "recon_error: 78070.231\n",
            "perplexity: 1.000\n",
            "\n",
            "3000 iterations\n",
            "recon_error: 77726.417\n",
            "perplexity: 1.000\n",
            "\n",
            "3100 iterations\n",
            "recon_error: 83450.802\n",
            "perplexity: 1.000\n",
            "\n",
            "3200 iterations\n",
            "recon_error: 85227.858\n",
            "perplexity: 1.000\n",
            "\n",
            "3300 iterations\n",
            "recon_error: 79946.954\n",
            "perplexity: 1.000\n",
            "\n",
            "3400 iterations\n",
            "recon_error: 78974.284\n",
            "perplexity: 1.000\n",
            "\n",
            "3500 iterations\n",
            "recon_error: 76857.384\n",
            "perplexity: 1.000\n",
            "\n",
            "3600 iterations\n",
            "recon_error: 76018.261\n",
            "perplexity: 1.000\n",
            "\n",
            "3700 iterations\n",
            "recon_error: 77301.295\n",
            "perplexity: 1.000\n",
            "\n",
            "3800 iterations\n",
            "recon_error: 82839.099\n",
            "perplexity: 1.000\n",
            "\n",
            "3900 iterations\n",
            "recon_error: 77224.870\n",
            "perplexity: 1.000\n",
            "\n",
            "4000 iterations\n",
            "recon_error: 75833.014\n",
            "perplexity: 1.000\n",
            "\n",
            "4100 iterations\n",
            "recon_error: 79823.203\n",
            "perplexity: 1.000\n",
            "\n",
            "4200 iterations\n",
            "recon_error: 76903.990\n",
            "perplexity: 1.000\n",
            "\n",
            "4300 iterations\n",
            "recon_error: 77961.062\n",
            "perplexity: 1.000\n",
            "\n",
            "4400 iterations\n",
            "recon_error: 75665.674\n",
            "perplexity: 1.000\n",
            "\n",
            "4500 iterations\n",
            "recon_error: 77077.994\n",
            "perplexity: 1.000\n",
            "\n",
            "4600 iterations\n",
            "recon_error: 75467.875\n",
            "perplexity: 1.000\n",
            "\n",
            "4700 iterations\n",
            "recon_error: 74116.853\n",
            "perplexity: 1.000\n",
            "\n",
            "4800 iterations\n",
            "recon_error: 77738.582\n",
            "perplexity: 1.000\n",
            "\n",
            "4900 iterations\n",
            "recon_error: 74788.682\n",
            "perplexity: 1.000\n",
            "\n",
            "5000 iterations\n",
            "recon_error: 72172.082\n",
            "perplexity: 1.000\n",
            "\n",
            "5100 iterations\n",
            "recon_error: 73675.936\n",
            "perplexity: 1.000\n",
            "\n",
            "5200 iterations\n",
            "recon_error: 76324.591\n",
            "perplexity: 1.000\n",
            "\n",
            "5300 iterations\n",
            "recon_error: 75576.807\n",
            "perplexity: 1.000\n",
            "\n",
            "5400 iterations\n",
            "recon_error: 72893.001\n",
            "perplexity: 1.000\n",
            "\n",
            "5500 iterations\n",
            "recon_error: 72450.081\n",
            "perplexity: 1.000\n",
            "\n",
            "5600 iterations\n",
            "recon_error: 74133.708\n",
            "perplexity: 1.000\n",
            "\n",
            "5700 iterations\n",
            "recon_error: 75260.254\n",
            "perplexity: 1.000\n",
            "\n",
            "5800 iterations\n",
            "recon_error: 74050.209\n",
            "perplexity: 1.000\n",
            "\n",
            "5900 iterations\n",
            "recon_error: 72884.196\n",
            "perplexity: 1.000\n",
            "\n",
            "6000 iterations\n",
            "recon_error: 72416.695\n",
            "perplexity: 1.000\n",
            "\n",
            "6100 iterations\n",
            "recon_error: 73258.960\n",
            "perplexity: 1.000\n",
            "\n",
            "6200 iterations\n",
            "recon_error: 71818.425\n",
            "perplexity: 1.000\n",
            "\n",
            "6300 iterations\n",
            "recon_error: 70847.932\n",
            "perplexity: 1.000\n",
            "\n",
            "6400 iterations\n",
            "recon_error: 72318.719\n",
            "perplexity: 1.000\n",
            "\n",
            "6500 iterations\n",
            "recon_error: 71960.512\n",
            "perplexity: 1.000\n",
            "\n",
            "6600 iterations\n",
            "recon_error: 71682.572\n",
            "perplexity: 1.000\n",
            "\n",
            "6700 iterations\n",
            "recon_error: 70751.252\n",
            "perplexity: 1.000\n",
            "\n",
            "6800 iterations\n",
            "recon_error: 70180.250\n",
            "perplexity: 1.000\n",
            "\n",
            "6900 iterations\n",
            "recon_error: 72218.592\n",
            "perplexity: 1.000\n",
            "\n",
            "7000 iterations\n",
            "recon_error: 70015.286\n",
            "perplexity: 1.000\n",
            "\n",
            "7100 iterations\n",
            "recon_error: 71279.589\n",
            "perplexity: 1.000\n",
            "\n",
            "7200 iterations\n",
            "recon_error: 69855.436\n",
            "perplexity: 1.000\n",
            "\n",
            "7300 iterations\n",
            "recon_error: 69977.510\n",
            "perplexity: 1.000\n",
            "\n",
            "7400 iterations\n",
            "recon_error: 70726.438\n",
            "perplexity: 1.000\n",
            "\n",
            "7500 iterations\n",
            "recon_error: 70197.634\n",
            "perplexity: 1.000\n",
            "\n",
            "7600 iterations\n",
            "recon_error: 69185.215\n",
            "perplexity: 1.000\n",
            "\n",
            "7700 iterations\n",
            "recon_error: 72744.055\n",
            "perplexity: 1.000\n",
            "\n",
            "7800 iterations\n",
            "recon_error: 71420.054\n",
            "perplexity: 1.000\n",
            "\n",
            "7900 iterations\n",
            "recon_error: 70558.370\n",
            "perplexity: 1.000\n",
            "\n",
            "8000 iterations\n",
            "recon_error: 69899.890\n",
            "perplexity: 1.000\n",
            "\n",
            "8100 iterations\n",
            "recon_error: 72658.073\n",
            "perplexity: 1.000\n",
            "\n",
            "8200 iterations\n",
            "recon_error: 69747.844\n",
            "perplexity: 1.000\n",
            "\n",
            "8300 iterations\n",
            "recon_error: 68040.896\n",
            "perplexity: 1.000\n",
            "\n",
            "8400 iterations\n",
            "recon_error: 70883.834\n",
            "perplexity: 1.000\n",
            "\n",
            "8500 iterations\n",
            "recon_error: 70251.554\n",
            "perplexity: 1.000\n",
            "\n",
            "8600 iterations\n",
            "recon_error: 71564.391\n",
            "perplexity: 1.000\n",
            "\n",
            "8700 iterations\n",
            "recon_error: 70808.432\n",
            "perplexity: 1.000\n",
            "\n",
            "8800 iterations\n",
            "recon_error: 70952.211\n",
            "perplexity: 1.000\n",
            "\n",
            "8900 iterations\n",
            "recon_error: 74809.983\n",
            "perplexity: 1.000\n",
            "\n",
            "9000 iterations\n",
            "recon_error: 70826.069\n",
            "perplexity: 1.000\n",
            "\n",
            "9100 iterations\n",
            "recon_error: 70987.265\n",
            "perplexity: 1.000\n",
            "\n",
            "9200 iterations\n",
            "recon_error: 70644.013\n",
            "perplexity: 1.000\n",
            "\n",
            "9300 iterations\n",
            "recon_error: 70182.129\n",
            "perplexity: 1.000\n",
            "\n",
            "9400 iterations\n",
            "recon_error: 69503.700\n",
            "perplexity: 1.000\n",
            "\n",
            "9500 iterations\n",
            "recon_error: 69522.862\n",
            "perplexity: 1.000\n",
            "\n",
            "9600 iterations\n",
            "recon_error: 70631.377\n",
            "perplexity: 1.000\n",
            "\n",
            "9700 iterations\n",
            "recon_error: 70149.916\n",
            "perplexity: 1.000\n",
            "\n",
            "9800 iterations\n",
            "recon_error: 70671.762\n",
            "perplexity: 1.000\n",
            "\n",
            "9900 iterations\n",
            "recon_error: 70044.409\n",
            "perplexity: 1.000\n",
            "\n",
            "10000 iterations\n",
            "recon_error: 70157.003\n",
            "perplexity: 1.000\n",
            "\n",
            "10100 iterations\n",
            "recon_error: 71522.489\n",
            "perplexity: 1.000\n",
            "\n",
            "10200 iterations\n",
            "recon_error: 71317.485\n",
            "perplexity: 1.000\n",
            "\n",
            "10300 iterations\n",
            "recon_error: 69671.763\n",
            "perplexity: 1.000\n",
            "\n",
            "10400 iterations\n",
            "recon_error: 69901.805\n",
            "perplexity: 1.000\n",
            "\n",
            "10500 iterations\n",
            "recon_error: 71548.672\n",
            "perplexity: 1.000\n",
            "\n",
            "10600 iterations\n",
            "recon_error: 72041.960\n",
            "perplexity: 1.000\n",
            "\n",
            "10700 iterations\n",
            "recon_error: 70389.022\n",
            "perplexity: 1.000\n",
            "\n",
            "10800 iterations\n",
            "recon_error: 70572.445\n",
            "perplexity: 1.000\n",
            "\n",
            "10900 iterations\n",
            "recon_error: 69191.443\n",
            "perplexity: 1.000\n",
            "\n",
            "11000 iterations\n",
            "recon_error: 71918.309\n",
            "perplexity: 1.000\n",
            "\n",
            "11100 iterations\n",
            "recon_error: 70837.526\n",
            "perplexity: 1.000\n",
            "\n",
            "11200 iterations\n",
            "recon_error: 70334.735\n",
            "perplexity: 1.000\n",
            "\n",
            "11300 iterations\n",
            "recon_error: 70926.208\n",
            "perplexity: 1.000\n",
            "\n",
            "11400 iterations\n",
            "recon_error: 71985.696\n",
            "perplexity: 1.000\n",
            "\n",
            "11500 iterations\n",
            "recon_error: 70090.578\n",
            "perplexity: 1.000\n",
            "\n",
            "11600 iterations\n",
            "recon_error: 69749.742\n",
            "perplexity: 1.000\n",
            "\n",
            "11700 iterations\n",
            "recon_error: 69810.584\n",
            "perplexity: 1.000\n",
            "\n",
            "11800 iterations\n",
            "recon_error: 69121.147\n",
            "perplexity: 1.000\n",
            "\n",
            "11900 iterations\n",
            "recon_error: 69278.240\n",
            "perplexity: 1.000\n",
            "\n",
            "12000 iterations\n",
            "recon_error: 72009.767\n",
            "perplexity: 1.000\n",
            "\n",
            "12100 iterations\n",
            "recon_error: 70835.185\n",
            "perplexity: 1.000\n",
            "\n",
            "12200 iterations\n",
            "recon_error: 72739.658\n",
            "perplexity: 1.000\n",
            "\n",
            "12300 iterations\n",
            "recon_error: 69325.516\n",
            "perplexity: 1.000\n",
            "\n",
            "12400 iterations\n",
            "recon_error: 70982.499\n",
            "perplexity: 1.000\n",
            "\n",
            "12500 iterations\n",
            "recon_error: 69781.493\n",
            "perplexity: 1.000\n",
            "\n",
            "12600 iterations\n",
            "recon_error: 68741.126\n",
            "perplexity: 1.000\n",
            "\n",
            "12700 iterations\n",
            "recon_error: 68038.181\n",
            "perplexity: 1.000\n",
            "\n",
            "12800 iterations\n",
            "recon_error: 70181.763\n",
            "perplexity: 1.000\n",
            "\n",
            "12900 iterations\n",
            "recon_error: 70429.778\n",
            "perplexity: 1.000\n",
            "\n",
            "13000 iterations\n",
            "recon_error: 72377.522\n",
            "perplexity: 1.000\n",
            "\n",
            "13100 iterations\n",
            "recon_error: 70886.698\n",
            "perplexity: 1.000\n",
            "\n",
            "13200 iterations\n",
            "recon_error: 68839.257\n",
            "perplexity: 1.000\n",
            "\n",
            "13300 iterations\n",
            "recon_error: 70124.091\n",
            "perplexity: 1.000\n",
            "\n",
            "13400 iterations\n",
            "recon_error: 69772.612\n",
            "perplexity: 1.000\n",
            "\n",
            "13500 iterations\n",
            "recon_error: 69095.222\n",
            "perplexity: 1.000\n",
            "\n",
            "13600 iterations\n",
            "recon_error: 69920.097\n",
            "perplexity: 1.000\n",
            "\n",
            "13700 iterations\n",
            "recon_error: 69090.066\n",
            "perplexity: 1.000\n",
            "\n",
            "13800 iterations\n",
            "recon_error: 69494.179\n",
            "perplexity: 1.000\n",
            "\n",
            "13900 iterations\n",
            "recon_error: 68656.171\n",
            "perplexity: 1.000\n",
            "\n",
            "14000 iterations\n",
            "recon_error: 70950.570\n",
            "perplexity: 1.000\n",
            "\n",
            "14100 iterations\n",
            "recon_error: 67478.412\n",
            "perplexity: 1.000\n",
            "\n",
            "14200 iterations\n",
            "recon_error: 68228.869\n",
            "perplexity: 1.000\n",
            "\n",
            "14300 iterations\n",
            "recon_error: 67344.421\n",
            "perplexity: 1.000\n",
            "\n",
            "14400 iterations\n",
            "recon_error: 68618.658\n",
            "perplexity: 1.000\n",
            "\n",
            "14500 iterations\n",
            "recon_error: 72141.561\n",
            "perplexity: 1.000\n",
            "\n",
            "14600 iterations\n",
            "recon_error: 69139.875\n",
            "perplexity: 1.000\n",
            "\n",
            "14700 iterations\n",
            "recon_error: 68892.756\n",
            "perplexity: 1.000\n",
            "\n",
            "14800 iterations\n",
            "recon_error: 69721.398\n",
            "perplexity: 1.000\n",
            "\n",
            "14900 iterations\n",
            "recon_error: 69100.835\n",
            "perplexity: 1.000\n",
            "\n",
            "15000 iterations\n",
            "recon_error: 69306.358\n",
            "perplexity: 1.000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "train_res_recon_error = []\n",
        "train_res_perplexity = []\n",
        "\n",
        "for i in xrange(num_training_updates):\n",
        "    # (data, _) = next(iter(training_loader))\n",
        "    # data = data.to(device)\n",
        "    data = next(iter(training_loader))\n",
        "    data = torch.stack(data).to(device)\n",
        "    #for i in range(len(data)):\n",
        "    #  data[i] = data[i].to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    vq_loss, data_recon, perplexity = model(data)\n",
        "    recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "    loss = recon_error + vq_loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_res_recon_error.append(recon_error.item())\n",
        "    train_res_perplexity.append(perplexity.item())\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "        print('%d iterations' % (i+1))\n",
        "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
        "        % print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*The reason of the error:*\n",
        "\n",
        "When using the ReLU activation function, if the input of a neuron is negative, it causes the neuron to output zero constantly, resulting in deactivation. Since the gradient is zero at this point, it cannot recover.\n",
        "\n"
      ],
      "metadata": {
        "id": "6-vxEsMNKzpW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szkLa9JcrCi7"
      },
      "source": [
        "## Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__AOkue0rCi7"
      },
      "outputs": [],
      "source": [
        "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
        "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sp67spXrCi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "outputId": "4f8ade0e-6038-4519-ef8e-508766c56f46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nax = f.add_subplot(1,2,2)\\nax.plot(train_res_perplexity_smooth)\\nax.set_title('Smoothed Average codebook usage (perplexity).')\\nax.set_xlabel('iteration')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "f = plt.figure(figsize=(16,8))\n",
        "ax = f.add_subplot(1,2,1)\n",
        "\n",
        "ax.plot(train_res_recon_error_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed NMSE.')\n",
        "ax.set_xlabel('iteration')\n",
        "'''\n",
        "ax = f.add_subplot(1,2,2)\n",
        "ax.plot(train_res_perplexity_smooth)\n",
        "ax.set_title('Smoothed Average codebook usage (perplexity).')\n",
        "ax.set_xlabel('iteration')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMbb0fPrCi7"
      },
      "source": [
        "## View Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnVCwvK8rCi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b352b098-38f6-4e81-93c9-3f2b7b01ab7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vq_output_eval= tensor([[ 4.2000e+00,  3.4573e-01, -6.0922e-03,  4.9490e+00,  5.8922e+00,\n",
            "         -3.4437e+00, -4.6094e+00,  3.3776e+00, -2.6236e+00, -4.9217e+00,\n",
            "          3.7969e+00,  1.7772e+00,  4.1331e+00,  5.3000e+00,  4.5955e+00,\n",
            "          1.4478e+00, -1.7131e-01, -3.7096e+00, -2.9014e+00, -6.2335e+00,\n",
            "         -9.5910e+00,  6.6834e+00,  2.3847e+00,  2.9382e+00, -2.6834e+00,\n",
            "          5.3260e-02,  1.3684e+00, -4.0423e+00,  1.0650e+00, -6.5871e+00,\n",
            "         -2.0515e+00,  2.5385e+00, -1.0978e+00,  5.2365e+00,  4.2382e+00,\n",
            "          2.3444e+00, -1.2222e+00, -1.8329e+00, -3.4228e+00, -3.7165e+00,\n",
            "         -1.3708e+00, -4.7944e+00, -9.4455e-01,  3.5895e+00,  9.3222e-01,\n",
            "         -1.0197e+00,  9.4478e-01,  2.8057e+00,  2.4831e+00, -8.2739e-01,\n",
            "          1.0693e+00, -3.8591e+00,  2.5269e+00,  1.6218e+00,  4.7415e+00,\n",
            "          2.3302e+00, -4.7821e+00,  4.8902e+00, -2.2899e-01, -2.6762e+00,\n",
            "         -2.1920e-01, -1.8884e+00,  4.0961e+00,  9.6069e+00]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "vq_output_eval.shape= torch.Size([1, 64])\n",
            "tensor([[ 0.6332,  0.0875,  1.0996, -0.9334,  3.0431, -1.1957, -2.2209,  1.4962,\n",
            "          0.7201,  2.1730,  0.9013, -0.4555,  0.9455,  0.5387,  1.4553,  1.7930,\n",
            "          1.1250,  2.1223, -0.2571, -2.1093, -2.6673,  3.6323,  2.1947,  1.4848,\n",
            "         -1.4343, -1.6951,  0.2334, -1.3048, -0.0433, -0.2573, -0.6028, -0.0987,\n",
            "          2.0513, -0.2212,  3.0067, -0.9002,  0.6948,  0.5545, -1.8902, -0.6016,\n",
            "          2.5929,  0.4604,  1.3241, -0.4880,  1.6555,  0.9654, -0.2983,  2.1307,\n",
            "         -2.9652,  1.1361, -2.4624, -1.4435,  0.5898,  0.4589, -1.4996, -0.5316,\n",
            "         -1.3739,  1.9687, -1.0509, -1.2524,  0.2677, -1.4869,  2.1978,  3.0155]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "valid_quantize= tensor([[ 0.6332,  0.0875,  1.0996, -0.9334,  3.0431, -1.1957, -2.2209,  1.4962,\n",
            "          0.7201,  2.1730,  0.9013, -0.4555,  0.9455,  0.5387,  1.4553,  1.7930,\n",
            "          1.1250,  2.1223, -0.2571, -2.1093, -2.6673,  3.6323,  2.1947,  1.4848,\n",
            "         -1.4343, -1.6951,  0.2334, -1.3048, -0.0433, -0.2573, -0.6028, -0.0987,\n",
            "          2.0513, -0.2212,  3.0067, -0.9002,  0.6948,  0.5545, -1.8902, -0.6016,\n",
            "          2.5929,  0.4604,  1.3241, -0.4880,  1.6555,  0.9654, -0.2983,  2.1307,\n",
            "         -2.9652,  1.1361, -2.4624, -1.4435,  0.5898,  0.4589, -1.4996, -0.5316,\n",
            "         -1.3739,  1.9687, -1.0509, -1.2524,  0.2677, -1.4869,  2.1978,  3.0155]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "valid_quantize.shape= torch.Size([1, 64])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "data = next(iter(training_loader))\n",
        "train_originals = torch.stack(data).to(device)\n",
        "vq_output_eval = model._pre_vq_linear(model._encoder(train_originals))\n",
        "print(\"vq_output_eval=\", vq_output_eval)\n",
        "print(\"vq_output_eval.shape=\", vq_output_eval.shape)\n",
        "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
        "print(valid_quantize)\n",
        "print(\"valid_quantize=\", valid_quantize)\n",
        "print(\"valid_quantize.shape=\", valid_quantize.shape)\n",
        "# (train_originals, _) = next(iter(training_loader))\n",
        "# train_originals = train_originals.to(device)\n",
        "valid_reconstructions = model._decoder(valid_quantize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RF5noYrCi8"
      },
      "outputs": [],
      "source": [
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VAi9TanrCi8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "a9c9eb6b-50b8-49c3-b644-2283554712ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAbCAYAAADhwYyIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAC1ElEQVR4nO3cwWriUBTG8WPVorWpuhNRxJ1v0fd/A3ciggjiQqMoas0sBmcxkPNlenV05v5/q8JpYrw5iV/LiaUsyzIDAADRenn0AQAAgMciDAAAEDnCAAAAkSMMAAAQOcIAAACRIwwAABA5wgAAAJGrFPmly+Vi8/nckiSxUql072MCAAA3kGWZpWlq3W7XXl7y//4vFAbm87n1+/2bHRwAAPh7ZrOZ9Xq93HqhMJAkiZmZTSaTXz//brlc5m7farXc/a9WK7f+/v7+7Xq1WnW3nU6nbn0wGLh1z3g8duuj0citHw4Ht16r1f74mK7SNHXreef5yvviyvP57G673W7dunpf9XrdrXtOp5NbV/3iHbs6X2pN1bp42u22W9/v92690Wi49dlsllvrdDrutvdcF9UL6nyrXvP6XK1puVx268rr62tuLfQ/tJvN5tvbfnx8BL32brdz6977VuezUvE/0tS96e3tLbe2WCzcbdU1pPrBe23v89XMbL1e59a22619fn7K66xQGLg2XpIkuY3gXfCqeY7Ho1tXbyIkDKh9hzS+CjFq395FYRYWBtTN5J5hwPtXldlzhwHv2NW26nyH3ODVvtWxqRuZ1w+hfax6zVvzR4YBtabPHAZChIYBtS7PGgZUiFH3+5AwoAL119eXWzfTPcMAIQAAkSMMAAAQOcIAAACRIwwAABC5QgOE1yEabwLdq6nBCTXZ7g3xmP38HoQ8ashHvXbI1K2aDlf7VkMjavDSE7rm93yaQL0vNUQUsu09nyZQQp4mUINTavJdDSB5/aKGD9W6qF7z1kWdT1VXvcbTBLf3zE8TeHV1z/Q+h8x0P4S8tneNXGvqOisUBq4HMhwOi/w6AAB4ImmaWrPZzK2XMhUXjG8gBADgX1T0GwgLhQEAAPD/YoAQAIDIEQYAAIgcYQAAgMgRBgAAiBxhAACAyBEGAACIHGEAAIDI/QAQ8DXrh+jBQQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show(make_grid(valid_reconstructions.cpu().data)+0.5, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J620xHA6rCi8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "ac2aa869-3ce4-4c00-9d49-2b99f60fdef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAbCAYAAADhwYyIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAC/klEQVR4nO3cvU4qQRjG8UHUKAqhJtJzAcZSvQNjZUtpYWNDwi1YcgX20tBTaIKJegfGwkJDYmEj+BE1rMUJVmeed8O4Bzzz/7XPvjuzrLv7GobNJUmSOAAAEK25aU8AAABMF80AAACRoxkAACByNAMAAESOZgAAgMjRDAAAEDmaAQAAIjefZqPRaOT6/b4rFosul8tlPScAAPADkiRxg8HAVSoVNzfn//8/VTPQ7/ddtVr9sckBAIB/5+7uzq2trXnzVM1AsVj83lmpVPrrNldXV976jY0Nuf/Pz0+Zz8/raT49PXmzt7c3WXt+fi7z3d1dmXc6HW+2s7Mjay0nJycyv7+/92aHh4ey9uzsTOZbW1syn1VHR0cybzQamY3d7XZl/vj4KPO9vT2Zt1otb7a4uChr9/f3ZR7i4uJC5uVyWea1Wk3m19fX3uzm5kbW3t7eynw4HMq82Wx6s16vJ2vX19dlvrS0JPMQp6enMt/e3pb5w8ODN7PO5+Xlpcw3NzdlnqXj42OZ1+v1zMZ+fX2V+fLy8sT7brfbctyDg4Pv57hPqmZg/NVAqVTyNgMrKyveel/NWGgzoCwsLMi8UCjI3Jq7qrdqLdbc1M3EGludrzT1s8q6wWZ5XNZnat0MrLmpY7OagWke9+rqqsytuan6kGvEOfveo+YWeg1l2QyEzu3l5WXi2lm+t1gP3CznZj2LQpoB6zpwzplf8bOAEACAyNEMAAAQOZoBAAAiRzMAAEDkUq3MS5LEOadX7T8/P3szVefcdH9NoBbKWPu26q1aizU3dWzW2Op8pamfVdb5zvK4rM809G9NHdtoNAradwjruK2FU9bc1Ir/kGskTT7pPc+qdc659/d3mYcIndtgMPBm+Xw+07GzZC3izXJu1tgfHx8T71tdB+Nxx89xn1xibeH+/ISN9wwAAPA7We8ZSNUM8AZCAAB+n7RvIEzVDAAAgP8XCwgBAIgczQAAAJGjGQAAIHI0AwAARI5mAACAyNEMAAAQOZoBAAAi9wVrCBtCwEPSHQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show(make_grid(train_originals.cpu()+0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqs594NmrCi8"
      },
      "source": [
        "## View Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW0ZlSrHrCi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96348310-3f22-4893-d2fa-6e50ecc42827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping umap as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=29e94e255f4601a597809a1cb7e766de5f620d4706d405df062173e663dda1d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=4e7ff888694b1e0899dcc8a97b676d90cffaec2c31e37411b25d52953099cf2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "! pip uninstall umap\n",
        "! pip install umap-learn\n",
        "\n",
        "import umap.umap_ as umap\n",
        "\n",
        "proj = umap.UMAP(n_neighbors=3,\n",
        "                 min_dist=0.1,\n",
        "                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNXVBcWCrCi9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "5d25aa1c-123c-4ee2-872f-dc8c86b87663"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7bebcb34d1e0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWak2D-rCi9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX version"
      ],
      "metadata": {
        "id": "XdJN_axiTU07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Haiku implementation of VQ-VAE https://arxiv.org/abs/1711.00937.\"\"\"\n",
        "\n",
        "from typing import Any, Optional\n",
        "\n",
        "from haiku._src import base\n",
        "from haiku._src import initializers\n",
        "from haiku._src import module\n",
        "from haiku._src import moving_averages\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "# If you are forking replace this with `import haiku as hk`.\n",
        "# pylint: disable=invalid-name\n",
        "class hk:\n",
        "  get_parameter = base.get_parameter\n",
        "  get_state = base.get_state\n",
        "  set_state = base.set_state\n",
        "  initializers = initializers\n",
        "  ExponentialMovingAverage = moving_averages.ExponentialMovingAverage\n",
        "  Module = module.Module\n",
        "# pylint: enable=invalid-name\n",
        "del base, initializers, module, moving_averages\n",
        "\n",
        "\n",
        "class VectorQuantizer(hk.Module):\n",
        "  \"\"\"Haiku module representing the VQ-VAE layer.\n",
        "\n",
        "  Implements the algorithm presented in\n",
        "  \"Neural Discrete Representation Learning\" by van den Oord et al.\n",
        "  https://arxiv.org/abs/1711.00937\n",
        "\n",
        "  Input any tensor to be quantized. Last dimension will be used as space in\n",
        "  which to quantize. All other dimensions will be flattened and will be seen\n",
        "  as different examples to quantize.\n",
        "\n",
        "  The output tensor will have the same shape as the input.\n",
        "\n",
        "  For example a tensor with shape ``[16, 32, 32, 64]`` will be reshaped into\n",
        "  ``[16384, 64]`` and all ``16384`` vectors (each of ``64`` dimensions)  will be\n",
        "  quantized independently.\n",
        "\n",
        "  Attributes:\n",
        "    embedding_dim: integer representing the dimensionality of the tensors in the\n",
        "      quantized space. Inputs to the modules must be in this format as well.\n",
        "    num_embeddings: integer, the number of vectors in the quantized space.\n",
        "    commitment_cost: scalar which controls the weighting of the loss terms (see\n",
        "      equation 4 in the paper - this variable is Beta).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dim: int,\n",
        "      num_embeddings: int,\n",
        "      commitment_cost: float,\n",
        "      dtype: Any = jnp.float32,\n",
        "      name: Optional[str] = None,\n",
        "      cross_replica_axis: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Initializes a VQ-VAE module.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: dimensionality of the tensors in the quantized space.\n",
        "        Inputs to the modules must be in this format as well.\n",
        "      num_embeddings: number of vectors in the quantized space.\n",
        "      commitment_cost: scalar which controls the weighting of the loss terms\n",
        "        (see equation 4 in the paper - this variable is Beta).\n",
        "      dtype: dtype for the embeddings variable, defaults to ``float32``.\n",
        "      name: name of the module.\n",
        "      cross_replica_axis: If not ``None``, it should be a string representing\n",
        "        the axis name over which this module is being run within a\n",
        "        :func:`jax.pmap`. Supplying this argument means that perplexity is\n",
        "        calculated across all replicas on that axis.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.commitment_cost = commitment_cost\n",
        "    self.cross_replica_axis = cross_replica_axis\n",
        "\n",
        "    self._embedding_shape = [embedding_dim, num_embeddings]\n",
        "    self._embedding_dtype = dtype\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    initializer = hk.initializers.VarianceScaling(distribution=\"uniform\")\n",
        "    return hk.get_parameter(\n",
        "        \"embeddings\",\n",
        "        self._embedding_shape,\n",
        "        self._embedding_dtype,\n",
        "        init=initializer)\n",
        "\n",
        "  def __call__(self, inputs, is_training):\n",
        "    \"\"\"Connects the module to some inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Tensor, final dimension must be equal to ``embedding_dim``. All\n",
        "        other leading dimensions will be flattened and treated as a large batch.\n",
        "      is_training: boolean, whether this connection is to training data.\n",
        "\n",
        "    Returns:\n",
        "      dict: Dictionary containing the following keys and values:\n",
        "        * ``quantize``: Tensor containing the quantized version of the input.\n",
        "        * ``loss``: Tensor containing the loss to optimize.\n",
        "        * ``perplexity``: Tensor containing the perplexity of the encodings.\n",
        "        * ``encodings``: Tensor containing the discrete encodings, ie which\n",
        "          element of the quantized space each input element was mapped to.\n",
        "        * ``encoding_indices``: Tensor containing the discrete encoding indices,\n",
        "          ie which element of the quantized space each input element was mapped\n",
        "          to.\n",
        "    \"\"\"\n",
        "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
        "\n",
        "    distances = (\n",
        "        jnp.sum(jnp.square(flat_inputs), 1, keepdims=True) -\n",
        "        2 * jnp.matmul(flat_inputs, self.embeddings) +\n",
        "        jnp.sum(jnp.square(self.embeddings), 0, keepdims=True))\n",
        "\n",
        "    encoding_indices = jnp.argmax(-distances, 1)\n",
        "    encodings = jax.nn.one_hot(encoding_indices,\n",
        "                               self.num_embeddings,\n",
        "                               dtype=distances.dtype)\n",
        "\n",
        "    # NB: if your code crashes with a reshape error on the line below about a\n",
        "    # Tensor containing the wrong number of values, then the most likely cause\n",
        "    # is that the input passed in does not have a final dimension equal to\n",
        "    # self.embedding_dim. Ideally we would catch this with an Assert but that\n",
        "    # creates various other problems related to device placement / TPUs.\n",
        "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
        "    quantized = self.quantize(encoding_indices)\n",
        "\n",
        "    e_latent_loss = jnp.mean(\n",
        "        jnp.square(jax.lax.stop_gradient(quantized) - inputs))\n",
        "    q_latent_loss = jnp.mean(\n",
        "        jnp.square(quantized - jax.lax.stop_gradient(inputs)))\n",
        "    loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "    # Straight Through Estimator\n",
        "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
        "    avg_probs = jnp.mean(encodings, 0)\n",
        "    if self.cross_replica_axis:\n",
        "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
        "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
        "\n",
        "    return {\n",
        "        \"quantize\": quantized,\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"encodings\": encodings,\n",
        "        \"encoding_indices\": encoding_indices,\n",
        "        \"distances\": distances,\n",
        "    }\n",
        "\n",
        "  def quantize(self, encoding_indices):\n",
        "    \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
        "    w = self.embeddings.swapaxes(1, 0)\n",
        "    w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
        "    return w[(encoding_indices,)]\n",
        "\n",
        "\n",
        "class VectorQuantizerEMA(hk.Module):\n",
        "  r\"\"\"Haiku module representing the VQ-VAE layer.\n",
        "\n",
        "  Implements a slightly modified version of the algorithm presented in\n",
        "  \"Neural Discrete Representation Learning\" by van den Oord et al.\n",
        "  https://arxiv.org/abs/1711.00937\n",
        "\n",
        "  The difference between :class:`VectorQuantizerEMA` and\n",
        "  :class:`VectorQuantizer` is that this module uses\n",
        "  :class:`~haiku.ExponentialMovingAverage`\\ s to update the embedding vectors\n",
        "  instead of an auxiliary loss. This has the advantage that the embedding\n",
        "  updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac,\n",
        "  ...) used for the encoder, decoder and other parts of the architecture. For\n",
        "  most experiments the EMA version trains faster than the non-EMA version.\n",
        "\n",
        "  Input any tensor to be quantized. Last dimension will be used as space in\n",
        "  which to quantize. All other dimensions will be flattened and will be seen\n",
        "  as different examples to quantize.\n",
        "\n",
        "  The output tensor will have the same shape as the input.\n",
        "\n",
        "  For example a tensor with shape ``[16, 32, 32, 64]`` will be reshaped into\n",
        "  ``[16384, 64]`` and all ``16384`` vectors (each of 64 dimensions)  will be\n",
        "  quantized independently.\n",
        "\n",
        "  Attributes:\n",
        "    embedding_dim: integer representing the dimensionality of the tensors in\n",
        "      the quantized space. Inputs to the modules must be in this format as well.\n",
        "    num_embeddings: integer, the number of vectors in the quantized space.\n",
        "    commitment_cost: scalar which controls the weighting of the loss terms\n",
        "      (see equation 4 in the paper).\n",
        "    decay: float, decay for the moving averages.\n",
        "    epsilon: small float constant to avoid numerical instability.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dim,\n",
        "      num_embeddings,\n",
        "      commitment_cost,\n",
        "      decay,\n",
        "      epsilon: float = 1e-5,\n",
        "      dtype: Any = jnp.float32,\n",
        "      cross_replica_axis: Optional[str] = None,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Initializes a VQ-VAE EMA module.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: integer representing the dimensionality of the tensors in\n",
        "        the quantized space. Inputs to the modules must be in this format as\n",
        "        well.\n",
        "      num_embeddings: integer, the number of vectors in the quantized space.\n",
        "      commitment_cost: scalar which controls the weighting of the loss terms\n",
        "        (see equation 4 in the paper - this variable is Beta).\n",
        "      decay: float between 0 and 1, controls the speed of the Exponential Moving\n",
        "        Averages.\n",
        "      epsilon: small constant to aid numerical stability, default ``1e-5``.\n",
        "      dtype: dtype for the embeddings variable, defaults to ``float32``.\n",
        "      cross_replica_axis: If not ``None``, it should be a string representing\n",
        "        the axis name over which this module is being run within a\n",
        "        :func:`jax.pmap`. Supplying this argument means that cluster statistics\n",
        "        and the perplexity are calculated across all replicas on that axis.\n",
        "      name: name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    if not 0 <= decay <= 1:\n",
        "      raise ValueError(\"decay must be in range [0, 1]\")\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.decay = decay\n",
        "    self.commitment_cost = commitment_cost\n",
        "    self.epsilon = epsilon\n",
        "    self.cross_replica_axis = cross_replica_axis\n",
        "\n",
        "    self._embedding_shape = [embedding_dim, num_embeddings]\n",
        "    self._dtype = dtype\n",
        "\n",
        "    self._ema_cluster_size = hk.ExponentialMovingAverage(\n",
        "        decay=self.decay, name=\"ema_cluster_size\")\n",
        "    self._ema_dw = hk.ExponentialMovingAverage(decay=self.decay, name=\"ema_dw\")\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    initializer = hk.initializers.VarianceScaling(distribution=\"uniform\")\n",
        "    return hk.get_state(\n",
        "        \"embeddings\", self._embedding_shape, self._dtype, init=initializer)\n",
        "\n",
        "  @property\n",
        "  def ema_cluster_size(self):\n",
        "    self._ema_cluster_size.initialize([self.num_embeddings], self._dtype)\n",
        "    return self._ema_cluster_size\n",
        "\n",
        "  @property\n",
        "  def ema_dw(self):\n",
        "    self._ema_dw.initialize(self._embedding_shape, self._dtype)\n",
        "    return self._ema_dw\n",
        "\n",
        "  def __call__(self, inputs, is_training):\n",
        "    \"\"\"Connects the module to some inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Tensor, final dimension must be equal to ``embedding_dim``. All\n",
        "        other leading dimensions will be flattened and treated as a large batch.\n",
        "      is_training: boolean, whether this connection is to training data. When\n",
        "        this is set to ``False``, the internal moving average statistics will\n",
        "        not be updated.\n",
        "\n",
        "    Returns:\n",
        "      dict: Dictionary containing the following keys and values:\n",
        "        * ``quantize``: Tensor containing the quantized version of the input.\n",
        "        * ``loss``: Tensor containing the loss to optimize.\n",
        "        * ``perplexity``: Tensor containing the perplexity of the encodings.\n",
        "        * ``encodings``: Tensor containing the discrete encodings, ie which\n",
        "          element of the quantized space each input element was mapped to.\n",
        "        * ``encoding_indices``: Tensor containing the discrete encoding indices,\n",
        "          ie which element of the quantized space each input element was mapped\n",
        "          to.\n",
        "    \"\"\"\n",
        "    flat_inputs = jnp.reshape(inputs, [-1, self.embedding_dim])\n",
        "    embeddings = self.embeddings\n",
        "\n",
        "    distances = (\n",
        "        jnp.sum(jnp.square(flat_inputs), 1, keepdims=True) -\n",
        "        2 * jnp.matmul(flat_inputs, embeddings) +\n",
        "        jnp.sum(jnp.square(embeddings), 0, keepdims=True))\n",
        "\n",
        "    encoding_indices = jnp.argmax(-distances, 1)\n",
        "    encodings = jax.nn.one_hot(encoding_indices,\n",
        "                               self.num_embeddings,\n",
        "                               dtype=distances.dtype)\n",
        "\n",
        "    # NB: if your code crashes with a reshape error on the line below about a\n",
        "    # Tensor containing the wrong number of values, then the most likely cause\n",
        "    # is that the input passed in does not have a final dimension equal to\n",
        "    # self.embedding_dim. Ideally we would catch this with an Assert but that\n",
        "    # creates various other problems related to device placement / TPUs.\n",
        "    encoding_indices = jnp.reshape(encoding_indices, inputs.shape[:-1])\n",
        "    quantized = self.quantize(encoding_indices)\n",
        "    e_latent_loss = jnp.mean(\n",
        "        jnp.square(jax.lax.stop_gradient(quantized) - inputs))\n",
        "\n",
        "    if is_training:\n",
        "      cluster_size = jnp.sum(encodings, axis=0)\n",
        "      if self.cross_replica_axis:\n",
        "        cluster_size = jax.lax.psum(\n",
        "            cluster_size, axis_name=self.cross_replica_axis)\n",
        "      updated_ema_cluster_size = self.ema_cluster_size(cluster_size)\n",
        "\n",
        "      dw = jnp.matmul(flat_inputs.T, encodings)\n",
        "      if self.cross_replica_axis:\n",
        "        dw = jax.lax.psum(dw, axis_name=self.cross_replica_axis)\n",
        "      updated_ema_dw = self.ema_dw(dw)\n",
        "\n",
        "      n = jnp.sum(updated_ema_cluster_size)\n",
        "      updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /\n",
        "                                  (n + self.num_embeddings * self.epsilon) * n)\n",
        "\n",
        "      normalised_updated_ema_w = (\n",
        "          updated_ema_dw / jnp.reshape(updated_ema_cluster_size, [1, -1]))\n",
        "\n",
        "      hk.set_state(\"embeddings\", normalised_updated_ema_w)\n",
        "      loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "    else:\n",
        "      loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "    # Straight Through Estimator\n",
        "    quantized = inputs + jax.lax.stop_gradient(quantized - inputs)\n",
        "    avg_probs = jnp.mean(encodings, 0)\n",
        "    if self.cross_replica_axis:\n",
        "      avg_probs = jax.lax.pmean(avg_probs, axis_name=self.cross_replica_axis)\n",
        "    perplexity = jnp.exp(-jnp.sum(avg_probs * jnp.log(avg_probs + 1e-10)))\n",
        "\n",
        "    return {\n",
        "        \"quantize\": quantized,\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"encodings\": encodings,\n",
        "        \"encoding_indices\": encoding_indices,\n",
        "        \"distances\": distances,\n",
        "    }\n",
        "\n",
        "  def quantize(self, encoding_indices):\n",
        "    \"\"\"Returns embedding tensor for a batch of indices.\"\"\"\n",
        "    w = self.embeddings.swapaxes(1, 0)\n",
        "    w = jax.device_put(w)  # Required when embeddings is a NumPy array.\n",
        "    return w[(encoding_indices,)]"
      ],
      "metadata": {
        "id": "uHsrsUu_TYny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for haiku._src.nets.vqvae.\"\"\"\n",
        "\n",
        "import functools\n",
        "\n",
        "from absl.testing import absltest\n",
        "from absl.testing import parameterized\n",
        "\n",
        "from haiku._src import stateful\n",
        "from haiku._src import test_utils\n",
        "from haiku._src import transform\n",
        "from haiku._src.nets import vqvae\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VqvaeTest(parameterized.TestCase):\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testConstruct(self, constructor, kwargs):\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    # Batch of input vectors to quantize\n",
        "    inputs_np = np.random.randn(100, kwargs['embedding_dim']).astype(np.float32)\n",
        "    inputs = jnp.array(inputs_np)\n",
        "\n",
        "    # Set is_training to False, otherwise for the EMA case just evaluating the\n",
        "    # forward pass will change the embeddings, meaning that some of our computed\n",
        "    # closest embeddings will be incorrect.\n",
        "    vq_output = vqvae_module(inputs, is_training=False)\n",
        "\n",
        "    # Output shape is correct\n",
        "    self.assertEqual(vq_output['quantize'].shape, inputs.shape)\n",
        "\n",
        "    vq_output_np = jax.tree_util.tree_map(lambda t: t, vq_output)\n",
        "    embeddings_np = vqvae_module.embeddings\n",
        "\n",
        "    self.assertEqual(embeddings_np.shape,\n",
        "                     (kwargs['embedding_dim'], kwargs['num_embeddings']))\n",
        "\n",
        "    # Check that each input was assigned to the embedding it is closest to.\n",
        "    distances = (jnp.square(inputs_np).sum(axis=1, keepdims=True) -\n",
        "                 2 * np.dot(inputs_np, embeddings_np) +\n",
        "                 jnp.square(embeddings_np).sum(axis=0, keepdims=True))\n",
        "    closest_index = np.argmax(-distances, axis=1)\n",
        "    # On TPU, distances can be different by ~1% due to precision. This can cause\n",
        "    # the distanc to the closest embedding to flip, leading to a difference\n",
        "    # in the encoding indices tensor. First we check that the continuous\n",
        "    # distances are reasonably close, and then we only allow N differences in\n",
        "    # the encodings. For batch of 100, N == 3 seems okay (passed 1000x tests).\n",
        "    np.testing.assert_allclose(distances, vq_output_np['distances'], atol=5e-2)\n",
        "    num_differences_in_encodings = (closest_index !=\n",
        "                                    vq_output_np['encoding_indices']).sum()\n",
        "    num_differences_allowed = 3\n",
        "    self.assertLessEqual(num_differences_in_encodings, num_differences_allowed)\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testShapeChecking(self, constructor, kwargs):\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    wrong_shape_input = np.random.randn(100, kwargs['embedding_dim'] * 2)\n",
        "    with self.assertRaisesRegex(TypeError, 'total size must be unchanged'):\n",
        "      vqvae_module(\n",
        "          jnp.array(wrong_shape_input.astype(np.float32)), is_training=False)\n",
        "\n",
        "  @parameterized.parameters((vqvae.VectorQuantizer, {\n",
        "      'embedding_dim': 4,\n",
        "      'num_embeddings': 8,\n",
        "      'commitment_cost': 0.25\n",
        "  }), (vqvae.VectorQuantizerEMA, {\n",
        "      'embedding_dim': 6,\n",
        "      'num_embeddings': 13,\n",
        "      'commitment_cost': 0.5,\n",
        "      'decay': 0.1\n",
        "  }))\n",
        "  @test_utils.transform_and_run\n",
        "  def testNoneBatch(self, constructor, kwargs):\n",
        "    \"\"\"Check that vqvae can be built on input with a None batch dimension.\"\"\"\n",
        "    vqvae_module = constructor(**kwargs)\n",
        "    inputs = jnp.zeros([0, 5, 5, kwargs['embedding_dim']])\n",
        "    vqvae_module(inputs, is_training=False)\n",
        "\n",
        "  @parameterized.parameters({'use_jit': True, 'dtype': jnp.float32},\n",
        "                            {'use_jit': True, 'dtype': jnp.float64},\n",
        "                            {'use_jit': False, 'dtype': jnp.float32},\n",
        "                            {'use_jit': False, 'dtype': jnp.float64})\n",
        "  @test_utils.transform_and_run\n",
        "  def testEmaUpdating(self, use_jit, dtype):\n",
        "    if jax.local_devices()[0].platform == 'tpu' and dtype == jnp.float64:\n",
        "      self.skipTest('F64 not supported by TPU')\n",
        "\n",
        "    embedding_dim = 6\n",
        "    np_dtype = np.float64 if dtype is jnp.float64 else np.float32\n",
        "    decay = np.array(0.1, dtype=np_dtype)\n",
        "    vqvae_module = vqvae.VectorQuantizerEMA(\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_embeddings=7,\n",
        "        commitment_cost=0.5,\n",
        "        decay=decay,\n",
        "        dtype=dtype)\n",
        "\n",
        "    if use_jit:\n",
        "      vqvae_f = stateful.jit(vqvae_module, static_argnums=1)\n",
        "    else:\n",
        "      vqvae_f = vqvae_module\n",
        "\n",
        "    batch_size = 16\n",
        "\n",
        "    prev_embeddings = vqvae_module.embeddings\n",
        "\n",
        "    # Embeddings should change with every forwards pass if is_training == True.\n",
        "    for _ in range(10):\n",
        "      inputs = np.random.rand(batch_size, embedding_dim).astype(dtype)\n",
        "      vqvae_f(inputs, True)\n",
        "      current_embeddings = vqvae_module.embeddings\n",
        "      self.assertFalse((prev_embeddings == current_embeddings).all())\n",
        "      prev_embeddings = current_embeddings\n",
        "\n",
        "    # Forward passes with is_training == False don't change anything\n",
        "    for _ in range(10):\n",
        "      inputs = np.random.rand(batch_size, embedding_dim).astype(dtype)\n",
        "      vqvae_f(inputs, False)\n",
        "      current_embeddings = vqvae_module.embeddings\n",
        "      self.assertTrue((current_embeddings == prev_embeddings).all())\n",
        "\n",
        "  def testEmaCrossReplica(self):\n",
        "    embedding_dim = 6\n",
        "    batch_size = 16\n",
        "    inputs = np.random.rand(jax.local_device_count(), batch_size, embedding_dim)\n",
        "    embeddings = {}\n",
        "    perplexities = {}\n",
        "\n",
        "    for axis_name in [None, 'i']:\n",
        "      def my_function(x, axis_name):\n",
        "        decay = np.array(0.9, dtype=np.float32)\n",
        "        vqvae_module = vqvae.VectorQuantizerEMA(\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_embeddings=7,\n",
        "            commitment_cost=0.5,\n",
        "            decay=decay,\n",
        "            cross_replica_axis=axis_name,\n",
        "            dtype=jnp.float32)\n",
        "\n",
        "        outputs = vqvae_module(x, is_training=True)\n",
        "        return vqvae_module.embeddings, outputs['perplexity']\n",
        "\n",
        "      vqvae_f = transform.transform_with_state(\n",
        "          functools.partial(my_function, axis_name=axis_name))\n",
        "\n",
        "      rng = jax.random.PRNGKey(42)\n",
        "      rng = jnp.broadcast_to(rng, (jax.local_device_count(), *rng.shape))\n",
        "\n",
        "      params, state = jax.pmap(\n",
        "          vqvae_f.init, axis_name='i')(rng, inputs)\n",
        "      update_fn = jax.pmap(vqvae_f.apply, axis_name='i')\n",
        "\n",
        "      for _ in range(10):\n",
        "        outputs, state = update_fn(params, state, None, inputs)\n",
        "      embeddings[axis_name], perplexities[axis_name] = outputs\n",
        "\n",
        "    # In the single-device case, specifying a cross_replica_axis should have\n",
        "    # no effect. Otherwise, it should!\n",
        "    if jax.device_count() == 1:\n",
        "      # Have to use assert_allclose here rather than checking exact matches to\n",
        "      # make the test pass on GPU, presumably because of nondeterministic\n",
        "      # reductions.\n",
        "      np.testing.assert_allclose(\n",
        "          embeddings[None], embeddings['i'], rtol=1e-6, atol=1e-6)\n",
        "      np.testing.assert_allclose(\n",
        "          perplexities[None], perplexities['i'], rtol=1e-6, atol=1e-6)\n",
        "    else:\n",
        "      self.assertFalse((embeddings[None] == embeddings['i']).all())\n",
        "      self.assertFalse((perplexities[None] == perplexities['i']).all())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  absltest.main()"
      ],
      "metadata": {
        "id": "z_srkCXLTiGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}