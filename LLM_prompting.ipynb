{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan0v0/nninn/blob/master/LLM_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3oJrsrbEmk"
      },
      "source": [
        "# LLM Prompting\n",
        "\n",
        "I use a standardized library ([pyllms][1]) to interact with different APIs. This will allow to conduct comparative tests.\n",
        "\n",
        "[1]: https://github.com/kagisearch/pyllms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll insert some code that will make this notebook's visualizations and text formatting easier to read."
      ],
      "metadata": {
        "id": "CizJ6J3heuPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bldx_ebF3Mgu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import locale\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll install a library we'll need to run the cells in this notebook."
      ],
      "metadata": {
        "id": "0KcOb49OevM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyllms"
      ],
      "metadata": {
        "id": "WynHdvXgeyk8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e747292-5271-41a1-bd70-602d7feedc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyllms\n",
            "  Downloading pyllms-0.2.5-py3-none-any.whl (25 kB)\n",
            "Collecting openai (from pyllms)\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from pyllms)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anthropic (from pyllms)\n",
            "  Downloading anthropic-0.2.10-py3-none-any.whl (6.3 kB)\n",
            "Collecting ai21 (from pyllms)\n",
            "  Downloading ai21-1.1.4.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cohere (from pyllms)\n",
            "  Downloading cohere-4.11.2-py3-none-any.whl (39 kB)\n",
            "Collecting aleph-alpha-client (from pyllms)\n",
            "  Downloading aleph_alpha_client-3.1.2-py3-none-any.whl (31 kB)\n",
            "Collecting huggingface-hub (from pyllms)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-aiplatform (from pyllms)\n",
            "  Downloading google_cloud_aiplatform-1.26.0-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from pyllms) (0.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ai21->pyllms) (2.27.1)\n",
            "Collecting requests (from ai21->pyllms)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client->pyllms) (1.26.15)\n",
            "Collecting aiohttp>=3.8.3 (from aleph-alpha-client->pyllms)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiodns>=3.0.0 (from aleph-alpha-client->pyllms)\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from aleph-alpha-client->pyllms)\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Collecting tokenizers>=0.13.2 (from aleph-alpha-client->pyllms)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client->pyllms) (4.5.0)\n",
            "Collecting Pillow>=9.2.0 (from aleph-alpha-client->pyllms)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from anthropic->pyllms)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0,>=2.0 (from cohere->pyllms)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting importlib_metadata<7.0,>=6.0 (from cohere->pyllms)\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (2.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->pyllms) (3.9.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform->pyllms)\n",
            "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform->pyllms)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->pyllms) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->pyllms) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->pyllms) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->pyllms) (6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyllms) (2022.10.31)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=3.0.0->aleph-alpha-client->pyllms)\n",
            "  Downloading pycares-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.3->aleph-alpha-client->pyllms) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.3->aleph-alpha-client->pyllms) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.8.3->aleph-alpha-client->pyllms)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp>=3.8.3->aleph-alpha-client->pyllms)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.8.3->aleph-alpha-client->pyllms)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp>=3.8.3->aleph-alpha-client->pyllms)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp>=3.8.3->aleph-alpha-client->pyllms)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->pyllms) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->pyllms) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->pyllms) (2.8.2)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->pyllms)\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere->pyllms) (3.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ai21->pyllms) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ai21->pyllms) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->anthropic->pyllms)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->anthropic->pyllms) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->pyllms) (1.5.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx->anthropic->pyllms)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->anthropic->pyllms) (3.6.2)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->pyllms) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->pyllms) (2.21)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->pyllms) (0.5.0)\n",
            "Building wheels for collected packages: ai21\n",
            "  Building wheel for ai21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ai21: filename=ai21-1.1.4-py3-none-any.whl size=21793 sha256=ad6af013a895a378ce4bb8489d30da247d9c7f3a706a7936769294792d1c761f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/8a/20/798f6f060c424f53fc8d4e6cf55e0a625ebc459d4c3da52978\n",
            "Successfully built ai21\n",
            "Installing collected packages: tokenizers, shapely, requests, Pillow, multidict, importlib_metadata, h11, frozenlist, backoff, async-timeout, yarl, tiktoken, pycares, huggingface-hub, httpcore, aiosignal, ai21, httpx, grpc-google-iam-v1, aiohttp, aiodns, openai, cohere, anthropic, aiohttp-retry, google-cloud-resource-manager, aleph-alpha-client, google-cloud-aiplatform, pyllms\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.5.0 ai21-1.1.4 aiodns-3.0.0 aiohttp-3.8.4 aiohttp-retry-2.8.3 aiosignal-1.3.1 aleph-alpha-client-3.1.2 anthropic-0.2.10 async-timeout-4.0.2 backoff-2.2.1 cohere-4.11.2 frozenlist-1.3.3 google-cloud-aiplatform-1.26.0 google-cloud-resource-manager-1.10.1 grpc-google-iam-v1-0.12.6 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 importlib_metadata-6.7.0 multidict-6.0.4 openai-0.27.8 pycares-4.3.0 pyllms-0.2.5 requests-2.31.0 shapely-1.8.5.post1 tiktoken-0.4.0 tokenizers-0.13.3 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll import our API keyes for OpenAI and Anthropic.\n",
        "\n",
        "If you're running this notebook by yourself, you'll need to request API keys and then insert them here. These keys are typically obtained on their respective websites, e.g. [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "UstxaZobe2xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZO9ZDBFXbAMZ",
        "outputId": "24010925-22e0-44ec-da36-628b02e39177"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "openai_api_key = \"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\"\n",
        "anthropic_api_key = \"\" # data['anthropic_api_key']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijPUaxUvhyNP"
      },
      "source": [
        "<br><br><br><br>\n",
        "\n",
        "## Getting Responses from ChatGPT\n",
        "\n",
        "In this first step, we will create an *endpoint* we can use to get response from chatGPT.\n",
        "\n",
        "First, we'll import a library and load a GPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "r4L0hsXfhaaW",
        "outputId": "d2d85b99-af33-424e-de75-22c0d204462b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import llms\n",
        "\n",
        "gpt_model = llms.init(openai_api_key=openai_api_key, model='gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try a simple prompt completion."
      ],
      "metadata": {
        "id": "trzZ5Z6bfSPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sNPzb2gvlhe7",
        "outputId": "e3c0c986-a010-4a75-801a-0b96f542de09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am an AI language model created by OpenAI, designed to assist with various tasks such as answering questions, generating text, and providing information.\n"
          ]
        }
      ],
      "source": [
        "text = \"Who are you?\"\n",
        "output = gpt_model.complete(text)\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can request information about the request you just made (model, token count, costs)."
      ],
      "metadata": {
        "id": "QWd1m9KjfVur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jXiPxf9-o1_g",
        "outputId": "f9964843-57f7-4c59-f739-db71e2d94ebc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'gpt-3.5-turbo', 'tokens': 41, 'tokens_prompt': 12, 'tokens_completion': 29, 'cost': 8e-05, 'latency': 1.72}\n"
          ]
        }
      ],
      "source": [
        "print(output.meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try something more complicated.\n",
        "\n",
        "Let's set up a history of prompts to feed into the model."
      ],
      "metadata": {
        "id": "LnLawWybfaF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []"
      ],
      "metadata": {
        "id": "GaUHMC9gfj5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c9621e90-7ba0-4d69-eae5-f91db839acbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we set up a general description of the role of the model.\n",
        "When using ChatGPT, this is a dictionary with role marked as assistant."
      ],
      "metadata": {
        "id": "37EeX-VufkAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_instructions = '''\\\n",
        "You are an helpful AI Assistant.\n",
        "You have knowledge of which day it is, and location of the user.\n",
        "\n",
        "Date: {date}\n",
        "Location: {location}\\\n",
        "'''.format(\n",
        "    date=\"June 20, 2023\",\n",
        "    location=\"Cambridge, UK\"\n",
        ")\n",
        "history.append(\n",
        "    {'role': 'assistant', 'content': base_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "zmF_HaBNfqqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "310b8d27-003c-41d5-bea6-5125cdb4cf66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we add a couple of back and forth between user and system."
      ],
      "metadata": {
        "id": "wFj_j-kCfqyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history.append(\n",
        "    {'role': 'user', 'content': 'Who are you?'}\n",
        ")"
      ],
      "metadata": {
        "id": "MBNDmFlBfvp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8e92704a-63d5-4d58-df83-ea1bcefac8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how we are role-playing the \"system\" (aka chatGPT) in the 2nd one!"
      ],
      "metadata": {
        "id": "JPJjkz_ffvxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WBPBhBYco53s",
        "outputId": "890d36d5-24cc-4f82-dca4-ed43d45045e9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The William Gates Building is located at the University of Cambridge's West Cambridge site, about 3 miles west of the city center. The address is 15 JJ Thomson Ave, Cambridge CB3 0FD, United Kingdom.\n"
          ]
        }
      ],
      "source": [
        "history.append(\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'I am an AI language model. We are at Cambridge Machine Learning Systems Lab.'\n",
        "    }\n",
        ")\n",
        "\n",
        "output = gpt_model.complete(\"Where is Wiiliam Gates Building?\", history=history)\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One common instruction is asking a model to be brief. Let's see how it changes the response."
      ],
      "metadata": {
        "id": "esK5_7a0f39q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TkSwj3X1wD09",
        "outputId": "b886ce3f-d27b-4b1d-f82c-9475709f86d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The William Gates Building is located in the West Cambridge Site of the University of Cambridge, in the UK.\n"
          ]
        }
      ],
      "source": [
        "output = gpt_model.complete(\"Where is Wiiliam Gates Building? Be as brief as possible.\", history=history)\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try to instruct the model to be brief in the system prompt as well, but that fails sometimes..."
      ],
      "metadata": {
        "id": "7-6GteFGgAue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rGns9mP8vtoX",
        "outputId": "ddc8c6f2-fcbb-4b6c-89dd-28d5b0445a8a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The William Gates Building is located at the University of Cambridge, 15 JJ Thomson Ave, Cambridge CB3 0FD, United Kingdom.\n"
          ]
        }
      ],
      "source": [
        "base_instructions = '''\\\n",
        "You are an helpful AI Assistant.\n",
        "Your responses are as brief as possible.\n",
        "You have knowledge of which day it is, and location of the user.\n",
        "\n",
        "Date: {date}\n",
        "Location: {location}\\\n",
        "'''.format(\n",
        "    date=\"June 20, 2023\",\n",
        "    location=\"Cambridge, UK\"\n",
        ")\n",
        "history[0] = {'role': 'assistant', 'content': base_instructions}\n",
        "output = gpt_model.complete(\"Where is Wiiliam Gates Building?\", history=history)\n",
        "print(output.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try using two LLMs at once and compare their output!"
      ],
      "metadata": {
        "id": "swzSzSCIgGR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "q1xYraIO1jZk",
        "outputId": "14cbc715-2a3d-43e8-8a97-a733187e7b85"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-fa7a64b3f503>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ],
      "source": [
        "models=llms.init(\n",
        "    model=['gpt-3.5-turbo'],# 'claude-instant-v1'],\n",
        "    openai_api_key=openai_api_key,\n",
        "    # anthropic_api_key=anthropic_api_key\n",
        ")\n",
        "\n",
        "outputs = models.complete(\n",
        "    \"Write a Python program to greet a user. Make sure to refer to refer to the user using their chosen pronouns.\"\n",
        ")\n",
        "for meta, output in zip(outputs.meta, outputs.text):\n",
        "    print(meta['models'])\n",
        "    print(output)\n",
        "    print('\\n' + '-' * 40 + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q16youdedcV"
      },
      "source": [
        "<br><br><br><br>\n",
        "\n",
        "## Local Model Prompting\n",
        "\n",
        "From  this section on, we will try prompting a local model.\n",
        "\n",
        "These models have pros and cons:\n",
        "\n",
        "* **PRO**: they are trained on corpora available to the research community, making it possible to study the interplay between training data and model output.\n",
        "* **PRO**: they can be run locally, often on consumer level GPU (or even in a Colab notebook!)\n",
        "\n",
        "\n",
        "* **CON**: they are typically just instruction finetuned: that is, they have received no preference feedback. That makes more tricky to output responses that match what you request.\n",
        "* **CON**: rapidly evolving landscape: the open source community is developing these models at a rapid pace, and that often comes at the expense of rigorous evaluation of harms and risks of each. Be very careful with the output you get from each model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using a recently software to interactively play with LLM locally called [falcontune](https://github.com/rmihaylov/falcontune). It is a very simple command line tool to both generate using an LLM, as well as fine tune an LLM on your own data.\n",
        "\n",
        "It uses a technique called [LoRA](https://arxiv.org/abs/2106.09685) to minimize the amount of resources used to run a model.\n",
        "\n",
        "We are using a model called [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct), a model recently released Apache 2.0 license. It was trained on web data, and fine-tuned on instruction data obtained from a combination academic datasets and output from OpenAI GPT models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--------------------\n",
        "\n",
        "\n",
        "Download model from huggingface using wget and save it to `/tmp/`"
      ],
      "metadata": {
        "id": "mtc2l-lbgKpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/resolve/main/gptq_model-4bit-64g.safetensors' -O '/tmp/falcon-7b-instruct-GPTQ.safetensors'"
      ],
      "metadata": {
        "id": "aj59n7GsSnEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the repository and install all its dependencies."
      ],
      "metadata": {
        "id": "y6BmZ4WugOAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rmihaylov/falcontune /tmp/falcontune\n",
        "!pip install -r /tmp/falcontune/requirements.txt\n",
        "!pip install /tmp/falcontune/\n",
        "!cd /tmp/falcontune && python setup_cuda.py install"
      ],
      "metadata": {
        "id": "GSAYA5RJUvK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the command line to generate."
      ],
      "metadata": {
        "id": "Wegvzxa1gVJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!falcontune generate \\\n",
        "    --interactive \\\n",
        "    --model falcon-7b-instruct-4bit \\\n",
        "    --weights /tmp/falcon-7b-instruct-GPTQ.safetensors \\\n",
        "    --max_new_tokens=160 \\\n",
        "    --use_cache \\\n",
        "    --do_sample \\\n",
        "    --instruction \"Write a Python program to greet a user.\""
      ],
      "metadata": {
        "id": "7swYnC2PUcfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}